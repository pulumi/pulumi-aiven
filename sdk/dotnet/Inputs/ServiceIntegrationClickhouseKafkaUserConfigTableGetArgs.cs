// *** WARNING: this file was generated by pulumi-language-dotnet. ***
// *** Do not edit by hand unless you're certain you know what you are doing! ***

using System;
using System.Collections.Generic;
using System.Collections.Immutable;
using System.Threading.Tasks;
using Pulumi.Serialization;

namespace Pulumi.Aiven.Inputs
{

    public sealed class ServiceIntegrationClickhouseKafkaUserConfigTableGetArgs : global::Pulumi.ResourceArgs
    {
        /// <summary>
        /// Enum: `beginning`, `earliest`, `end`, `largest`, `latest`, `smallest`. Determines where to start reading from Kafka when no offset is stored or the stored offset is out of range. `earliest` starts from the beginning, `latest` starts from the end. Default: `earliest`.
        /// </summary>
        [Input("autoOffsetReset")]
        public Input<string>? AutoOffsetReset { get; set; }

        [Input("columns", required: true)]
        private InputList<Inputs.ServiceIntegrationClickhouseKafkaUserConfigTableColumnGetArgs>? _columns;

        /// <summary>
        /// Array of column definitions that specify the structure of the ClickHouse table. Each column maps to a field in the Kafka messages
        /// </summary>
        public InputList<Inputs.ServiceIntegrationClickhouseKafkaUserConfigTableColumnGetArgs> Columns
        {
            get => _columns ?? (_columns = new InputList<Inputs.ServiceIntegrationClickhouseKafkaUserConfigTableColumnGetArgs>());
            set => _columns = value;
        }

        /// <summary>
        /// Enum: `Avro`, `AvroConfluent`, `CSV`, `JSONAsString`, `JSONCompactEachRow`, `JSONCompactStringsEachRow`, `JSONEachRow`, `JSONStringsEachRow`, `MsgPack`, `Parquet`, `RawBLOB`, `TSKV`, `TSV`, `TabSeparated`. The format of the messages in the Kafka topics. Determines how ClickHouse parses and serializes the data (e.g., JSON, CSV, Avro). Default: `JSONEachRow`.
        /// </summary>
        [Input("dataFormat", required: true)]
        public Input<string> DataFormat { get; set; } = null!;

        /// <summary>
        /// Enum: `basic`, `best_effort`, `best_effort_us`. Specifies how ClickHouse should parse DateTime values from text-based input formats. `basic` uses simple parsing, `best_effort` attempts more flexible parsing. Default: `basic`.
        /// </summary>
        [Input("dateTimeInputFormat")]
        public Input<string>? DateTimeInputFormat { get; set; }

        /// <summary>
        /// The Kafka consumer group name. Multiple consumers with the same group name will share the workload and maintain offset positions. Default: `clickhouse`.
        /// </summary>
        [Input("groupName", required: true)]
        public Input<string> GroupName { get; set; } = null!;

        /// <summary>
        /// Enum: `default`, `stream`. Defines how ClickHouse should handle errors when processing Kafka messages. `default` stops on errors, `stream` continues processing and logs errors. Default: `default`.
        /// </summary>
        [Input("handleErrorMode")]
        public Input<string>? HandleErrorMode { get; set; }

        /// <summary>
        /// Maximum number of rows to collect before flushing data between Kafka and ClickHouse. Default: `0`.
        /// </summary>
        [Input("maxBlockSize")]
        public Input<int>? MaxBlockSize { get; set; }

        /// <summary>
        /// Maximum number of rows that can be processed from a single Kafka message for row-based formats. Useful for controlling memory usage. Default: `1`.
        /// </summary>
        [Input("maxRowsPerMessage")]
        public Input<int>? MaxRowsPerMessage { get; set; }

        /// <summary>
        /// The name of the ClickHouse table to be created. This table can consume data from and write data to the specified Kafka topics. Example: `events`.
        /// </summary>
        [Input("name", required: true)]
        public Input<string> Name { get; set; } = null!;

        /// <summary>
        /// Number of Kafka consumers to run per table per replica. Increasing this can improve throughput but may increase resource usage. Default: `1`.
        /// </summary>
        [Input("numConsumers")]
        public Input<int>? NumConsumers { get; set; }

        /// <summary>
        /// Maximum number of messages to fetch in a single Kafka poll operation for reading. Default: `0`.
        /// </summary>
        [Input("pollMaxBatchSize")]
        public Input<int>? PollMaxBatchSize { get; set; }

        /// <summary>
        /// Timeout in milliseconds for a single poll from Kafka. Takes the value of the stream*flush*interval_ms server setting by default (500ms). Default: `0`.
        /// </summary>
        [Input("pollMaxTimeoutMs")]
        public Input<int>? PollMaxTimeoutMs { get; set; }

        /// <summary>
        /// The maximum number of messages in a batch sent to Kafka. If the number of messages exceeds this value, the batch is sent. Default: `10000`.
        /// </summary>
        [Input("producerBatchNumMessages")]
        public Input<int>? ProducerBatchNumMessages { get; set; }

        /// <summary>
        /// The maximum size in bytes of a batch of messages sent to Kafka. If the batch size is exceeded, the batch is sent.
        /// </summary>
        [Input("producerBatchSize")]
        public Input<int>? ProducerBatchSize { get; set; }

        /// <summary>
        /// Enum: `gzip`, `lz4`, `none`, `snappy`, `zstd`. The compression codec to use when sending a batch of messages to Kafka. Default: `none`.
        /// </summary>
        [Input("producerCompressionCodec")]
        public Input<string>? ProducerCompressionCodec { get; set; }

        /// <summary>
        /// The compression level to use when sending a batch of messages to Kafka. Usable range is algorithm-dependent: [0-9] for gzip; [0-12] for lz4; only 0 for snappy; -1 = codec-dependent default compression level. Default: `-1`.
        /// </summary>
        [Input("producerCompressionLevel")]
        public Input<int>? ProducerCompressionLevel { get; set; }

        /// <summary>
        /// The time in milliseconds to wait for additional messages before sending a batch. If the time is exceeded, the batch is sent. Default: `5`.
        /// </summary>
        [Input("producerLingerMs")]
        public Input<int>? ProducerLingerMs { get; set; }

        /// <summary>
        /// The maximum size of the buffer in kilobytes before sending.
        /// </summary>
        [Input("producerQueueBufferingMaxKbytes")]
        public Input<int>? ProducerQueueBufferingMaxKbytes { get; set; }

        /// <summary>
        /// The maximum number of messages to buffer before sending. Default: `100000`.
        /// </summary>
        [Input("producerQueueBufferingMaxMessages")]
        public Input<int>? ProducerQueueBufferingMaxMessages { get; set; }

        /// <summary>
        /// The number of acknowledgements the leader broker must receive from ISR brokers before responding to the request: 0=Broker does not send any response/ack to client, -1 will block until message is committed by all in sync replicas (ISRs). Default: `-1`.
        /// </summary>
        [Input("producerRequestRequiredAcks")]
        public Input<int>? ProducerRequestRequiredAcks { get; set; }

        /// <summary>
        /// Number of broken messages to skip before stopping processing when reading from Kafka. Useful for handling corrupted data without failing the entire integration. Default: `0`.
        /// </summary>
        [Input("skipBrokenMessages")]
        public Input<int>? SkipBrokenMessages { get; set; }

        /// <summary>
        /// When enabled, each consumer runs in its own thread, providing better isolation and potentially better performance for high-throughput scenarios. Default: `false`.
        /// </summary>
        [Input("threadPerConsumer")]
        public Input<bool>? ThreadPerConsumer { get; set; }

        [Input("topics", required: true)]
        private InputList<Inputs.ServiceIntegrationClickhouseKafkaUserConfigTableTopicGetArgs>? _topics;

        /// <summary>
        /// Array of Kafka topics that this table will read data from or write data to. Messages from all specified topics will be inserted into this table, and data inserted into this table will be published to the topics
        /// </summary>
        public InputList<Inputs.ServiceIntegrationClickhouseKafkaUserConfigTableTopicGetArgs> Topics
        {
            get => _topics ?? (_topics = new InputList<Inputs.ServiceIntegrationClickhouseKafkaUserConfigTableTopicGetArgs>());
            set => _topics = value;
        }

        public ServiceIntegrationClickhouseKafkaUserConfigTableGetArgs()
        {
        }
        public static new ServiceIntegrationClickhouseKafkaUserConfigTableGetArgs Empty => new ServiceIntegrationClickhouseKafkaUserConfigTableGetArgs();
    }
}
