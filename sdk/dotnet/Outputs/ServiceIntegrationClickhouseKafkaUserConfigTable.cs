// *** WARNING: this file was generated by pulumi-language-dotnet. ***
// *** Do not edit by hand unless you're certain you know what you are doing! ***

using System;
using System.Collections.Generic;
using System.Collections.Immutable;
using System.Threading.Tasks;
using Pulumi.Serialization;

namespace Pulumi.Aiven.Outputs
{

    [OutputType]
    public sealed class ServiceIntegrationClickhouseKafkaUserConfigTable
    {
        /// <summary>
        /// Enum: `beginning`, `earliest`, `end`, `largest`, `latest`, `smallest`. Action to take when there is no initial offset in offset store or the desired offset is out of range. Default: `earliest`.
        /// </summary>
        public readonly string? AutoOffsetReset;
        /// <summary>
        /// Table columns
        /// </summary>
        public readonly ImmutableArray<Outputs.ServiceIntegrationClickhouseKafkaUserConfigTableColumn> Columns;
        /// <summary>
        /// Enum: `Avro`, `AvroConfluent`, `CSV`, `JSONAsString`, `JSONCompactEachRow`, `JSONCompactStringsEachRow`, `JSONEachRow`, `JSONStringsEachRow`, `MsgPack`, `Parquet`, `RawBLOB`, `TSKV`, `TSV`, `TabSeparated`. Message data format. Default: `JSONEachRow`.
        /// </summary>
        public readonly string DataFormat;
        /// <summary>
        /// Enum: `basic`, `best_effort`, `best_effort_us`. Method to read DateTime from text input formats. Default: `basic`.
        /// </summary>
        public readonly string? DateTimeInputFormat;
        /// <summary>
        /// Kafka consumers group. Default: `clickhouse`.
        /// </summary>
        public readonly string GroupName;
        /// <summary>
        /// Enum: `default`, `stream`. How to handle errors for Kafka engine. Default: `default`.
        /// </summary>
        public readonly string? HandleErrorMode;
        /// <summary>
        /// Number of row collected by poll(s) for flushing data from Kafka. Default: `0`.
        /// </summary>
        public readonly int? MaxBlockSize;
        /// <summary>
        /// The maximum number of rows produced in one kafka message for row-based formats. Default: `1`.
        /// </summary>
        public readonly int? MaxRowsPerMessage;
        /// <summary>
        /// Name of the table. Example: `events`.
        /// </summary>
        public readonly string Name;
        /// <summary>
        /// The number of consumers per table per replica. Default: `1`.
        /// </summary>
        public readonly int? NumConsumers;
        /// <summary>
        /// Maximum amount of messages to be polled in a single Kafka poll. Default: `0`.
        /// </summary>
        public readonly int? PollMaxBatchSize;
        /// <summary>
        /// Timeout in milliseconds for a single poll from Kafka. Takes the value of the stream*flush*interval_ms server setting by default (500ms). Default: `0`.
        /// </summary>
        public readonly int? PollMaxTimeoutMs;
        /// <summary>
        /// The maximum number of messages in a batch sent to Kafka. If the number of messages exceeds this value, the batch is sent. Default: `10000`.
        /// </summary>
        public readonly int? ProducerBatchNumMessages;
        /// <summary>
        /// The maximum size in bytes of a batch of messages sent to Kafka. If the batch size is exceeded, the batch is sent. Default: `1000000`.
        /// </summary>
        public readonly int? ProducerBatchSize;
        /// <summary>
        /// Enum: `gzip`, `lz4`, `none`, `snappy`, `zstd`. The compression codec to use when sending a batch of messages to Kafka. Default: `none`.
        /// </summary>
        public readonly string? ProducerCompressionCodec;
        /// <summary>
        /// The compression level to use when sending a batch of messages to Kafka. Usable range is algorithm-dependent: [0-9] for gzip; [0-12] for lz4; only 0 for snappy; -1 = codec-dependent default compression level. Default: `-1`.
        /// </summary>
        public readonly int? ProducerCompressionLevel;
        /// <summary>
        /// The time in milliseconds to wait for additional messages before sending a batch. If the time is exceeded, the batch is sent. Default: `5`.
        /// </summary>
        public readonly int? ProducerLingerMs;
        /// <summary>
        /// The maximum size of the buffer in kilobytes before sending. Default: `1048576`.
        /// </summary>
        public readonly int? ProducerQueueBufferingMaxKbytes;
        /// <summary>
        /// The maximum number of messages to buffer before sending. Default: `100000`.
        /// </summary>
        public readonly int? ProducerQueueBufferingMaxMessages;
        /// <summary>
        /// The number of acknowledgements the leader broker must receive from ISR brokers before responding to the request: 0=Broker does not send any response/ack to client, -1 will block until message is committed by all in sync replicas (ISRs). Default: `-1`.
        /// </summary>
        public readonly int? ProducerRequestRequiredAcks;
        /// <summary>
        /// Skip at least this number of broken messages from Kafka topic per block. Default: `0`.
        /// </summary>
        public readonly int? SkipBrokenMessages;
        /// <summary>
        /// Provide an independent thread for each consumer. All consumers run in the same thread by default. Default: `false`.
        /// </summary>
        public readonly bool? ThreadPerConsumer;
        /// <summary>
        /// Kafka topics
        /// </summary>
        public readonly ImmutableArray<Outputs.ServiceIntegrationClickhouseKafkaUserConfigTableTopic> Topics;

        [OutputConstructor]
        private ServiceIntegrationClickhouseKafkaUserConfigTable(
            string? autoOffsetReset,

            ImmutableArray<Outputs.ServiceIntegrationClickhouseKafkaUserConfigTableColumn> columns,

            string dataFormat,

            string? dateTimeInputFormat,

            string groupName,

            string? handleErrorMode,

            int? maxBlockSize,

            int? maxRowsPerMessage,

            string name,

            int? numConsumers,

            int? pollMaxBatchSize,

            int? pollMaxTimeoutMs,

            int? producerBatchNumMessages,

            int? producerBatchSize,

            string? producerCompressionCodec,

            int? producerCompressionLevel,

            int? producerLingerMs,

            int? producerQueueBufferingMaxKbytes,

            int? producerQueueBufferingMaxMessages,

            int? producerRequestRequiredAcks,

            int? skipBrokenMessages,

            bool? threadPerConsumer,

            ImmutableArray<Outputs.ServiceIntegrationClickhouseKafkaUserConfigTableTopic> topics)
        {
            AutoOffsetReset = autoOffsetReset;
            Columns = columns;
            DataFormat = dataFormat;
            DateTimeInputFormat = dateTimeInputFormat;
            GroupName = groupName;
            HandleErrorMode = handleErrorMode;
            MaxBlockSize = maxBlockSize;
            MaxRowsPerMessage = maxRowsPerMessage;
            Name = name;
            NumConsumers = numConsumers;
            PollMaxBatchSize = pollMaxBatchSize;
            PollMaxTimeoutMs = pollMaxTimeoutMs;
            ProducerBatchNumMessages = producerBatchNumMessages;
            ProducerBatchSize = producerBatchSize;
            ProducerCompressionCodec = producerCompressionCodec;
            ProducerCompressionLevel = producerCompressionLevel;
            ProducerLingerMs = producerLingerMs;
            ProducerQueueBufferingMaxKbytes = producerQueueBufferingMaxKbytes;
            ProducerQueueBufferingMaxMessages = producerQueueBufferingMaxMessages;
            ProducerRequestRequiredAcks = producerRequestRequiredAcks;
            SkipBrokenMessages = skipBrokenMessages;
            ThreadPerConsumer = threadPerConsumer;
            Topics = topics;
        }
    }
}
