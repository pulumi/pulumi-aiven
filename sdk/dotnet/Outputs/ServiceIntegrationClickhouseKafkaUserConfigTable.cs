// *** WARNING: this file was generated by pulumi-language-dotnet. ***
// *** Do not edit by hand unless you're certain you know what you are doing! ***

using System;
using System.Collections.Generic;
using System.Collections.Immutable;
using System.Threading.Tasks;
using Pulumi.Serialization;

namespace Pulumi.Aiven.Outputs
{

    [OutputType]
    public sealed class ServiceIntegrationClickhouseKafkaUserConfigTable
    {
        /// <summary>
        /// Enum: `Beginning`, `Earliest`, `End`, `Largest`, `Latest`, `Smallest`. Determines where to start reading from Kafka when no offset is stored or the stored offset is out of range. `Earliest` starts from the beginning, `Latest` starts from the end. Default: `Earliest`.
        /// </summary>
        public readonly string? AutoOffsetReset;
        /// <summary>
        /// Array of column definitions that specify the structure of the ClickHouse table. Each column maps to a field in the Kafka messages
        /// </summary>
        public readonly ImmutableArray<Outputs.ServiceIntegrationClickhouseKafkaUserConfigTableColumn> Columns;
        /// <summary>
        /// Enum: `Avro`, `AvroConfluent`, `CSV`, `JSONAsString`, `JSONCompactEachRow`, `JSONCompactStringsEachRow`, `JSONEachRow`, `JSONStringsEachRow`, `MsgPack`, `Parquet`, `RawBLOB`, `TSKV`, `TSV`, `TabSeparated`. The format of the messages in the Kafka topics. Determines how ClickHouse parses and serializes the data (e.g., JSON, CSV, Avro). Default: `JSONEachRow`.
        /// </summary>
        public readonly string DataFormat;
        /// <summary>
        /// Enum: `Basic`, `BestEffort`, `BestEffortUs`. Specifies how ClickHouse should parse DateTime values from text-based input formats. `Basic` uses simple parsing, `BestEffort` attempts more flexible parsing. Default: `Basic`.
        /// </summary>
        public readonly string? DateTimeInputFormat;
        /// <summary>
        /// The Kafka consumer group name. Multiple consumers with the same group name will share the workload and maintain offset positions. Default: `Clickhouse`.
        /// </summary>
        public readonly string GroupName;
        /// <summary>
        /// Enum: `Default`, `Stream`. Defines how ClickHouse should handle errors when processing Kafka messages. `Default` stops on errors, `Stream` continues processing and logs errors. Default: `Default`.
        /// </summary>
        public readonly string? HandleErrorMode;
        /// <summary>
        /// Maximum number of rows to collect before flushing data between Kafka and ClickHouse. Default: `0`.
        /// </summary>
        public readonly int? MaxBlockSize;
        /// <summary>
        /// Maximum number of rows that can be processed from a single Kafka message for row-based formats. Useful for controlling memory usage. Default: `1`.
        /// </summary>
        public readonly int? MaxRowsPerMessage;
        /// <summary>
        /// The name of the ClickHouse table to be created. This table can consume data from and write data to the specified Kafka topics. Example: `Events`.
        /// </summary>
        public readonly string Name;
        /// <summary>
        /// Number of Kafka consumers to run per table per replica. Increasing this can improve throughput but may increase resource usage. Default: `1`.
        /// </summary>
        public readonly int? NumConsumers;
        /// <summary>
        /// Maximum number of messages to fetch in a single Kafka poll operation for reading. Default: `0`.
        /// </summary>
        public readonly int? PollMaxBatchSize;
        /// <summary>
        /// Timeout in milliseconds for a single poll from Kafka. Takes the value of the stream*flush*interval_ms server setting by default (500ms). Default: `0`.
        /// </summary>
        public readonly int? PollMaxTimeoutMs;
        /// <summary>
        /// The maximum number of messages in a batch sent to Kafka. If the number of messages exceeds this value, the batch is sent. Default: `10000`.
        /// </summary>
        public readonly int? ProducerBatchNumMessages;
        /// <summary>
        /// The maximum size in bytes of a batch of messages sent to Kafka. If the batch size is exceeded, the batch is sent.
        /// </summary>
        public readonly int? ProducerBatchSize;
        /// <summary>
        /// Enum: `Gzip`, `Lz4`, `None`, `Snappy`, `Zstd`. The compression codec to use when sending a batch of messages to Kafka. Default: `None`.
        /// </summary>
        public readonly string? ProducerCompressionCodec;
        /// <summary>
        /// The compression level to use when sending a batch of messages to Kafka. Usable range is algorithm-dependent: [0-9] for gzip; [0-12] for lz4; only 0 for snappy; -1 = codec-dependent default compression level. Default: `-1`.
        /// </summary>
        public readonly int? ProducerCompressionLevel;
        /// <summary>
        /// The time in milliseconds to wait for additional messages before sending a batch. If the time is exceeded, the batch is sent. Default: `5`.
        /// </summary>
        public readonly int? ProducerLingerMs;
        /// <summary>
        /// The maximum size of the buffer in kilobytes before sending.
        /// </summary>
        public readonly int? ProducerQueueBufferingMaxKbytes;
        /// <summary>
        /// The maximum number of messages to buffer before sending. Default: `100000`.
        /// </summary>
        public readonly int? ProducerQueueBufferingMaxMessages;
        /// <summary>
        /// The number of acknowledgements the leader broker must receive from ISR brokers before responding to the request: 0=Broker does not send any response/ack to client, -1 will block until message is committed by all in sync replicas (ISRs). Default: `-1`.
        /// </summary>
        public readonly int? ProducerRequestRequiredAcks;
        /// <summary>
        /// Number of broken messages to skip before stopping processing when reading from Kafka. Useful for handling corrupted data without failing the entire integration. Default: `0`.
        /// </summary>
        public readonly int? SkipBrokenMessages;
        /// <summary>
        /// When enabled, each consumer runs in its own thread, providing better isolation and potentially better performance for high-throughput scenarios. Default: `False`.
        /// </summary>
        public readonly bool? ThreadPerConsumer;
        /// <summary>
        /// Array of Kafka topics that this table will read data from or write data to. Messages from all specified topics will be inserted into this table, and data inserted into this table will be published to the topics
        /// </summary>
        public readonly ImmutableArray<Outputs.ServiceIntegrationClickhouseKafkaUserConfigTableTopic> Topics;

        [OutputConstructor]
        private ServiceIntegrationClickhouseKafkaUserConfigTable(
            string? autoOffsetReset,

            ImmutableArray<Outputs.ServiceIntegrationClickhouseKafkaUserConfigTableColumn> columns,

            string dataFormat,

            string? dateTimeInputFormat,

            string groupName,

            string? handleErrorMode,

            int? maxBlockSize,

            int? maxRowsPerMessage,

            string name,

            int? numConsumers,

            int? pollMaxBatchSize,

            int? pollMaxTimeoutMs,

            int? producerBatchNumMessages,

            int? producerBatchSize,

            string? producerCompressionCodec,

            int? producerCompressionLevel,

            int? producerLingerMs,

            int? producerQueueBufferingMaxKbytes,

            int? producerQueueBufferingMaxMessages,

            int? producerRequestRequiredAcks,

            int? skipBrokenMessages,

            bool? threadPerConsumer,

            ImmutableArray<Outputs.ServiceIntegrationClickhouseKafkaUserConfigTableTopic> topics)
        {
            AutoOffsetReset = autoOffsetReset;
            Columns = columns;
            DataFormat = dataFormat;
            DateTimeInputFormat = dateTimeInputFormat;
            GroupName = groupName;
            HandleErrorMode = handleErrorMode;
            MaxBlockSize = maxBlockSize;
            MaxRowsPerMessage = maxRowsPerMessage;
            Name = name;
            NumConsumers = numConsumers;
            PollMaxBatchSize = pollMaxBatchSize;
            PollMaxTimeoutMs = pollMaxTimeoutMs;
            ProducerBatchNumMessages = producerBatchNumMessages;
            ProducerBatchSize = producerBatchSize;
            ProducerCompressionCodec = producerCompressionCodec;
            ProducerCompressionLevel = producerCompressionLevel;
            ProducerLingerMs = producerLingerMs;
            ProducerQueueBufferingMaxKbytes = producerQueueBufferingMaxKbytes;
            ProducerQueueBufferingMaxMessages = producerQueueBufferingMaxMessages;
            ProducerRequestRequiredAcks = producerRequestRequiredAcks;
            SkipBrokenMessages = skipBrokenMessages;
            ThreadPerConsumer = threadPerConsumer;
            Topics = topics;
        }
    }
}
