// *** WARNING: this file was generated by the Pulumi Terraform Bridge (tfgen) Tool. ***
// *** Do not edit by hand unless you're certain you know what you are doing! ***

using System;
using System.Collections.Generic;
using System.Collections.Immutable;
using System.Threading.Tasks;
using Pulumi.Serialization;

namespace Pulumi.Aiven
{
    /// <summary>
    /// The Flink Table resource allows the creation and management of Aiven Tables.
    /// 
    /// ## Example Usage
    /// 
    /// ```csharp
    /// using Pulumi;
    /// using Aiven = Pulumi.Aiven;
    /// 
    /// class MyStack : Stack
    /// {
    ///     public MyStack()
    ///     {
    ///         var table = new Aiven.FlinkJobTable("table", new Aiven.FlinkJobTableArgs
    ///         {
    ///             Project = data.Aiven_project.Pr1.Project,
    ///             ServiceName = aiven_flink.Flink.Service_name,
    ///             TableName = "&lt;TABLE_NAME&gt;",
    ///             IntegrationId = aiven_service_integration.Flink_kafka.Service_id,
    ///             JdbcTable = "&lt;JDBC_TABLE_NAME&gt;",
    ///             KafkaTopic = aiven_kafka_topic.Table_topic.Topic_name,
    ///             SchemaSql = @"      `+""`cpu`""+` INT,
    ///       `+""`node`""+` INT,
    ///       `+""`occurred_at`""+` TIMESTAMP(3) METADATA FROM 'timestamp',
    ///       WATERMARK FOR `+""`occurred_at`""+` AS `+""`occurred_at`""+` - INTERVAL '5' SECOND
    /// ",
    ///         });
    ///     }
    /// 
    /// }
    /// ```
    /// 
    /// ## Import
    /// 
    /// ```sh
    ///  $ pulumi import aiven:index/flinkJobTable:FlinkJobTable table project/service_name/table_id
    /// ```
    /// </summary>
    [AivenResourceType("aiven:index/flinkJobTable:FlinkJobTable")]
    public partial class FlinkJobTable : Pulumi.CustomResource
    {
        /// <summary>
        /// The id of the service integration that is used with this table. It must have the service integration type `flink`. To set up proper dependencies please refer to this variable as a reference. This property cannot be changed, doing so forces recreation of the resource.
        /// </summary>
        [Output("integrationId")]
        public Output<string> IntegrationId { get; private set; } = null!;

        /// <summary>
        /// Name of the jdbc table that is to be connected to this table. Valid if the service integration id refers to a mysql or postgres service. This property cannot be changed, doing so forces recreation of the resource.
        /// </summary>
        [Output("jdbcTable")]
        public Output<string?> JdbcTable { get; private set; } = null!;

        /// <summary>
        /// When used as a source, upsert Kafka connectors update values that use an existing key and delete values that are null. For sinks, the connector correspondingly writes update or delete messages in a compacted topic. If no matching key is found, the values are added as new entries. For more information, see the Apache Flink documentation The possible values are `kafka` and `upsert-kafka`. This property cannot be changed, doing so forces recreation of the resource.
        /// </summary>
        [Output("kafkaConnectorType")]
        public Output<string?> KafkaConnectorType { get; private set; } = null!;

        /// <summary>
        /// Defines an explicit list of physical columns from the table schema that configure the data type for the key format. This property cannot be changed, doing so forces recreation of the resource.
        /// </summary>
        [Output("kafkaKeyFields")]
        public Output<ImmutableArray<string>> KafkaKeyFields { get; private set; } = null!;

        /// <summary>
        /// Kafka Key Format The possible values are `avro`, `avro-confluent`, `debezium-avro-confluent`, `debezium-json` and `json`. This property cannot be changed, doing so forces recreation of the resource.
        /// </summary>
        [Output("kafkaKeyFormat")]
        public Output<string?> KafkaKeyFormat { get; private set; } = null!;

        /// <summary>
        /// Startup mode The possible values are `earliest-offset`, `latest-offset`, `group-offsets` and `timestamp`. This property cannot be changed, doing so forces recreation of the resource.
        /// </summary>
        [Output("kafkaStartupMode")]
        public Output<string?> KafkaStartupMode { get; private set; } = null!;

        /// <summary>
        /// Name of the kafka topic that is to be connected to this table. Valid if the service integration id refers to a kafka service. This property cannot be changed, doing so forces recreation of the resource.
        /// </summary>
        [Output("kafkaTopic")]
        public Output<string?> KafkaTopic { get; private set; } = null!;

        /// <summary>
        /// Controls how key columns are handled in the message value. Select ALL to include the physical columns of the table schema in the message value. Select EXCEPT_KEY to exclude the physical columns of the table schema from the message value. This is the default for upsert Kafka connectors. The possible values are `[ALL EXCEPT_KEY]`. This property cannot be changed, doing so forces recreation of the resource.
        /// </summary>
        [Output("kafkaValueFieldsInclude")]
        public Output<string?> KafkaValueFieldsInclude { get; private set; } = null!;

        /// <summary>
        /// Kafka Value Format The possible values are `avro`, `avro-confluent`, `debezium-avro-confluent`, `debezium-json` and `json`. This property cannot be changed, doing so forces recreation of the resource.
        /// </summary>
        [Output("kafkaValueFormat")]
        public Output<string?> KafkaValueFormat { get; private set; } = null!;

        /// <summary>
        /// [LIKE](https://nightlies.apache.org/flink/flink-docs-master/docs/dev/table/sql/create/#like) statement for table creation. This property cannot be changed, doing so forces recreation of the resource.
        /// </summary>
        [Output("likeOptions")]
        public Output<string?> LikeOptions { get; private set; } = null!;

        /// <summary>
        /// Identifies the project this resource belongs to. To set up proper dependencies please refer to this variable as a reference. This property cannot be changed, doing so forces recreation of the resource.
        /// </summary>
        [Output("project")]
        public Output<string> Project { get; private set; } = null!;

        /// <summary>
        /// The SQL statement to create the table. This property cannot be changed, doing so forces recreation of the resource.
        /// </summary>
        [Output("schemaSql")]
        public Output<string> SchemaSql { get; private set; } = null!;

        /// <summary>
        /// Specifies the name of the service that this resource belongs to. To set up proper dependencies please refer to this variable as a reference. This property cannot be changed, doing so forces recreation of the resource.
        /// </summary>
        [Output("serviceName")]
        public Output<string> ServiceName { get; private set; } = null!;

        /// <summary>
        /// The Table ID of the flink table in the flink service.
        /// </summary>
        [Output("tableId")]
        public Output<string> TableId { get; private set; } = null!;

        /// <summary>
        /// Specifies the name of the table. This property cannot be changed, doing so forces recreation of the resource.
        /// </summary>
        [Output("tableName")]
        public Output<string> TableName { get; private set; } = null!;

        /// <summary>
        /// Kafka upsert connector configuration.
        /// </summary>
        [Output("upsertKafka")]
        public Output<Outputs.FlinkJobTableUpsertKafka?> UpsertKafka { get; private set; } = null!;


        /// <summary>
        /// Create a FlinkJobTable resource with the given unique name, arguments, and options.
        /// </summary>
        ///
        /// <param name="name">The unique name of the resource</param>
        /// <param name="args">The arguments used to populate this resource's properties</param>
        /// <param name="options">A bag of options that control this resource's behavior</param>
        public FlinkJobTable(string name, FlinkJobTableArgs args, CustomResourceOptions? options = null)
            : base("aiven:index/flinkJobTable:FlinkJobTable", name, args ?? new FlinkJobTableArgs(), MakeResourceOptions(options, ""))
        {
        }

        private FlinkJobTable(string name, Input<string> id, FlinkJobTableState? state = null, CustomResourceOptions? options = null)
            : base("aiven:index/flinkJobTable:FlinkJobTable", name, state, MakeResourceOptions(options, id))
        {
        }

        private static CustomResourceOptions MakeResourceOptions(CustomResourceOptions? options, Input<string>? id)
        {
            var defaultOptions = new CustomResourceOptions
            {
                Version = Utilities.Version,
            };
            var merged = CustomResourceOptions.Merge(defaultOptions, options);
            // Override the ID if one was specified for consistency with other language SDKs.
            merged.Id = id ?? merged.Id;
            return merged;
        }
        /// <summary>
        /// Get an existing FlinkJobTable resource's state with the given name, ID, and optional extra
        /// properties used to qualify the lookup.
        /// </summary>
        ///
        /// <param name="name">The unique name of the resulting resource.</param>
        /// <param name="id">The unique provider ID of the resource to lookup.</param>
        /// <param name="state">Any extra arguments used during the lookup.</param>
        /// <param name="options">A bag of options that control this resource's behavior</param>
        public static FlinkJobTable Get(string name, Input<string> id, FlinkJobTableState? state = null, CustomResourceOptions? options = null)
        {
            return new FlinkJobTable(name, id, state, options);
        }
    }

    public sealed class FlinkJobTableArgs : Pulumi.ResourceArgs
    {
        /// <summary>
        /// The id of the service integration that is used with this table. It must have the service integration type `flink`. To set up proper dependencies please refer to this variable as a reference. This property cannot be changed, doing so forces recreation of the resource.
        /// </summary>
        [Input("integrationId", required: true)]
        public Input<string> IntegrationId { get; set; } = null!;

        /// <summary>
        /// Name of the jdbc table that is to be connected to this table. Valid if the service integration id refers to a mysql or postgres service. This property cannot be changed, doing so forces recreation of the resource.
        /// </summary>
        [Input("jdbcTable")]
        public Input<string>? JdbcTable { get; set; }

        /// <summary>
        /// When used as a source, upsert Kafka connectors update values that use an existing key and delete values that are null. For sinks, the connector correspondingly writes update or delete messages in a compacted topic. If no matching key is found, the values are added as new entries. For more information, see the Apache Flink documentation The possible values are `kafka` and `upsert-kafka`. This property cannot be changed, doing so forces recreation of the resource.
        /// </summary>
        [Input("kafkaConnectorType")]
        public Input<string>? KafkaConnectorType { get; set; }

        [Input("kafkaKeyFields")]
        private InputList<string>? _kafkaKeyFields;

        /// <summary>
        /// Defines an explicit list of physical columns from the table schema that configure the data type for the key format. This property cannot be changed, doing so forces recreation of the resource.
        /// </summary>
        public InputList<string> KafkaKeyFields
        {
            get => _kafkaKeyFields ?? (_kafkaKeyFields = new InputList<string>());
            set => _kafkaKeyFields = value;
        }

        /// <summary>
        /// Kafka Key Format The possible values are `avro`, `avro-confluent`, `debezium-avro-confluent`, `debezium-json` and `json`. This property cannot be changed, doing so forces recreation of the resource.
        /// </summary>
        [Input("kafkaKeyFormat")]
        public Input<string>? KafkaKeyFormat { get; set; }

        /// <summary>
        /// Startup mode The possible values are `earliest-offset`, `latest-offset`, `group-offsets` and `timestamp`. This property cannot be changed, doing so forces recreation of the resource.
        /// </summary>
        [Input("kafkaStartupMode")]
        public Input<string>? KafkaStartupMode { get; set; }

        /// <summary>
        /// Name of the kafka topic that is to be connected to this table. Valid if the service integration id refers to a kafka service. This property cannot be changed, doing so forces recreation of the resource.
        /// </summary>
        [Input("kafkaTopic")]
        public Input<string>? KafkaTopic { get; set; }

        /// <summary>
        /// Controls how key columns are handled in the message value. Select ALL to include the physical columns of the table schema in the message value. Select EXCEPT_KEY to exclude the physical columns of the table schema from the message value. This is the default for upsert Kafka connectors. The possible values are `[ALL EXCEPT_KEY]`. This property cannot be changed, doing so forces recreation of the resource.
        /// </summary>
        [Input("kafkaValueFieldsInclude")]
        public Input<string>? KafkaValueFieldsInclude { get; set; }

        /// <summary>
        /// Kafka Value Format The possible values are `avro`, `avro-confluent`, `debezium-avro-confluent`, `debezium-json` and `json`. This property cannot be changed, doing so forces recreation of the resource.
        /// </summary>
        [Input("kafkaValueFormat")]
        public Input<string>? KafkaValueFormat { get; set; }

        /// <summary>
        /// [LIKE](https://nightlies.apache.org/flink/flink-docs-master/docs/dev/table/sql/create/#like) statement for table creation. This property cannot be changed, doing so forces recreation of the resource.
        /// </summary>
        [Input("likeOptions")]
        public Input<string>? LikeOptions { get; set; }

        /// <summary>
        /// Identifies the project this resource belongs to. To set up proper dependencies please refer to this variable as a reference. This property cannot be changed, doing so forces recreation of the resource.
        /// </summary>
        [Input("project", required: true)]
        public Input<string> Project { get; set; } = null!;

        /// <summary>
        /// The SQL statement to create the table. This property cannot be changed, doing so forces recreation of the resource.
        /// </summary>
        [Input("schemaSql", required: true)]
        public Input<string> SchemaSql { get; set; } = null!;

        /// <summary>
        /// Specifies the name of the service that this resource belongs to. To set up proper dependencies please refer to this variable as a reference. This property cannot be changed, doing so forces recreation of the resource.
        /// </summary>
        [Input("serviceName", required: true)]
        public Input<string> ServiceName { get; set; } = null!;

        /// <summary>
        /// Specifies the name of the table. This property cannot be changed, doing so forces recreation of the resource.
        /// </summary>
        [Input("tableName", required: true)]
        public Input<string> TableName { get; set; } = null!;

        /// <summary>
        /// Kafka upsert connector configuration.
        /// </summary>
        [Input("upsertKafka")]
        public Input<Inputs.FlinkJobTableUpsertKafkaArgs>? UpsertKafka { get; set; }

        public FlinkJobTableArgs()
        {
        }
    }

    public sealed class FlinkJobTableState : Pulumi.ResourceArgs
    {
        /// <summary>
        /// The id of the service integration that is used with this table. It must have the service integration type `flink`. To set up proper dependencies please refer to this variable as a reference. This property cannot be changed, doing so forces recreation of the resource.
        /// </summary>
        [Input("integrationId")]
        public Input<string>? IntegrationId { get; set; }

        /// <summary>
        /// Name of the jdbc table that is to be connected to this table. Valid if the service integration id refers to a mysql or postgres service. This property cannot be changed, doing so forces recreation of the resource.
        /// </summary>
        [Input("jdbcTable")]
        public Input<string>? JdbcTable { get; set; }

        /// <summary>
        /// When used as a source, upsert Kafka connectors update values that use an existing key and delete values that are null. For sinks, the connector correspondingly writes update or delete messages in a compacted topic. If no matching key is found, the values are added as new entries. For more information, see the Apache Flink documentation The possible values are `kafka` and `upsert-kafka`. This property cannot be changed, doing so forces recreation of the resource.
        /// </summary>
        [Input("kafkaConnectorType")]
        public Input<string>? KafkaConnectorType { get; set; }

        [Input("kafkaKeyFields")]
        private InputList<string>? _kafkaKeyFields;

        /// <summary>
        /// Defines an explicit list of physical columns from the table schema that configure the data type for the key format. This property cannot be changed, doing so forces recreation of the resource.
        /// </summary>
        public InputList<string> KafkaKeyFields
        {
            get => _kafkaKeyFields ?? (_kafkaKeyFields = new InputList<string>());
            set => _kafkaKeyFields = value;
        }

        /// <summary>
        /// Kafka Key Format The possible values are `avro`, `avro-confluent`, `debezium-avro-confluent`, `debezium-json` and `json`. This property cannot be changed, doing so forces recreation of the resource.
        /// </summary>
        [Input("kafkaKeyFormat")]
        public Input<string>? KafkaKeyFormat { get; set; }

        /// <summary>
        /// Startup mode The possible values are `earliest-offset`, `latest-offset`, `group-offsets` and `timestamp`. This property cannot be changed, doing so forces recreation of the resource.
        /// </summary>
        [Input("kafkaStartupMode")]
        public Input<string>? KafkaStartupMode { get; set; }

        /// <summary>
        /// Name of the kafka topic that is to be connected to this table. Valid if the service integration id refers to a kafka service. This property cannot be changed, doing so forces recreation of the resource.
        /// </summary>
        [Input("kafkaTopic")]
        public Input<string>? KafkaTopic { get; set; }

        /// <summary>
        /// Controls how key columns are handled in the message value. Select ALL to include the physical columns of the table schema in the message value. Select EXCEPT_KEY to exclude the physical columns of the table schema from the message value. This is the default for upsert Kafka connectors. The possible values are `[ALL EXCEPT_KEY]`. This property cannot be changed, doing so forces recreation of the resource.
        /// </summary>
        [Input("kafkaValueFieldsInclude")]
        public Input<string>? KafkaValueFieldsInclude { get; set; }

        /// <summary>
        /// Kafka Value Format The possible values are `avro`, `avro-confluent`, `debezium-avro-confluent`, `debezium-json` and `json`. This property cannot be changed, doing so forces recreation of the resource.
        /// </summary>
        [Input("kafkaValueFormat")]
        public Input<string>? KafkaValueFormat { get; set; }

        /// <summary>
        /// [LIKE](https://nightlies.apache.org/flink/flink-docs-master/docs/dev/table/sql/create/#like) statement for table creation. This property cannot be changed, doing so forces recreation of the resource.
        /// </summary>
        [Input("likeOptions")]
        public Input<string>? LikeOptions { get; set; }

        /// <summary>
        /// Identifies the project this resource belongs to. To set up proper dependencies please refer to this variable as a reference. This property cannot be changed, doing so forces recreation of the resource.
        /// </summary>
        [Input("project")]
        public Input<string>? Project { get; set; }

        /// <summary>
        /// The SQL statement to create the table. This property cannot be changed, doing so forces recreation of the resource.
        /// </summary>
        [Input("schemaSql")]
        public Input<string>? SchemaSql { get; set; }

        /// <summary>
        /// Specifies the name of the service that this resource belongs to. To set up proper dependencies please refer to this variable as a reference. This property cannot be changed, doing so forces recreation of the resource.
        /// </summary>
        [Input("serviceName")]
        public Input<string>? ServiceName { get; set; }

        /// <summary>
        /// The Table ID of the flink table in the flink service.
        /// </summary>
        [Input("tableId")]
        public Input<string>? TableId { get; set; }

        /// <summary>
        /// Specifies the name of the table. This property cannot be changed, doing so forces recreation of the resource.
        /// </summary>
        [Input("tableName")]
        public Input<string>? TableName { get; set; }

        /// <summary>
        /// Kafka upsert connector configuration.
        /// </summary>
        [Input("upsertKafka")]
        public Input<Inputs.FlinkJobTableUpsertKafkaGetArgs>? UpsertKafka { get; set; }

        public FlinkJobTableState()
        {
        }
    }
}
