// *** WARNING: this file was generated by the Pulumi Terraform Bridge (tfgen) Tool. ***
// *** Do not edit by hand unless you're certain you know what you are doing! ***

import * as pulumi from "@pulumi/pulumi";
import { input as inputs, output as outputs } from "./types";
import * as utilities from "./utilities";

/**
 * The Flink Table resource allows the creation and management of Aiven Tables.
 *
 * ## Example Usage
 *
 * ```typescript
 * import * as pulumi from "@pulumi/pulumi";
 * import * as aiven from "@pulumi/aiven";
 *
 * const table = new aiven.FlinkJobTable("table", {
 *     project: data.aiven_project.pr1.project,
 *     serviceName: aiven_flink.flink.service_name,
 *     tableName: "<TABLE_NAME>",
 *     integrationId: aiven_service_integration.flink_kafka.service_id,
 *     jdbcTable: "<JDBC_TABLE_NAME>",
 *     kafkaTopic: aiven_kafka_topic.table_topic.topic_name,
 *     schemaSql: `      `+"`cpu`"+` INT,
 *       `+"`node`"+` INT,
 *       `+"`occurred_at`"+` TIMESTAMP(3) METADATA FROM 'timestamp',
 *       WATERMARK FOR `+"`occurred_at`"+` AS `+"`occurred_at`"+` - INTERVAL '5' SECOND
 * `,
 * });
 * ```
 *
 * ## Import
 *
 * ```sh
 *  $ pulumi import aiven:index/flinkJobTable:FlinkJobTable table project/service_name/table_id
 * ```
 */
export class FlinkJobTable extends pulumi.CustomResource {
    /**
     * Get an existing FlinkJobTable resource's state with the given name, ID, and optional extra
     * properties used to qualify the lookup.
     *
     * @param name The _unique_ name of the resulting resource.
     * @param id The _unique_ provider ID of the resource to lookup.
     * @param state Any extra arguments used during the lookup.
     * @param opts Optional settings to control the behavior of the CustomResource.
     */
    public static get(name: string, id: pulumi.Input<pulumi.ID>, state?: FlinkJobTableState, opts?: pulumi.CustomResourceOptions): FlinkJobTable {
        return new FlinkJobTable(name, <any>state, { ...opts, id: id });
    }

    /** @internal */
    public static readonly __pulumiType = 'aiven:index/flinkJobTable:FlinkJobTable';

    /**
     * Returns true if the given object is an instance of FlinkJobTable.  This is designed to work even
     * when multiple copies of the Pulumi SDK have been loaded into the same process.
     */
    public static isInstance(obj: any): obj is FlinkJobTable {
        if (obj === undefined || obj === null) {
            return false;
        }
        return obj['__pulumiType'] === FlinkJobTable.__pulumiType;
    }

    /**
     * The id of the service integration that is used with this table. It must have the service integration type `flink`. To set up proper dependencies please refer to this variable as a reference. This property cannot be changed, doing so forces recreation of the resource.
     */
    public readonly integrationId!: pulumi.Output<string>;
    /**
     * Name of the jdbc table that is to be connected to this table. Valid if the service integration id refers to a mysql or postgres service. This property cannot be changed, doing so forces recreation of the resource.
     */
    public readonly jdbcTable!: pulumi.Output<string | undefined>;
    /**
     * When used as a source, upsert Kafka connectors update values that use an existing key and delete values that are null. For sinks, the connector correspondingly writes update or delete messages in a compacted topic. If no matching key is found, the values are added as new entries. For more information, see the Apache Flink documentation The possible values are `kafka` and `upsert-kafka`. This property cannot be changed, doing so forces recreation of the resource.
     */
    public readonly kafkaConnectorType!: pulumi.Output<string | undefined>;
    /**
     * Defines an explicit list of physical columns from the table schema that configure the data type for the key format. This property cannot be changed, doing so forces recreation of the resource.
     */
    public readonly kafkaKeyFields!: pulumi.Output<string[] | undefined>;
    /**
     * Kafka Key Format The possible values are `avro`, `avro-confluent`, `debezium-avro-confluent`, `debezium-json` and `json`. This property cannot be changed, doing so forces recreation of the resource.
     */
    public readonly kafkaKeyFormat!: pulumi.Output<string | undefined>;
    /**
     * Startup mode The possible values are `earliest-offset`, `latest-offset`, `group-offsets` and `timestamp`. This property cannot be changed, doing so forces recreation of the resource.
     */
    public readonly kafkaStartupMode!: pulumi.Output<string | undefined>;
    /**
     * Name of the kafka topic that is to be connected to this table. Valid if the service integration id refers to a kafka service. This property cannot be changed, doing so forces recreation of the resource.
     */
    public readonly kafkaTopic!: pulumi.Output<string | undefined>;
    /**
     * Controls how key columns are handled in the message value. Select ALL to include the physical columns of the table schema in the message value. Select EXCEPT_KEY to exclude the physical columns of the table schema from the message value. This is the default for upsert Kafka connectors. The possible values are `[ALL EXCEPT_KEY]`. This property cannot be changed, doing so forces recreation of the resource.
     */
    public readonly kafkaValueFieldsInclude!: pulumi.Output<string | undefined>;
    /**
     * Kafka Value Format The possible values are `avro`, `avro-confluent`, `debezium-avro-confluent`, `debezium-json` and `json`. This property cannot be changed, doing so forces recreation of the resource.
     */
    public readonly kafkaValueFormat!: pulumi.Output<string | undefined>;
    /**
     * [LIKE](https://nightlies.apache.org/flink/flink-docs-master/docs/dev/table/sql/create/#like) statement for table creation. This property cannot be changed, doing so forces recreation of the resource.
     */
    public readonly likeOptions!: pulumi.Output<string | undefined>;
    /**
     * For an OpenSearch table, the OpenSearch index the table outputs to. This property cannot be changed, doing so forces recreation of the resource.
     */
    public readonly opensearchIndex!: pulumi.Output<string | undefined>;
    /**
     * Identifies the project this resource belongs to. To set up proper dependencies please refer to this variable as a reference. This property cannot be changed, doing so forces recreation of the resource.
     */
    public readonly project!: pulumi.Output<string>;
    /**
     * The SQL statement to create the table. This property cannot be changed, doing so forces recreation of the resource.
     */
    public readonly schemaSql!: pulumi.Output<string>;
    /**
     * Specifies the name of the service that this resource belongs to. To set up proper dependencies please refer to this variable as a reference. This property cannot be changed, doing so forces recreation of the resource.
     */
    public readonly serviceName!: pulumi.Output<string>;
    /**
     * The Table ID of the flink table in the flink service.
     */
    public /*out*/ readonly tableId!: pulumi.Output<string>;
    /**
     * Specifies the name of the table. This property cannot be changed, doing so forces recreation of the resource.
     */
    public readonly tableName!: pulumi.Output<string>;
    /**
     * Kafka upsert connector configuration.
     */
    public readonly upsertKafka!: pulumi.Output<outputs.FlinkJobTableUpsertKafka | undefined>;

    /**
     * Create a FlinkJobTable resource with the given unique name, arguments, and options.
     *
     * @param name The _unique_ name of the resource.
     * @param args The arguments to use to populate this resource's properties.
     * @param opts A bag of options that control this resource's behavior.
     */
    constructor(name: string, args: FlinkJobTableArgs, opts?: pulumi.CustomResourceOptions)
    constructor(name: string, argsOrState?: FlinkJobTableArgs | FlinkJobTableState, opts?: pulumi.CustomResourceOptions) {
        let resourceInputs: pulumi.Inputs = {};
        opts = opts || {};
        if (opts.id) {
            const state = argsOrState as FlinkJobTableState | undefined;
            resourceInputs["integrationId"] = state ? state.integrationId : undefined;
            resourceInputs["jdbcTable"] = state ? state.jdbcTable : undefined;
            resourceInputs["kafkaConnectorType"] = state ? state.kafkaConnectorType : undefined;
            resourceInputs["kafkaKeyFields"] = state ? state.kafkaKeyFields : undefined;
            resourceInputs["kafkaKeyFormat"] = state ? state.kafkaKeyFormat : undefined;
            resourceInputs["kafkaStartupMode"] = state ? state.kafkaStartupMode : undefined;
            resourceInputs["kafkaTopic"] = state ? state.kafkaTopic : undefined;
            resourceInputs["kafkaValueFieldsInclude"] = state ? state.kafkaValueFieldsInclude : undefined;
            resourceInputs["kafkaValueFormat"] = state ? state.kafkaValueFormat : undefined;
            resourceInputs["likeOptions"] = state ? state.likeOptions : undefined;
            resourceInputs["opensearchIndex"] = state ? state.opensearchIndex : undefined;
            resourceInputs["project"] = state ? state.project : undefined;
            resourceInputs["schemaSql"] = state ? state.schemaSql : undefined;
            resourceInputs["serviceName"] = state ? state.serviceName : undefined;
            resourceInputs["tableId"] = state ? state.tableId : undefined;
            resourceInputs["tableName"] = state ? state.tableName : undefined;
            resourceInputs["upsertKafka"] = state ? state.upsertKafka : undefined;
        } else {
            const args = argsOrState as FlinkJobTableArgs | undefined;
            if ((!args || args.integrationId === undefined) && !opts.urn) {
                throw new Error("Missing required property 'integrationId'");
            }
            if ((!args || args.project === undefined) && !opts.urn) {
                throw new Error("Missing required property 'project'");
            }
            if ((!args || args.schemaSql === undefined) && !opts.urn) {
                throw new Error("Missing required property 'schemaSql'");
            }
            if ((!args || args.serviceName === undefined) && !opts.urn) {
                throw new Error("Missing required property 'serviceName'");
            }
            if ((!args || args.tableName === undefined) && !opts.urn) {
                throw new Error("Missing required property 'tableName'");
            }
            resourceInputs["integrationId"] = args ? args.integrationId : undefined;
            resourceInputs["jdbcTable"] = args ? args.jdbcTable : undefined;
            resourceInputs["kafkaConnectorType"] = args ? args.kafkaConnectorType : undefined;
            resourceInputs["kafkaKeyFields"] = args ? args.kafkaKeyFields : undefined;
            resourceInputs["kafkaKeyFormat"] = args ? args.kafkaKeyFormat : undefined;
            resourceInputs["kafkaStartupMode"] = args ? args.kafkaStartupMode : undefined;
            resourceInputs["kafkaTopic"] = args ? args.kafkaTopic : undefined;
            resourceInputs["kafkaValueFieldsInclude"] = args ? args.kafkaValueFieldsInclude : undefined;
            resourceInputs["kafkaValueFormat"] = args ? args.kafkaValueFormat : undefined;
            resourceInputs["likeOptions"] = args ? args.likeOptions : undefined;
            resourceInputs["opensearchIndex"] = args ? args.opensearchIndex : undefined;
            resourceInputs["project"] = args ? args.project : undefined;
            resourceInputs["schemaSql"] = args ? args.schemaSql : undefined;
            resourceInputs["serviceName"] = args ? args.serviceName : undefined;
            resourceInputs["tableName"] = args ? args.tableName : undefined;
            resourceInputs["upsertKafka"] = args ? args.upsertKafka : undefined;
            resourceInputs["tableId"] = undefined /*out*/;
        }
        opts = pulumi.mergeOptions(utilities.resourceOptsDefaults(), opts);
        super(FlinkJobTable.__pulumiType, name, resourceInputs, opts);
    }
}

/**
 * Input properties used for looking up and filtering FlinkJobTable resources.
 */
export interface FlinkJobTableState {
    /**
     * The id of the service integration that is used with this table. It must have the service integration type `flink`. To set up proper dependencies please refer to this variable as a reference. This property cannot be changed, doing so forces recreation of the resource.
     */
    integrationId?: pulumi.Input<string>;
    /**
     * Name of the jdbc table that is to be connected to this table. Valid if the service integration id refers to a mysql or postgres service. This property cannot be changed, doing so forces recreation of the resource.
     */
    jdbcTable?: pulumi.Input<string>;
    /**
     * When used as a source, upsert Kafka connectors update values that use an existing key and delete values that are null. For sinks, the connector correspondingly writes update or delete messages in a compacted topic. If no matching key is found, the values are added as new entries. For more information, see the Apache Flink documentation The possible values are `kafka` and `upsert-kafka`. This property cannot be changed, doing so forces recreation of the resource.
     */
    kafkaConnectorType?: pulumi.Input<string>;
    /**
     * Defines an explicit list of physical columns from the table schema that configure the data type for the key format. This property cannot be changed, doing so forces recreation of the resource.
     */
    kafkaKeyFields?: pulumi.Input<pulumi.Input<string>[]>;
    /**
     * Kafka Key Format The possible values are `avro`, `avro-confluent`, `debezium-avro-confluent`, `debezium-json` and `json`. This property cannot be changed, doing so forces recreation of the resource.
     */
    kafkaKeyFormat?: pulumi.Input<string>;
    /**
     * Startup mode The possible values are `earliest-offset`, `latest-offset`, `group-offsets` and `timestamp`. This property cannot be changed, doing so forces recreation of the resource.
     */
    kafkaStartupMode?: pulumi.Input<string>;
    /**
     * Name of the kafka topic that is to be connected to this table. Valid if the service integration id refers to a kafka service. This property cannot be changed, doing so forces recreation of the resource.
     */
    kafkaTopic?: pulumi.Input<string>;
    /**
     * Controls how key columns are handled in the message value. Select ALL to include the physical columns of the table schema in the message value. Select EXCEPT_KEY to exclude the physical columns of the table schema from the message value. This is the default for upsert Kafka connectors. The possible values are `[ALL EXCEPT_KEY]`. This property cannot be changed, doing so forces recreation of the resource.
     */
    kafkaValueFieldsInclude?: pulumi.Input<string>;
    /**
     * Kafka Value Format The possible values are `avro`, `avro-confluent`, `debezium-avro-confluent`, `debezium-json` and `json`. This property cannot be changed, doing so forces recreation of the resource.
     */
    kafkaValueFormat?: pulumi.Input<string>;
    /**
     * [LIKE](https://nightlies.apache.org/flink/flink-docs-master/docs/dev/table/sql/create/#like) statement for table creation. This property cannot be changed, doing so forces recreation of the resource.
     */
    likeOptions?: pulumi.Input<string>;
    /**
     * For an OpenSearch table, the OpenSearch index the table outputs to. This property cannot be changed, doing so forces recreation of the resource.
     */
    opensearchIndex?: pulumi.Input<string>;
    /**
     * Identifies the project this resource belongs to. To set up proper dependencies please refer to this variable as a reference. This property cannot be changed, doing so forces recreation of the resource.
     */
    project?: pulumi.Input<string>;
    /**
     * The SQL statement to create the table. This property cannot be changed, doing so forces recreation of the resource.
     */
    schemaSql?: pulumi.Input<string>;
    /**
     * Specifies the name of the service that this resource belongs to. To set up proper dependencies please refer to this variable as a reference. This property cannot be changed, doing so forces recreation of the resource.
     */
    serviceName?: pulumi.Input<string>;
    /**
     * The Table ID of the flink table in the flink service.
     */
    tableId?: pulumi.Input<string>;
    /**
     * Specifies the name of the table. This property cannot be changed, doing so forces recreation of the resource.
     */
    tableName?: pulumi.Input<string>;
    /**
     * Kafka upsert connector configuration.
     */
    upsertKafka?: pulumi.Input<inputs.FlinkJobTableUpsertKafka>;
}

/**
 * The set of arguments for constructing a FlinkJobTable resource.
 */
export interface FlinkJobTableArgs {
    /**
     * The id of the service integration that is used with this table. It must have the service integration type `flink`. To set up proper dependencies please refer to this variable as a reference. This property cannot be changed, doing so forces recreation of the resource.
     */
    integrationId: pulumi.Input<string>;
    /**
     * Name of the jdbc table that is to be connected to this table. Valid if the service integration id refers to a mysql or postgres service. This property cannot be changed, doing so forces recreation of the resource.
     */
    jdbcTable?: pulumi.Input<string>;
    /**
     * When used as a source, upsert Kafka connectors update values that use an existing key and delete values that are null. For sinks, the connector correspondingly writes update or delete messages in a compacted topic. If no matching key is found, the values are added as new entries. For more information, see the Apache Flink documentation The possible values are `kafka` and `upsert-kafka`. This property cannot be changed, doing so forces recreation of the resource.
     */
    kafkaConnectorType?: pulumi.Input<string>;
    /**
     * Defines an explicit list of physical columns from the table schema that configure the data type for the key format. This property cannot be changed, doing so forces recreation of the resource.
     */
    kafkaKeyFields?: pulumi.Input<pulumi.Input<string>[]>;
    /**
     * Kafka Key Format The possible values are `avro`, `avro-confluent`, `debezium-avro-confluent`, `debezium-json` and `json`. This property cannot be changed, doing so forces recreation of the resource.
     */
    kafkaKeyFormat?: pulumi.Input<string>;
    /**
     * Startup mode The possible values are `earliest-offset`, `latest-offset`, `group-offsets` and `timestamp`. This property cannot be changed, doing so forces recreation of the resource.
     */
    kafkaStartupMode?: pulumi.Input<string>;
    /**
     * Name of the kafka topic that is to be connected to this table. Valid if the service integration id refers to a kafka service. This property cannot be changed, doing so forces recreation of the resource.
     */
    kafkaTopic?: pulumi.Input<string>;
    /**
     * Controls how key columns are handled in the message value. Select ALL to include the physical columns of the table schema in the message value. Select EXCEPT_KEY to exclude the physical columns of the table schema from the message value. This is the default for upsert Kafka connectors. The possible values are `[ALL EXCEPT_KEY]`. This property cannot be changed, doing so forces recreation of the resource.
     */
    kafkaValueFieldsInclude?: pulumi.Input<string>;
    /**
     * Kafka Value Format The possible values are `avro`, `avro-confluent`, `debezium-avro-confluent`, `debezium-json` and `json`. This property cannot be changed, doing so forces recreation of the resource.
     */
    kafkaValueFormat?: pulumi.Input<string>;
    /**
     * [LIKE](https://nightlies.apache.org/flink/flink-docs-master/docs/dev/table/sql/create/#like) statement for table creation. This property cannot be changed, doing so forces recreation of the resource.
     */
    likeOptions?: pulumi.Input<string>;
    /**
     * For an OpenSearch table, the OpenSearch index the table outputs to. This property cannot be changed, doing so forces recreation of the resource.
     */
    opensearchIndex?: pulumi.Input<string>;
    /**
     * Identifies the project this resource belongs to. To set up proper dependencies please refer to this variable as a reference. This property cannot be changed, doing so forces recreation of the resource.
     */
    project: pulumi.Input<string>;
    /**
     * The SQL statement to create the table. This property cannot be changed, doing so forces recreation of the resource.
     */
    schemaSql: pulumi.Input<string>;
    /**
     * Specifies the name of the service that this resource belongs to. To set up proper dependencies please refer to this variable as a reference. This property cannot be changed, doing so forces recreation of the resource.
     */
    serviceName: pulumi.Input<string>;
    /**
     * Specifies the name of the table. This property cannot be changed, doing so forces recreation of the resource.
     */
    tableName: pulumi.Input<string>;
    /**
     * Kafka upsert connector configuration.
     */
    upsertKafka?: pulumi.Input<inputs.FlinkJobTableUpsertKafka>;
}
