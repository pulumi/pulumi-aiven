// *** WARNING: this file was generated by the Pulumi Terraform Bridge (tfgen) Tool. ***
// *** Do not edit by hand unless you're certain you know what you are doing! ***

import * as pulumi from "@pulumi/pulumi";
import * as inputs from "../types/input";
import * as outputs from "../types/output";

export interface AccountAuthenticationSamlFieldMapping {
    /**
     * Field name for user email
     */
    email?: string;
    /**
     * Field name for user's first name
     */
    firstName?: string;
    /**
     * Field name for user's identity. This field must always exist in responses, and must be immutable and unique. Contents of this field are used to identify the user. Using user ID (such as unix user id) is highly recommended, as email address may change, requiring relinking user to Aiven user.
     */
    identity?: string;
    /**
     * Field name for user's last name
     */
    lastName?: string;
    /**
     * Field name for user's full name. If specified, first*name and last*name mappings are ignored
     */
    realName?: string;
}

export interface CassandraCassandra {
    /**
     * Cassandra server URIs.
     */
    uris: string[];
}

export interface CassandraCassandraUserConfig {
    /**
     * Additional Cloud Regions for Backup Replication.
     *
     * @deprecated This property is deprecated.
     */
    additionalBackupRegions?: string;
    /**
     * The hour of day (in UTC) when backup for the service is started. New backup is only started if previous backup has already completed. Example: `3`.
     */
    backupHour?: number;
    /**
     * The minute of an hour when backup for the service is started. New backup is only started if previous backup has already completed. Example: `30`.
     */
    backupMinute?: number;
    /**
     * Cassandra configuration values
     */
    cassandra?: outputs.CassandraCassandraUserConfigCassandra;
    /**
     * Enum: `3`, `4`, `4.1`, and newer. Cassandra version.
     */
    cassandraVersion?: string;
    /**
     * Allow incoming connections from CIDR address block, e.g. `10.20.0.0/16`
     */
    ipFilterObjects?: outputs.CassandraCassandraUserConfigIpFilterObject[];
    /**
     * Allow incoming connections from CIDR address block, e.g. `10.20.0.0/16`.
     */
    ipFilterStrings?: string[];
    /**
     * Allow incoming connections from CIDR address block, e.g. `10.20.0.0/16`.
     *
     * @deprecated Deprecated. Use `ipFilterString` instead.
     */
    ipFilters?: string[];
    /**
     * Sets the service into migration mode enabling the sstableloader utility to be used to upload Cassandra data files. Available only on service create.
     */
    migrateSstableloader?: boolean;
    /**
     * Allow access to selected service ports from private networks
     */
    privateAccess?: outputs.CassandraCassandraUserConfigPrivateAccess;
    /**
     * Name of another project to fork a service from. This has effect only when a new service is being created. Example: `anotherprojectname`.
     */
    projectToForkFrom?: string;
    /**
     * Allow access to selected service ports from the public Internet
     */
    publicAccess?: outputs.CassandraCassandraUserConfigPublicAccess;
    /**
     * Store logs for the service so that they are available in the HTTP API and console.
     */
    serviceLog?: boolean;
    /**
     * Name of another service to fork from. This has effect only when a new service is being created. Example: `anotherservicename`.
     */
    serviceToForkFrom?: string;
    /**
     * When bootstrapping, instead of creating a new Cassandra cluster try to join an existing one from another service. Can only be set on service creation. Example: `my-test-cassandra`.
     */
    serviceToJoinWith?: string;
    /**
     * Use static public IP addresses.
     */
    staticIps?: boolean;
}

export interface CassandraCassandraUserConfigCassandra {
    /**
     * Fail any multiple-partition batch exceeding this value. 50kb (10x warn threshold) by default. Example: `50`.
     */
    batchSizeFailThresholdInKb?: number;
    /**
     * Log a warning message on any multiple-partition batch size exceeding this value.5kb per batch by default.Caution should be taken on increasing the size of this thresholdas it can lead to node instability. Example: `5`.
     */
    batchSizeWarnThresholdInKb?: number;
    /**
     * Name of the datacenter to which nodes of this service belong. Can be set only when creating the service. Example: `my-service-google-west1`.
     */
    datacenter?: string;
    /**
     * How long the coordinator waits for read operations to complete before timing it out. 5 seconds by default. Example: `5000`.
     */
    readRequestTimeoutInMs?: number;
    /**
     * How long the coordinator waits for write requests to complete with at least one node in the local datacenter. 2 seconds by default. Example: `2000`.
     */
    writeRequestTimeoutInMs?: number;
}

export interface CassandraCassandraUserConfigIpFilterObject {
    /**
     * Description for IP filter list entry. Example: `Production service IP range`.
     */
    description?: string;
    /**
     * CIDR address block. Example: `10.20.0.0/16`.
     */
    network: string;
}

export interface CassandraCassandraUserConfigPrivateAccess {
    /**
     * Allow clients to connect to prometheus with a DNS name that always resolves to the service's private IP addresses. Only available in certain network locations.
     */
    prometheus?: boolean;
}

export interface CassandraCassandraUserConfigPublicAccess {
    /**
     * Allow clients to connect to prometheus from the public internet for service nodes that are in a project VPC or another type of private network.
     */
    prometheus?: boolean;
}

export interface CassandraComponent {
    /**
     * Service component name
     */
    component: string;
    /**
     * Connection info for connecting to the service component. This is a combination of host and port.
     */
    connectionUri: string;
    /**
     * Host name for connecting to the service component
     */
    host: string;
    /**
     * Kafka authentication method. This is a value specific to the 'kafka' service component
     */
    kafkaAuthenticationMethod: string;
    /**
     * Port number for connecting to the service component
     */
    port: number;
    /**
     * Network access route
     */
    route: string;
    /**
     * Whether the endpoint is encrypted or accepts plaintext. By default endpoints are always encrypted and this property is only included for service components they may disable encryption
     */
    ssl: boolean;
    /**
     * DNS usage name
     */
    usage: string;
}

export interface CassandraServiceIntegration {
    /**
     * Type of the service integration. The only supported value at the moment is `readReplica`
     */
    integrationType: string;
    /**
     * Name of the source service
     */
    sourceServiceName: string;
}

export interface CassandraTag {
    /**
     * Service tag key
     */
    key: string;
    /**
     * Service tag value
     */
    value: string;
}

export interface CassandraTechEmail {
    /**
     * An email address to contact for technical issues
     */
    email: string;
}

export interface ClickhouseClickhouse {
    /**
     * ClickHouse server URIs.
     */
    uris: string[];
}

export interface ClickhouseClickhouseUserConfig {
    /**
     * Additional Cloud Regions for Backup Replication.
     *
     * @deprecated This property is deprecated.
     */
    additionalBackupRegions?: string;
    /**
     * Allow incoming connections from CIDR address block, e.g. `10.20.0.0/16`
     */
    ipFilterObjects?: outputs.ClickhouseClickhouseUserConfigIpFilterObject[];
    /**
     * Allow incoming connections from CIDR address block, e.g. `10.20.0.0/16`.
     */
    ipFilterStrings?: string[];
    /**
     * Allow incoming connections from CIDR address block, e.g. `10.20.0.0/16`.
     *
     * @deprecated Deprecated. Use `ipFilterString` instead.
     */
    ipFilters?: string[];
    /**
     * Allow access to selected service ports from private networks
     */
    privateAccess?: outputs.ClickhouseClickhouseUserConfigPrivateAccess;
    /**
     * Allow access to selected service components through Privatelink
     */
    privatelinkAccess?: outputs.ClickhouseClickhouseUserConfigPrivatelinkAccess;
    /**
     * Name of another project to fork a service from. This has effect only when a new service is being created. Example: `anotherprojectname`.
     */
    projectToForkFrom?: string;
    /**
     * Allow access to selected service ports from the public Internet
     */
    publicAccess?: outputs.ClickhouseClickhouseUserConfigPublicAccess;
    /**
     * Store logs for the service so that they are available in the HTTP API and console.
     */
    serviceLog?: boolean;
    /**
     * Name of another service to fork from. This has effect only when a new service is being created. Example: `anotherservicename`.
     */
    serviceToForkFrom?: string;
    /**
     * Use static public IP addresses.
     */
    staticIps?: boolean;
}

export interface ClickhouseClickhouseUserConfigIpFilterObject {
    /**
     * Description for IP filter list entry. Example: `Production service IP range`.
     */
    description?: string;
    /**
     * CIDR address block. Example: `10.20.0.0/16`.
     */
    network: string;
}

export interface ClickhouseClickhouseUserConfigPrivateAccess {
    /**
     * Allow clients to connect to clickhouse with a DNS name that always resolves to the service's private IP addresses. Only available in certain network locations.
     */
    clickhouse?: boolean;
    /**
     * Allow clients to connect to clickhouseHttps with a DNS name that always resolves to the service's private IP addresses. Only available in certain network locations.
     */
    clickhouseHttps?: boolean;
    /**
     * Allow clients to connect to clickhouseMysql with a DNS name that always resolves to the service's private IP addresses. Only available in certain network locations.
     */
    clickhouseMysql?: boolean;
    /**
     * Allow clients to connect to prometheus with a DNS name that always resolves to the service's private IP addresses. Only available in certain network locations.
     */
    prometheus?: boolean;
}

export interface ClickhouseClickhouseUserConfigPrivatelinkAccess {
    /**
     * Enable clickhouse.
     */
    clickhouse?: boolean;
    /**
     * Enable clickhouse_https.
     */
    clickhouseHttps?: boolean;
    /**
     * Enable clickhouse_mysql.
     */
    clickhouseMysql?: boolean;
    /**
     * Enable prometheus.
     */
    prometheus?: boolean;
}

export interface ClickhouseClickhouseUserConfigPublicAccess {
    /**
     * Allow clients to connect to clickhouse from the public internet for service nodes that are in a project VPC or another type of private network.
     */
    clickhouse?: boolean;
    /**
     * Allow clients to connect to clickhouseHttps from the public internet for service nodes that are in a project VPC or another type of private network.
     */
    clickhouseHttps?: boolean;
    /**
     * Allow clients to connect to clickhouseMysql from the public internet for service nodes that are in a project VPC or another type of private network.
     */
    clickhouseMysql?: boolean;
    /**
     * Allow clients to connect to prometheus from the public internet for service nodes that are in a project VPC or another type of private network.
     */
    prometheus?: boolean;
}

export interface ClickhouseComponent {
    /**
     * Service component name
     */
    component: string;
    /**
     * Connection info for connecting to the service component. This is a combination of host and port.
     */
    connectionUri: string;
    /**
     * Host name for connecting to the service component
     */
    host: string;
    /**
     * Kafka authentication method. This is a value specific to the 'kafka' service component
     */
    kafkaAuthenticationMethod: string;
    /**
     * Port number for connecting to the service component
     */
    port: number;
    /**
     * Network access route
     */
    route: string;
    /**
     * Whether the endpoint is encrypted or accepts plaintext. By default endpoints are always encrypted and this property is only included for service components they may disable encryption
     */
    ssl: boolean;
    /**
     * DNS usage name
     */
    usage: string;
}

export interface ClickhouseGrantPrivilegeGrant {
    /**
     * The column to grant access to. Changing this property forces recreation of the resource.
     */
    column?: string;
    /**
     * The database to grant access to. To set up proper dependencies please refer to this variable as a reference. Changing this property forces recreation of the resource.
     */
    database: string;
    /**
     * The privileges to grant. For example: `INSERT`, `SELECT`, `CREATE TABLE`. A complete list is available in the [ClickHouse documentation](https://clickhouse.com/docs/en/sql-reference/statements/grant). Changing this property forces recreation of the resource.
     */
    privilege?: string;
    /**
     * The table to grant access to. Changing this property forces recreation of the resource.
     */
    table?: string;
    /**
     * Allow grantees to grant their privileges to other grantees. Changing this property forces recreation of the resource.
     */
    withGrant?: boolean;
}

export interface ClickhouseGrantRoleGrant {
    /**
     * The roles to grant. To set up proper dependencies please refer to this variable as a reference. Changing this property forces recreation of the resource.
     */
    role?: string;
}

export interface ClickhouseServiceIntegration {
    /**
     * Type of the service integration. Supported integrations are `clickhouseKafka` and `clickhousePostgresql`.
     */
    integrationType: string;
    /**
     * Name of the source service.
     */
    sourceServiceName: string;
}

export interface ClickhouseTag {
    /**
     * Service tag key
     */
    key: string;
    /**
     * Service tag value
     */
    value: string;
}

export interface ClickhouseTechEmail {
    /**
     * An email address to contact for technical issues
     */
    email: string;
}

export interface DragonflyComponent {
    /**
     * Service component name
     */
    component: string;
    /**
     * Connection info for connecting to the service component. This is a combination of host and port.
     */
    connectionUri: string;
    /**
     * Host name for connecting to the service component
     */
    host: string;
    /**
     * Kafka authentication method. This is a value specific to the 'kafka' service component
     */
    kafkaAuthenticationMethod: string;
    /**
     * Port number for connecting to the service component
     */
    port: number;
    /**
     * Network access route
     */
    route: string;
    /**
     * Whether the endpoint is encrypted or accepts plaintext. By default endpoints are always encrypted and this property is only included for service components they may disable encryption
     */
    ssl: boolean;
    /**
     * DNS usage name
     */
    usage: string;
}

export interface DragonflyDragonfly {
    /**
     * Dragonfly password.
     */
    password: string;
    /**
     * Dragonfly replica server URI.
     */
    replicaUri: string;
    /**
     * Dragonfly slave server URIs.
     */
    slaveUris: string[];
    /**
     * Dragonfly server URIs.
     */
    uris: string[];
}

export interface DragonflyDragonflyUserConfig {
    /**
     * Evict entries when getting close to maxmemory limit. Default: `false`.
     */
    cacheMode?: boolean;
    /**
     * Enum: `off`, `rdb`, `dfs`. When persistence is `rdb` or `dfs`, Dragonfly does RDB or DFS dumps every 10 minutes. Dumps are done according to the backup schedule for backup purposes. When persistence is `off`, no RDB/DFS dumps or backups are done, so data can be lost at any moment if the service is restarted for any reason, or if the service is powered off. Also, the service can't be forked.
     */
    dragonflyPersistence?: string;
    /**
     * Require SSL to access Dragonfly. Default: `true`.
     */
    dragonflySsl?: boolean;
    /**
     * Allow incoming connections from CIDR address block, e.g. `10.20.0.0/16`
     */
    ipFilterObjects?: outputs.DragonflyDragonflyUserConfigIpFilterObject[];
    /**
     * Allow incoming connections from CIDR address block, e.g. `10.20.0.0/16`.
     */
    ipFilterStrings?: string[];
    /**
     * Allow incoming connections from CIDR address block, e.g. `10.20.0.0/16`.
     *
     * @deprecated Deprecated. Use `ipFilterString` instead.
     */
    ipFilters?: string[];
    /**
     * Migrate data from existing server
     */
    migration?: outputs.DragonflyDragonflyUserConfigMigration;
    /**
     * Allow access to selected service ports from private networks
     */
    privateAccess?: outputs.DragonflyDragonflyUserConfigPrivateAccess;
    /**
     * Allow access to selected service components through Privatelink
     */
    privatelinkAccess?: outputs.DragonflyDragonflyUserConfigPrivatelinkAccess;
    /**
     * Name of another project to fork a service from. This has effect only when a new service is being created. Example: `anotherprojectname`.
     */
    projectToForkFrom?: string;
    /**
     * Allow access to selected service ports from the public Internet
     */
    publicAccess?: outputs.DragonflyDragonflyUserConfigPublicAccess;
    /**
     * Name of the basebackup to restore in forked service. Example: `backup-20191112t091354293891z`.
     */
    recoveryBasebackupName?: string;
    /**
     * Store logs for the service so that they are available in the HTTP API and console.
     */
    serviceLog?: boolean;
    /**
     * Name of another service to fork from. This has effect only when a new service is being created. Example: `anotherservicename`.
     */
    serviceToForkFrom?: string;
    /**
     * Use static public IP addresses.
     */
    staticIps?: boolean;
}

export interface DragonflyDragonflyUserConfigIpFilterObject {
    /**
     * Description for IP filter list entry. Example: `Production service IP range`.
     */
    description?: string;
    /**
     * CIDR address block. Example: `10.20.0.0/16`.
     */
    network: string;
}

export interface DragonflyDragonflyUserConfigMigration {
    /**
     * Database name for bootstrapping the initial connection. Example: `defaultdb`.
     */
    dbname?: string;
    /**
     * Hostname or IP address of the server where to migrate data from. Example: `my.server.com`.
     */
    host: string;
    /**
     * Comma-separated list of databases, which should be ignored during migration (supported by MySQL and PostgreSQL only at the moment). Example: `db1,db2`.
     */
    ignoreDbs?: string;
    /**
     * Comma-separated list of database roles, which should be ignored during migration (supported by PostgreSQL only at the moment). Example: `role1,role2`.
     */
    ignoreRoles?: string;
    /**
     * Enum: `dump`, `replication`. The migration method to be used (currently supported only by Redis, Dragonfly, MySQL and PostgreSQL service types).
     */
    method?: string;
    /**
     * Password for authentication with the server where to migrate data from. Example: `jjKk45Nnd`.
     */
    password?: string;
    /**
     * Port number of the server where to migrate data from. Example: `1234`.
     */
    port: number;
    /**
     * The server where to migrate data from is secured with SSL. Default: `true`.
     */
    ssl?: boolean;
    /**
     * User name for authentication with the server where to migrate data from. Example: `myname`.
     */
    username?: string;
}

export interface DragonflyDragonflyUserConfigPrivateAccess {
    /**
     * Allow clients to connect to dragonfly with a DNS name that always resolves to the service's private IP addresses. Only available in certain network locations.
     */
    dragonfly?: boolean;
    /**
     * Allow clients to connect to prometheus with a DNS name that always resolves to the service's private IP addresses. Only available in certain network locations.
     */
    prometheus?: boolean;
}

export interface DragonflyDragonflyUserConfigPrivatelinkAccess {
    /**
     * Enable dragonfly.
     */
    dragonfly?: boolean;
    /**
     * Enable prometheus.
     */
    prometheus?: boolean;
}

export interface DragonflyDragonflyUserConfigPublicAccess {
    /**
     * Allow clients to connect to dragonfly from the public internet for service nodes that are in a project VPC or another type of private network.
     */
    dragonfly?: boolean;
    /**
     * Allow clients to connect to prometheus from the public internet for service nodes that are in a project VPC or another type of private network.
     */
    prometheus?: boolean;
}

export interface DragonflyServiceIntegration {
    /**
     * Type of the service integration. The only supported value at the moment is `readReplica`
     */
    integrationType: string;
    /**
     * Name of the source service
     */
    sourceServiceName: string;
}

export interface DragonflyTag {
    /**
     * Service tag key
     */
    key: string;
    /**
     * Service tag value
     */
    value: string;
}

export interface DragonflyTechEmail {
    /**
     * An email address to contact for technical issues
     */
    email: string;
}

export interface FlinkApplicationVersionSink {
    /**
     * The CREATE TABLE statement
     */
    createTable: string;
    /**
     * The integration ID
     */
    integrationId?: string;
}

export interface FlinkApplicationVersionSource {
    /**
     * The CREATE TABLE statement
     */
    createTable: string;
    /**
     * The integration ID
     */
    integrationId?: string;
}

export interface FlinkComponent {
    /**
     * Service component name
     */
    component: string;
    /**
     * Connection info for connecting to the service component. This is a combination of host and port.
     */
    connectionUri: string;
    /**
     * Host name for connecting to the service component
     */
    host: string;
    /**
     * Kafka authentication method. This is a value specific to the 'kafka' service component
     */
    kafkaAuthenticationMethod: string;
    /**
     * Port number for connecting to the service component
     */
    port: number;
    /**
     * Network access route
     */
    route: string;
    /**
     * Whether the endpoint is encrypted or accepts plaintext. By default endpoints are always encrypted and this property is only included for service components they may disable encryption
     */
    ssl: boolean;
    /**
     * DNS usage name
     */
    usage: string;
}

export interface FlinkFlink {
    /**
     * The host and port of a Flink server.
     */
    hostPorts: string[];
}

export interface FlinkFlinkUserConfig {
    /**
     * Additional Cloud Regions for Backup Replication.
     *
     * @deprecated This property is deprecated.
     */
    additionalBackupRegions?: string;
    /**
     * Enum: `1.16`, `1.19`, and newer. Flink major version.
     */
    flinkVersion?: string;
    /**
     * Allow incoming connections from CIDR address block, e.g. `10.20.0.0/16`
     */
    ipFilterObjects?: outputs.FlinkFlinkUserConfigIpFilterObject[];
    /**
     * Allow incoming connections from CIDR address block, e.g. `10.20.0.0/16`.
     */
    ipFilterStrings?: string[];
    /**
     * Allow incoming connections from CIDR address block, e.g. `10.20.0.0/16`.
     *
     * @deprecated Deprecated. Use `ipFilterString` instead.
     */
    ipFilters?: string[];
    /**
     * Task slots per node. For a 3 node plan, total number of task slots is 3x this value. Example: `1`.
     */
    numberOfTaskSlots?: number;
    /**
     * Allow access to selected service components through Privatelink
     */
    privatelinkAccess?: outputs.FlinkFlinkUserConfigPrivatelinkAccess;
    /**
     * Store logs for the service so that they are available in the HTTP API and console.
     */
    serviceLog?: boolean;
    /**
     * Use static public IP addresses.
     */
    staticIps?: boolean;
}

export interface FlinkFlinkUserConfigIpFilterObject {
    /**
     * Description for IP filter list entry. Example: `Production service IP range`.
     */
    description?: string;
    /**
     * CIDR address block. Example: `10.20.0.0/16`.
     */
    network: string;
}

export interface FlinkFlinkUserConfigPrivatelinkAccess {
    /**
     * Enable flink.
     */
    flink?: boolean;
    /**
     * Enable prometheus.
     */
    prometheus?: boolean;
}

export interface FlinkServiceIntegration {
    /**
     * Type of the service integration. The only supported value at the moment is `readReplica`
     */
    integrationType: string;
    /**
     * Name of the source service
     */
    sourceServiceName: string;
}

export interface FlinkTag {
    /**
     * Service tag key
     */
    key: string;
    /**
     * Service tag value
     */
    value: string;
}

export interface FlinkTechEmail {
    /**
     * An email address to contact for technical issues
     */
    email: string;
}

export interface GetAccountAuthenticationSamlFieldMapping {
    /**
     * Field name for user email
     */
    email?: string;
    /**
     * Field name for user's first name
     */
    firstName?: string;
    /**
     * Field name for user's identity. This field must always exist in responses, and must be immutable and unique. Contents of this field are used to identify the user. Using user ID (such as unix user id) is highly recommended, as email address may change, requiring relinking user to Aiven user.
     */
    identity?: string;
    /**
     * Field name for user's last name
     */
    lastName?: string;
    /**
     * Field name for user's full name. If specified, firstName and lastName mappings are ignored
     */
    realName?: string;
}

export interface GetCassandaCassandra {
    /**
     * Cassandra server URIs.
     */
    uris: string[];
}

export interface GetCassandaCassandraUserConfig {
    /**
     * Additional Cloud Regions for Backup Replication.
     *
     * @deprecated This property is deprecated.
     */
    additionalBackupRegions?: string;
    /**
     * The hour of day (in UTC) when backup for the service is started. New backup is only started if previous backup has already completed. Example: `3`.
     */
    backupHour?: number;
    /**
     * The minute of an hour when backup for the service is started. New backup is only started if previous backup has already completed. Example: `30`.
     */
    backupMinute?: number;
    /**
     * Cassandra configuration values
     */
    cassandra?: outputs.GetCassandaCassandraUserConfigCassandra;
    /**
     * Enum: `3`, `4`, `4.1`, and newer. Cassandra version.
     */
    cassandraVersion?: string;
    /**
     * Allow incoming connections from CIDR address block, e.g. `10.20.0.0/16`
     */
    ipFilterObjects?: outputs.GetCassandaCassandraUserConfigIpFilterObject[];
    /**
     * Allow incoming connections from CIDR address block, e.g. `10.20.0.0/16`.
     */
    ipFilterStrings?: string[];
    /**
     * Allow incoming connections from CIDR address block, e.g. `10.20.0.0/16`.
     *
     * @deprecated Deprecated. Use `ipFilterString` instead.
     */
    ipFilters?: string[];
    /**
     * Sets the service into migration mode enabling the sstableloader utility to be used to upload Cassandra data files. Available only on service create.
     */
    migrateSstableloader?: boolean;
    /**
     * Allow access to selected service ports from private networks
     */
    privateAccess?: outputs.GetCassandaCassandraUserConfigPrivateAccess;
    /**
     * Name of another project to fork a service from. This has effect only when a new service is being created. Example: `anotherprojectname`.
     */
    projectToForkFrom?: string;
    /**
     * Allow access to selected service ports from the public Internet
     */
    publicAccess?: outputs.GetCassandaCassandraUserConfigPublicAccess;
    /**
     * Store logs for the service so that they are available in the HTTP API and console.
     */
    serviceLog?: boolean;
    /**
     * Name of another service to fork from. This has effect only when a new service is being created. Example: `anotherservicename`.
     */
    serviceToForkFrom?: string;
    /**
     * When bootstrapping, instead of creating a new Cassandra cluster try to join an existing one from another service. Can only be set on service creation. Example: `my-test-cassandra`.
     */
    serviceToJoinWith?: string;
    /**
     * Use static public IP addresses.
     */
    staticIps?: boolean;
}

export interface GetCassandaCassandraUserConfigCassandra {
    /**
     * Fail any multiple-partition batch exceeding this value. 50kb (10x warn threshold) by default. Example: `50`.
     */
    batchSizeFailThresholdInKb?: number;
    /**
     * Log a warning message on any multiple-partition batch size exceeding this value.5kb per batch by default.Caution should be taken on increasing the size of this thresholdas it can lead to node instability. Example: `5`.
     */
    batchSizeWarnThresholdInKb?: number;
    /**
     * Name of the datacenter to which nodes of this service belong. Can be set only when creating the service. Example: `my-service-google-west1`.
     */
    datacenter?: string;
    /**
     * How long the coordinator waits for read operations to complete before timing it out. 5 seconds by default. Example: `5000`.
     */
    readRequestTimeoutInMs?: number;
    /**
     * How long the coordinator waits for write requests to complete with at least one node in the local datacenter. 2 seconds by default. Example: `2000`.
     */
    writeRequestTimeoutInMs?: number;
}

export interface GetCassandaCassandraUserConfigIpFilterObject {
    /**
     * Description for IP filter list entry. Example: `Production service IP range`.
     */
    description?: string;
    /**
     * CIDR address block. Example: `10.20.0.0/16`.
     */
    network: string;
}

export interface GetCassandaCassandraUserConfigPrivateAccess {
    /**
     * Allow clients to connect to prometheus with a DNS name that always resolves to the service's private IP addresses. Only available in certain network locations.
     */
    prometheus?: boolean;
}

export interface GetCassandaCassandraUserConfigPublicAccess {
    /**
     * Allow clients to connect to prometheus from the public internet for service nodes that are in a project VPC or another type of private network.
     */
    prometheus?: boolean;
}

export interface GetCassandaComponent {
    /**
     * Service component name
     */
    component: string;
    /**
     * Connection info for connecting to the service component. This is a combination of host and port.
     */
    connectionUri: string;
    /**
     * Host name for connecting to the service component
     */
    host: string;
    /**
     * Kafka authentication method. This is a value specific to the 'kafka' service component
     */
    kafkaAuthenticationMethod: string;
    /**
     * Port number for connecting to the service component
     */
    port: number;
    /**
     * Network access route
     */
    route: string;
    /**
     * Whether the endpoint is encrypted or accepts plaintext. By default endpoints are always encrypted and this property is only included for service components they may disable encryption
     */
    ssl: boolean;
    /**
     * DNS usage name
     */
    usage: string;
}

export interface GetCassandaServiceIntegration {
    /**
     * Type of the service integration. The only supported value at the moment is `readReplica`
     */
    integrationType: string;
    /**
     * Name of the source service
     */
    sourceServiceName: string;
}

export interface GetCassandaTag {
    /**
     * Service tag key
     */
    key: string;
    /**
     * Service tag value
     */
    value: string;
}

export interface GetCassandaTechEmail {
    /**
     * An email address to contact for technical issues
     */
    email: string;
}

export interface GetCassandraCassandra {
    /**
     * Cassandra server URIs.
     */
    uris: string[];
}

export interface GetCassandraCassandraUserConfig {
    /**
     * Additional Cloud Regions for Backup Replication.
     *
     * @deprecated This property is deprecated.
     */
    additionalBackupRegions?: string;
    /**
     * The hour of day (in UTC) when backup for the service is started. New backup is only started if previous backup has already completed. Example: `3`.
     */
    backupHour?: number;
    /**
     * The minute of an hour when backup for the service is started. New backup is only started if previous backup has already completed. Example: `30`.
     */
    backupMinute?: number;
    /**
     * Cassandra configuration values
     */
    cassandra?: outputs.GetCassandraCassandraUserConfigCassandra;
    /**
     * Enum: `3`, `4`, `4.1`, and newer. Cassandra version.
     */
    cassandraVersion?: string;
    /**
     * Allow incoming connections from CIDR address block, e.g. `10.20.0.0/16`
     */
    ipFilterObjects?: outputs.GetCassandraCassandraUserConfigIpFilterObject[];
    /**
     * Allow incoming connections from CIDR address block, e.g. `10.20.0.0/16`.
     */
    ipFilterStrings?: string[];
    /**
     * Allow incoming connections from CIDR address block, e.g. `10.20.0.0/16`.
     *
     * @deprecated Deprecated. Use `ipFilterString` instead.
     */
    ipFilters?: string[];
    /**
     * Sets the service into migration mode enabling the sstableloader utility to be used to upload Cassandra data files. Available only on service create.
     */
    migrateSstableloader?: boolean;
    /**
     * Allow access to selected service ports from private networks
     */
    privateAccess?: outputs.GetCassandraCassandraUserConfigPrivateAccess;
    /**
     * Name of another project to fork a service from. This has effect only when a new service is being created. Example: `anotherprojectname`.
     */
    projectToForkFrom?: string;
    /**
     * Allow access to selected service ports from the public Internet
     */
    publicAccess?: outputs.GetCassandraCassandraUserConfigPublicAccess;
    /**
     * Store logs for the service so that they are available in the HTTP API and console.
     */
    serviceLog?: boolean;
    /**
     * Name of another service to fork from. This has effect only when a new service is being created. Example: `anotherservicename`.
     */
    serviceToForkFrom?: string;
    /**
     * When bootstrapping, instead of creating a new Cassandra cluster try to join an existing one from another service. Can only be set on service creation. Example: `my-test-cassandra`.
     */
    serviceToJoinWith?: string;
    /**
     * Use static public IP addresses.
     */
    staticIps?: boolean;
}

export interface GetCassandraCassandraUserConfigCassandra {
    /**
     * Fail any multiple-partition batch exceeding this value. 50kb (10x warn threshold) by default. Example: `50`.
     */
    batchSizeFailThresholdInKb?: number;
    /**
     * Log a warning message on any multiple-partition batch size exceeding this value.5kb per batch by default.Caution should be taken on increasing the size of this thresholdas it can lead to node instability. Example: `5`.
     */
    batchSizeWarnThresholdInKb?: number;
    /**
     * Name of the datacenter to which nodes of this service belong. Can be set only when creating the service. Example: `my-service-google-west1`.
     */
    datacenter?: string;
    /**
     * How long the coordinator waits for read operations to complete before timing it out. 5 seconds by default. Example: `5000`.
     */
    readRequestTimeoutInMs?: number;
    /**
     * How long the coordinator waits for write requests to complete with at least one node in the local datacenter. 2 seconds by default. Example: `2000`.
     */
    writeRequestTimeoutInMs?: number;
}

export interface GetCassandraCassandraUserConfigIpFilterObject {
    /**
     * Description for IP filter list entry. Example: `Production service IP range`.
     */
    description?: string;
    /**
     * CIDR address block. Example: `10.20.0.0/16`.
     */
    network: string;
}

export interface GetCassandraCassandraUserConfigPrivateAccess {
    /**
     * Allow clients to connect to prometheus with a DNS name that always resolves to the service's private IP addresses. Only available in certain network locations.
     */
    prometheus?: boolean;
}

export interface GetCassandraCassandraUserConfigPublicAccess {
    /**
     * Allow clients to connect to prometheus from the public internet for service nodes that are in a project VPC or another type of private network.
     */
    prometheus?: boolean;
}

export interface GetCassandraComponent {
    /**
     * Service component name
     */
    component: string;
    /**
     * Connection info for connecting to the service component. This is a combination of host and port.
     */
    connectionUri: string;
    /**
     * Host name for connecting to the service component
     */
    host: string;
    /**
     * Kafka authentication method. This is a value specific to the 'kafka' service component
     */
    kafkaAuthenticationMethod: string;
    /**
     * Port number for connecting to the service component
     */
    port: number;
    /**
     * Network access route
     */
    route: string;
    /**
     * Whether the endpoint is encrypted or accepts plaintext. By default endpoints are always encrypted and this property is only included for service components they may disable encryption
     */
    ssl: boolean;
    /**
     * DNS usage name
     */
    usage: string;
}

export interface GetCassandraServiceIntegration {
    /**
     * Type of the service integration. The only supported value at the moment is `readReplica`
     */
    integrationType: string;
    /**
     * Name of the source service
     */
    sourceServiceName: string;
}

export interface GetCassandraTag {
    /**
     * Service tag key
     */
    key: string;
    /**
     * Service tag value
     */
    value: string;
}

export interface GetCassandraTechEmail {
    /**
     * An email address to contact for technical issues
     */
    email: string;
}

export interface GetClickhouseClickhouse {
    /**
     * ClickHouse server URIs.
     */
    uris: string[];
}

export interface GetClickhouseClickhouseUserConfig {
    /**
     * Additional Cloud Regions for Backup Replication.
     *
     * @deprecated This property is deprecated.
     */
    additionalBackupRegions?: string;
    /**
     * Allow incoming connections from CIDR address block, e.g. `10.20.0.0/16`
     */
    ipFilterObjects?: outputs.GetClickhouseClickhouseUserConfigIpFilterObject[];
    /**
     * Allow incoming connections from CIDR address block, e.g. `10.20.0.0/16`.
     */
    ipFilterStrings?: string[];
    /**
     * Allow incoming connections from CIDR address block, e.g. `10.20.0.0/16`.
     *
     * @deprecated Deprecated. Use `ipFilterString` instead.
     */
    ipFilters?: string[];
    /**
     * Allow access to selected service ports from private networks
     */
    privateAccess?: outputs.GetClickhouseClickhouseUserConfigPrivateAccess;
    /**
     * Allow access to selected service components through Privatelink
     */
    privatelinkAccess?: outputs.GetClickhouseClickhouseUserConfigPrivatelinkAccess;
    /**
     * Name of another project to fork a service from. This has effect only when a new service is being created. Example: `anotherprojectname`.
     */
    projectToForkFrom?: string;
    /**
     * Allow access to selected service ports from the public Internet
     */
    publicAccess?: outputs.GetClickhouseClickhouseUserConfigPublicAccess;
    /**
     * Store logs for the service so that they are available in the HTTP API and console.
     */
    serviceLog?: boolean;
    /**
     * Name of another service to fork from. This has effect only when a new service is being created. Example: `anotherservicename`.
     */
    serviceToForkFrom?: string;
    /**
     * Use static public IP addresses.
     */
    staticIps?: boolean;
}

export interface GetClickhouseClickhouseUserConfigIpFilterObject {
    /**
     * Description for IP filter list entry. Example: `Production service IP range`.
     */
    description?: string;
    /**
     * CIDR address block. Example: `10.20.0.0/16`.
     */
    network: string;
}

export interface GetClickhouseClickhouseUserConfigPrivateAccess {
    /**
     * Allow clients to connect to clickhouse with a DNS name that always resolves to the service's private IP addresses. Only available in certain network locations.
     */
    clickhouse?: boolean;
    /**
     * Allow clients to connect to clickhouseHttps with a DNS name that always resolves to the service's private IP addresses. Only available in certain network locations.
     */
    clickhouseHttps?: boolean;
    /**
     * Allow clients to connect to clickhouseMysql with a DNS name that always resolves to the service's private IP addresses. Only available in certain network locations.
     */
    clickhouseMysql?: boolean;
    /**
     * Allow clients to connect to prometheus with a DNS name that always resolves to the service's private IP addresses. Only available in certain network locations.
     */
    prometheus?: boolean;
}

export interface GetClickhouseClickhouseUserConfigPrivatelinkAccess {
    /**
     * Enable clickhouse.
     */
    clickhouse?: boolean;
    /**
     * Enable clickhouse_https.
     */
    clickhouseHttps?: boolean;
    /**
     * Enable clickhouse_mysql.
     */
    clickhouseMysql?: boolean;
    /**
     * Enable prometheus.
     */
    prometheus?: boolean;
}

export interface GetClickhouseClickhouseUserConfigPublicAccess {
    /**
     * Allow clients to connect to clickhouse from the public internet for service nodes that are in a project VPC or another type of private network.
     */
    clickhouse?: boolean;
    /**
     * Allow clients to connect to clickhouseHttps from the public internet for service nodes that are in a project VPC or another type of private network.
     */
    clickhouseHttps?: boolean;
    /**
     * Allow clients to connect to clickhouseMysql from the public internet for service nodes that are in a project VPC or another type of private network.
     */
    clickhouseMysql?: boolean;
    /**
     * Allow clients to connect to prometheus from the public internet for service nodes that are in a project VPC or another type of private network.
     */
    prometheus?: boolean;
}

export interface GetClickhouseComponent {
    /**
     * Service component name
     */
    component: string;
    /**
     * Connection info for connecting to the service component. This is a combination of host and port.
     */
    connectionUri: string;
    /**
     * Host name for connecting to the service component
     */
    host: string;
    /**
     * Kafka authentication method. This is a value specific to the 'kafka' service component
     */
    kafkaAuthenticationMethod: string;
    /**
     * Port number for connecting to the service component
     */
    port: number;
    /**
     * Network access route
     */
    route: string;
    /**
     * Whether the endpoint is encrypted or accepts plaintext. By default endpoints are always encrypted and this property is only included for service components they may disable encryption
     */
    ssl: boolean;
    /**
     * DNS usage name
     */
    usage: string;
}

export interface GetClickhouseServiceIntegration {
    /**
     * Type of the service integration. Supported integrations are `clickhouseKafka` and `clickhousePostgresql`.
     */
    integrationType: string;
    /**
     * Name of the source service.
     */
    sourceServiceName: string;
}

export interface GetClickhouseTag {
    /**
     * Service tag key
     */
    key: string;
    /**
     * Service tag value
     */
    value: string;
}

export interface GetClickhouseTechEmail {
    /**
     * An email address to contact for technical issues
     */
    email: string;
}

export interface GetDragonflyComponent {
    /**
     * Service component name
     */
    component: string;
    /**
     * Connection info for connecting to the service component. This is a combination of host and port.
     */
    connectionUri: string;
    /**
     * Host name for connecting to the service component
     */
    host: string;
    /**
     * Kafka authentication method. This is a value specific to the 'kafka' service component
     */
    kafkaAuthenticationMethod: string;
    /**
     * Port number for connecting to the service component
     */
    port: number;
    /**
     * Network access route
     */
    route: string;
    /**
     * Whether the endpoint is encrypted or accepts plaintext. By default endpoints are always encrypted and this property is only included for service components they may disable encryption
     */
    ssl: boolean;
    /**
     * DNS usage name
     */
    usage: string;
}

export interface GetDragonflyDragonfly {
    /**
     * Dragonfly password.
     */
    password: string;
    /**
     * Dragonfly replica server URI.
     */
    replicaUri: string;
    /**
     * Dragonfly slave server URIs.
     */
    slaveUris: string[];
    /**
     * Dragonfly server URIs.
     */
    uris: string[];
}

export interface GetDragonflyDragonflyUserConfig {
    /**
     * Evict entries when getting close to maxmemory limit. Default: `false`.
     */
    cacheMode?: boolean;
    /**
     * Enum: `off`, `rdb`, `dfs`. When persistence is `rdb` or `dfs`, Dragonfly does RDB or DFS dumps every 10 minutes. Dumps are done according to the backup schedule for backup purposes. When persistence is `off`, no RDB/DFS dumps or backups are done, so data can be lost at any moment if the service is restarted for any reason, or if the service is powered off. Also, the service can't be forked.
     */
    dragonflyPersistence?: string;
    /**
     * Require SSL to access Dragonfly. Default: `true`.
     */
    dragonflySsl?: boolean;
    /**
     * Allow incoming connections from CIDR address block, e.g. `10.20.0.0/16`
     */
    ipFilterObjects?: outputs.GetDragonflyDragonflyUserConfigIpFilterObject[];
    /**
     * Allow incoming connections from CIDR address block, e.g. `10.20.0.0/16`.
     */
    ipFilterStrings?: string[];
    /**
     * Allow incoming connections from CIDR address block, e.g. `10.20.0.0/16`.
     *
     * @deprecated Deprecated. Use `ipFilterString` instead.
     */
    ipFilters?: string[];
    /**
     * Migrate data from existing server
     */
    migration?: outputs.GetDragonflyDragonflyUserConfigMigration;
    /**
     * Allow access to selected service ports from private networks
     */
    privateAccess?: outputs.GetDragonflyDragonflyUserConfigPrivateAccess;
    /**
     * Allow access to selected service components through Privatelink
     */
    privatelinkAccess?: outputs.GetDragonflyDragonflyUserConfigPrivatelinkAccess;
    /**
     * Name of another project to fork a service from. This has effect only when a new service is being created. Example: `anotherprojectname`.
     */
    projectToForkFrom?: string;
    /**
     * Allow access to selected service ports from the public Internet
     */
    publicAccess?: outputs.GetDragonflyDragonflyUserConfigPublicAccess;
    /**
     * Name of the basebackup to restore in forked service. Example: `backup-20191112t091354293891z`.
     */
    recoveryBasebackupName?: string;
    /**
     * Store logs for the service so that they are available in the HTTP API and console.
     */
    serviceLog?: boolean;
    /**
     * Name of another service to fork from. This has effect only when a new service is being created. Example: `anotherservicename`.
     */
    serviceToForkFrom?: string;
    /**
     * Use static public IP addresses.
     */
    staticIps?: boolean;
}

export interface GetDragonflyDragonflyUserConfigIpFilterObject {
    /**
     * Description for IP filter list entry. Example: `Production service IP range`.
     */
    description?: string;
    /**
     * CIDR address block. Example: `10.20.0.0/16`.
     */
    network: string;
}

export interface GetDragonflyDragonflyUserConfigMigration {
    /**
     * Database name for bootstrapping the initial connection. Example: `defaultdb`.
     */
    dbname?: string;
    /**
     * Hostname or IP address of the server where to migrate data from. Example: `my.server.com`.
     */
    host: string;
    /**
     * Comma-separated list of databases, which should be ignored during migration (supported by MySQL and PostgreSQL only at the moment). Example: `db1,db2`.
     */
    ignoreDbs?: string;
    /**
     * Comma-separated list of database roles, which should be ignored during migration (supported by PostgreSQL only at the moment). Example: `role1,role2`.
     */
    ignoreRoles?: string;
    /**
     * Enum: `dump`, `replication`. The migration method to be used (currently supported only by Redis, Dragonfly, MySQL and PostgreSQL service types).
     */
    method?: string;
    /**
     * Password for authentication with the server where to migrate data from. Example: `jjKk45Nnd`.
     */
    password?: string;
    /**
     * Port number of the server where to migrate data from. Example: `1234`.
     */
    port: number;
    /**
     * The server where to migrate data from is secured with SSL. Default: `true`.
     */
    ssl?: boolean;
    /**
     * User name for authentication with the server where to migrate data from. Example: `myname`.
     */
    username?: string;
}

export interface GetDragonflyDragonflyUserConfigPrivateAccess {
    /**
     * Allow clients to connect to dragonfly with a DNS name that always resolves to the service's private IP addresses. Only available in certain network locations.
     */
    dragonfly?: boolean;
    /**
     * Allow clients to connect to prometheus with a DNS name that always resolves to the service's private IP addresses. Only available in certain network locations.
     */
    prometheus?: boolean;
}

export interface GetDragonflyDragonflyUserConfigPrivatelinkAccess {
    /**
     * Enable dragonfly.
     */
    dragonfly?: boolean;
    /**
     * Enable prometheus.
     */
    prometheus?: boolean;
}

export interface GetDragonflyDragonflyUserConfigPublicAccess {
    /**
     * Allow clients to connect to dragonfly from the public internet for service nodes that are in a project VPC or another type of private network.
     */
    dragonfly?: boolean;
    /**
     * Allow clients to connect to prometheus from the public internet for service nodes that are in a project VPC or another type of private network.
     */
    prometheus?: boolean;
}

export interface GetDragonflyServiceIntegration {
    /**
     * Type of the service integration. The only supported value at the moment is `readReplica`
     */
    integrationType: string;
    /**
     * Name of the source service
     */
    sourceServiceName: string;
}

export interface GetDragonflyTag {
    /**
     * Service tag key
     */
    key: string;
    /**
     * Service tag value
     */
    value: string;
}

export interface GetDragonflyTechEmail {
    /**
     * An email address to contact for technical issues
     */
    email: string;
}

export interface GetFlinkApplicationVersionSink {
    /**
     * The CREATE TABLE statement
     */
    createTable: string;
    /**
     * The integration ID
     */
    integrationId?: string;
}

export interface GetFlinkApplicationVersionSource {
    /**
     * The CREATE TABLE statement
     */
    createTable: string;
    /**
     * The integration ID
     */
    integrationId?: string;
}

export interface GetFlinkComponent {
    /**
     * Service component name
     */
    component: string;
    /**
     * Connection info for connecting to the service component. This is a combination of host and port.
     */
    connectionUri: string;
    /**
     * Host name for connecting to the service component
     */
    host: string;
    /**
     * Kafka authentication method. This is a value specific to the 'kafka' service component
     */
    kafkaAuthenticationMethod: string;
    /**
     * Port number for connecting to the service component
     */
    port: number;
    /**
     * Network access route
     */
    route: string;
    /**
     * Whether the endpoint is encrypted or accepts plaintext. By default endpoints are always encrypted and this property is only included for service components they may disable encryption
     */
    ssl: boolean;
    /**
     * DNS usage name
     */
    usage: string;
}

export interface GetFlinkFlink {
    /**
     * The host and port of a Flink server.
     */
    hostPorts: string[];
}

export interface GetFlinkFlinkUserConfig {
    /**
     * Additional Cloud Regions for Backup Replication.
     *
     * @deprecated This property is deprecated.
     */
    additionalBackupRegions?: string;
    /**
     * Enum: `1.16`, `1.19`, and newer. Flink major version.
     */
    flinkVersion?: string;
    /**
     * Allow incoming connections from CIDR address block, e.g. `10.20.0.0/16`
     */
    ipFilterObjects?: outputs.GetFlinkFlinkUserConfigIpFilterObject[];
    /**
     * Allow incoming connections from CIDR address block, e.g. `10.20.0.0/16`.
     */
    ipFilterStrings?: string[];
    /**
     * Allow incoming connections from CIDR address block, e.g. `10.20.0.0/16`.
     *
     * @deprecated Deprecated. Use `ipFilterString` instead.
     */
    ipFilters?: string[];
    /**
     * Task slots per node. For a 3 node plan, total number of task slots is 3x this value. Example: `1`.
     */
    numberOfTaskSlots?: number;
    /**
     * Allow access to selected service components through Privatelink
     */
    privatelinkAccess?: outputs.GetFlinkFlinkUserConfigPrivatelinkAccess;
    /**
     * Store logs for the service so that they are available in the HTTP API and console.
     */
    serviceLog?: boolean;
    /**
     * Use static public IP addresses.
     */
    staticIps?: boolean;
}

export interface GetFlinkFlinkUserConfigIpFilterObject {
    /**
     * Description for IP filter list entry. Example: `Production service IP range`.
     */
    description?: string;
    /**
     * CIDR address block. Example: `10.20.0.0/16`.
     */
    network: string;
}

export interface GetFlinkFlinkUserConfigPrivatelinkAccess {
    /**
     * Enable flink.
     */
    flink?: boolean;
    /**
     * Enable prometheus.
     */
    prometheus?: boolean;
}

export interface GetFlinkServiceIntegration {
    /**
     * Type of the service integration. The only supported value at the moment is `readReplica`
     */
    integrationType: string;
    /**
     * Name of the source service
     */
    sourceServiceName: string;
}

export interface GetFlinkTag {
    /**
     * Service tag key
     */
    key: string;
    /**
     * Service tag value
     */
    value: string;
}

export interface GetFlinkTechEmail {
    /**
     * An email address to contact for technical issues
     */
    email: string;
}

export interface GetGrafanaComponent {
    /**
     * Service component name
     */
    component: string;
    /**
     * Connection info for connecting to the service component. This is a combination of host and port.
     */
    connectionUri: string;
    /**
     * Host name for connecting to the service component
     */
    host: string;
    /**
     * Kafka authentication method. This is a value specific to the 'kafka' service component
     */
    kafkaAuthenticationMethod: string;
    /**
     * Port number for connecting to the service component
     */
    port: number;
    /**
     * Network access route
     */
    route: string;
    /**
     * Whether the endpoint is encrypted or accepts plaintext. By default endpoints are always encrypted and this property is only included for service components they may disable encryption
     */
    ssl: boolean;
    /**
     * DNS usage name
     */
    usage: string;
}

export interface GetGrafanaGrafana {
    /**
     * Grafana server URIs.
     */
    uris: string[];
}

export interface GetGrafanaGrafanaUserConfig {
    /**
     * Additional Cloud Regions for Backup Replication.
     */
    additionalBackupRegions?: string;
    /**
     * Enable or disable Grafana legacy alerting functionality. This should not be enabled with unified_alerting_enabled.
     */
    alertingEnabled?: boolean;
    /**
     * Enum: `alerting`, `keepState`. Default error or timeout setting for new alerting rules.
     */
    alertingErrorOrTimeout?: string;
    /**
     * Max number of alert annotations that Grafana stores. 0 (default) keeps all alert annotations. Example: `0`.
     */
    alertingMaxAnnotationsToKeep?: number;
    /**
     * Enum: `alerting`, `noData`, `keepState`, `ok`. Default value for 'no data or null values' for new alerting rules.
     */
    alertingNodataOrNullvalues?: string;
    /**
     * Allow embedding Grafana dashboards with iframe/frame/object/embed tags. Disabled by default to limit impact of clickjacking.
     */
    allowEmbedding?: boolean;
    /**
     * Azure AD OAuth integration
     */
    authAzuread?: outputs.GetGrafanaGrafanaUserConfigAuthAzuread;
    /**
     * Enable or disable basic authentication form, used by Grafana built-in login.
     */
    authBasicEnabled?: boolean;
    /**
     * Generic OAuth integration
     */
    authGenericOauth?: outputs.GetGrafanaGrafanaUserConfigAuthGenericOauth;
    /**
     * Github Auth integration
     */
    authGithub?: outputs.GetGrafanaGrafanaUserConfigAuthGithub;
    /**
     * GitLab Auth integration
     */
    authGitlab?: outputs.GetGrafanaGrafanaUserConfigAuthGitlab;
    /**
     * Google Auth integration
     */
    authGoogle?: outputs.GetGrafanaGrafanaUserConfigAuthGoogle;
    /**
     * Enum: `lax`, `strict`, `none`. Cookie SameSite attribute: `strict` prevents sending cookie for cross-site requests, effectively disabling direct linking from other sites to Grafana. `lax` is the default value.
     */
    cookieSamesite?: string;
    /**
     * Serve the web frontend using a custom CNAME pointing to the Aiven DNS name. Example: `grafana.example.org`.
     */
    customDomain?: string;
    /**
     * This feature is new in Grafana 9 and is quite resource intensive. It may cause low-end plans to work more slowly while the dashboard previews are rendering.
     */
    dashboardPreviewsEnabled?: boolean;
    /**
     * Signed sequence of decimal numbers, followed by a unit suffix (ms, s, m, h, d), e.g. 30s, 1h. Example: `5s`.
     */
    dashboardsMinRefreshInterval?: string;
    /**
     * Dashboard versions to keep per dashboard. Example: `20`.
     */
    dashboardsVersionsToKeep?: number;
    /**
     * Send `X-Grafana-User` header to data source.
     */
    dataproxySendUserHeader?: boolean;
    /**
     * Timeout for data proxy requests in seconds. Example: `30`.
     */
    dataproxyTimeout?: number;
    /**
     * Grafana date format specifications
     */
    dateFormats?: outputs.GetGrafanaGrafanaUserConfigDateFormats;
    /**
     * Set to true to disable gravatar. Defaults to false (gravatar is enabled).
     */
    disableGravatar?: boolean;
    /**
     * Editors can manage folders, teams and dashboards created by them.
     */
    editorsCanAdmin?: boolean;
    /**
     * External image store settings
     */
    externalImageStorage?: outputs.GetGrafanaGrafanaUserConfigExternalImageStorage;
    /**
     * Google Analytics ID. Example: `UA-123456-4`.
     */
    googleAnalyticsUaId?: string;
    /**
     * Allow incoming connections from CIDR address block, e.g. `10.20.0.0/16`
     */
    ipFilterObjects?: outputs.GetGrafanaGrafanaUserConfigIpFilterObject[];
    /**
     * Allow incoming connections from CIDR address block, e.g. `10.20.0.0/16`.
     */
    ipFilterStrings?: string[];
    /**
     * Allow incoming connections from CIDR address block, e.g. `10.20.0.0/16`.
     *
     * @deprecated Deprecated. Use `ipFilterString` instead.
     */
    ipFilters?: string[];
    /**
     * Enable Grafana /metrics endpoint.
     */
    metricsEnabled?: boolean;
    /**
     * Enforce user lookup based on email instead of the unique ID provided by the IdP.
     */
    oauthAllowInsecureEmailLookup?: boolean;
    /**
     * Allow access to selected service ports from private networks
     */
    privateAccess?: outputs.GetGrafanaGrafanaUserConfigPrivateAccess;
    /**
     * Allow access to selected service components through Privatelink
     */
    privatelinkAccess?: outputs.GetGrafanaGrafanaUserConfigPrivatelinkAccess;
    /**
     * Name of another project to fork a service from. This has effect only when a new service is being created. Example: `anotherprojectname`.
     */
    projectToForkFrom?: string;
    /**
     * Allow access to selected service ports from the public Internet
     */
    publicAccess?: outputs.GetGrafanaGrafanaUserConfigPublicAccess;
    /**
     * Name of the basebackup to restore in forked service. Example: `backup-20191112t091354293891z`.
     */
    recoveryBasebackupName?: string;
    /**
     * Store logs for the service so that they are available in the HTTP API and console.
     */
    serviceLog?: boolean;
    /**
     * Name of another service to fork from. This has effect only when a new service is being created. Example: `anotherservicename`.
     */
    serviceToForkFrom?: string;
    /**
     * SMTP server settings
     */
    smtpServer?: outputs.GetGrafanaGrafanaUserConfigSmtpServer;
    /**
     * Use static public IP addresses.
     */
    staticIps?: boolean;
    /**
     * Enable or disable Grafana unified alerting functionality. By default this is enabled and any legacy alerts will be migrated on upgrade to Grafana 9+. To stay on legacy alerting, set unifiedAlertingEnabled to false and alertingEnabled to true. See https://grafana.com/docs/grafana/latest/alerting/set-up/migrating-alerts/ for more details.
     */
    unifiedAlertingEnabled?: boolean;
    /**
     * Auto-assign new users on signup to main organization. Defaults to false.
     */
    userAutoAssignOrg?: boolean;
    /**
     * Enum: `Viewer`, `Admin`, `Editor`. Set role for new signups. Defaults to Viewer.
     */
    userAutoAssignOrgRole?: string;
    /**
     * Users with view-only permission can edit but not save dashboards.
     */
    viewersCanEdit?: boolean;
    /**
     * Setting to enable/disable Write-Ahead Logging. The default value is false (disabled).
     */
    wal?: boolean;
}

export interface GetGrafanaGrafanaUserConfigAuthAzuread {
    /**
     * Automatically sign-up users on successful sign-in.
     */
    allowSignUp?: boolean;
    /**
     * Allowed domains.
     */
    allowedDomains?: string[];
    /**
     * Require users to belong to one of given groups.
     */
    allowedGroups?: string[];
    /**
     * Authorization URL. Example: `https://login.microsoftonline.com/<AZURE_TENANT_ID>/oauth2/v2.0/authorize`.
     */
    authUrl: string;
    /**
     * Client ID from provider. Example: `b1ba0bf54a4c2c0a1c29`.
     */
    clientId: string;
    /**
     * Client secret from provider. Example: `bfa6gea4f129076761dcba8ce5e1e406bd83af7b`.
     */
    clientSecret: string;
    /**
     * Token URL. Example: `https://login.microsoftonline.com/<AZURE_TENANT_ID>/oauth2/v2.0/token`.
     */
    tokenUrl: string;
}

export interface GetGrafanaGrafanaUserConfigAuthGenericOauth {
    /**
     * Automatically sign-up users on successful sign-in.
     */
    allowSignUp?: boolean;
    /**
     * Allowed domains.
     */
    allowedDomains?: string[];
    /**
     * Require user to be member of one of the listed organizations.
     */
    allowedOrganizations?: string[];
    /**
     * API URL. Example: `https://yourprovider.com/api`.
     */
    apiUrl: string;
    /**
     * Authorization URL. Example: `https://yourprovider.com/oauth/authorize`.
     */
    authUrl: string;
    /**
     * Allow users to bypass the login screen and automatically log in.
     */
    autoLogin?: boolean;
    /**
     * Client ID from provider. Example: `b1ba0bf54a4c2c0a1c29`.
     */
    clientId: string;
    /**
     * Client secret from provider. Example: `bfa6gea4f129076761dcba8ce5e1e406bd83af7b`.
     */
    clientSecret: string;
    /**
     * Name of the OAuth integration. Example: `My authentication`.
     */
    name?: string;
    /**
     * OAuth scopes.
     */
    scopes?: string[];
    /**
     * Token URL. Example: `https://yourprovider.com/oauth/token`.
     */
    tokenUrl: string;
    /**
     * Set to true to use refresh token and check access token expiration.
     */
    useRefreshToken?: boolean;
}

export interface GetGrafanaGrafanaUserConfigAuthGithub {
    /**
     * Automatically sign-up users on successful sign-in.
     */
    allowSignUp?: boolean;
    /**
     * Require users to belong to one of given organizations.
     */
    allowedOrganizations?: string[];
    /**
     * Allow users to bypass the login screen and automatically log in.
     */
    autoLogin?: boolean;
    /**
     * Client ID from provider. Example: `b1ba0bf54a4c2c0a1c29`.
     */
    clientId: string;
    /**
     * Client secret from provider. Example: `bfa6gea4f129076761dcba8ce5e1e406bd83af7b`.
     */
    clientSecret: string;
    /**
     * Stop automatically syncing user roles.
     */
    skipOrgRoleSync?: boolean;
    /**
     * Require users to belong to one of given team IDs.
     */
    teamIds?: number[];
}

export interface GetGrafanaGrafanaUserConfigAuthGitlab {
    /**
     * Automatically sign-up users on successful sign-in.
     */
    allowSignUp?: boolean;
    /**
     * Require users to belong to one of given groups.
     */
    allowedGroups: string[];
    /**
     * API URL. This only needs to be set when using self hosted GitLab. Example: `https://gitlab.com/api/v4`.
     */
    apiUrl?: string;
    /**
     * Authorization URL. This only needs to be set when using self hosted GitLab. Example: `https://gitlab.com/oauth/authorize`.
     */
    authUrl?: string;
    /**
     * Client ID from provider. Example: `b1ba0bf54a4c2c0a1c29`.
     */
    clientId: string;
    /**
     * Client secret from provider. Example: `bfa6gea4f129076761dcba8ce5e1e406bd83af7b`.
     */
    clientSecret: string;
    /**
     * Token URL. This only needs to be set when using self hosted GitLab. Example: `https://gitlab.com/oauth/token`.
     */
    tokenUrl?: string;
}

export interface GetGrafanaGrafanaUserConfigAuthGoogle {
    /**
     * Automatically sign-up users on successful sign-in.
     */
    allowSignUp?: boolean;
    /**
     * Domains allowed to sign-in to this Grafana.
     */
    allowedDomains: string[];
    /**
     * Client ID from provider. Example: `b1ba0bf54a4c2c0a1c29`.
     */
    clientId: string;
    /**
     * Client secret from provider. Example: `bfa6gea4f129076761dcba8ce5e1e406bd83af7b`.
     */
    clientSecret: string;
}

export interface GetGrafanaGrafanaUserConfigDateFormats {
    /**
     * Default time zone for user preferences. Value `browser` uses browser local time zone. Example: `Europe/Helsinki`.
     */
    defaultTimezone?: string;
    /**
     * Moment.js style format string for cases where full date is shown. Example: `YYYY MM DD`.
     */
    fullDate?: string;
    /**
     * Moment.js style format string used when a time requiring day accuracy is shown. Example: `MM/DD`.
     */
    intervalDay?: string;
    /**
     * Moment.js style format string used when a time requiring hour accuracy is shown. Example: `MM/DD HH:mm`.
     */
    intervalHour?: string;
    /**
     * Moment.js style format string used when a time requiring minute accuracy is shown. Example: `HH:mm`.
     */
    intervalMinute?: string;
    /**
     * Moment.js style format string used when a time requiring month accuracy is shown. Example: `YYYY-MM`.
     */
    intervalMonth?: string;
    /**
     * Moment.js style format string used when a time requiring second accuracy is shown. Example: `HH:mm:ss`.
     */
    intervalSecond?: string;
    /**
     * Moment.js style format string used when a time requiring year accuracy is shown. Example: `YYYY`.
     */
    intervalYear?: string;
}

export interface GetGrafanaGrafanaUserConfigExternalImageStorage {
    /**
     * S3 access key. Requires permissions to the S3 bucket for the s3:PutObject and s3:PutObjectAcl actions. Example: `AAAAAAAAAAAAAAAAAAA`.
     */
    accessKey: string;
    /**
     * Bucket URL for S3. Example: `https://grafana.s3-ap-southeast-2.amazonaws.com/`.
     */
    bucketUrl: string;
    /**
     * Enum: `s3`. Provider type.
     */
    provider: string;
    /**
     * S3 secret key. Example: `AAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAA`.
     */
    secretKey: string;
}

export interface GetGrafanaGrafanaUserConfigIpFilterObject {
    /**
     * Description for IP filter list entry. Example: `Production service IP range`.
     */
    description?: string;
    /**
     * CIDR address block. Example: `10.20.0.0/16`.
     */
    network: string;
}

export interface GetGrafanaGrafanaUserConfigPrivateAccess {
    /**
     * Allow clients to connect to grafana with a DNS name that always resolves to the service's private IP addresses. Only available in certain network locations.
     */
    grafana?: boolean;
}

export interface GetGrafanaGrafanaUserConfigPrivatelinkAccess {
    /**
     * Enable grafana.
     */
    grafana?: boolean;
}

export interface GetGrafanaGrafanaUserConfigPublicAccess {
    /**
     * Allow clients to connect to grafana from the public internet for service nodes that are in a project VPC or another type of private network.
     */
    grafana?: boolean;
}

export interface GetGrafanaGrafanaUserConfigSmtpServer {
    /**
     * Address used for sending emails. Example: `yourgrafanauser@yourdomain.example.com`.
     */
    fromAddress: string;
    /**
     * Name used in outgoing emails, defaults to Grafana.
     */
    fromName?: string;
    /**
     * Server hostname or IP. Example: `smtp.example.com`.
     */
    host: string;
    /**
     * Password for SMTP authentication. Example: `ein0eemeev5eeth3Ahfu`.
     */
    password?: string;
    /**
     * SMTP server port. Example: `25`.
     */
    port: number;
    /**
     * Skip verifying server certificate. Defaults to false.
     */
    skipVerify?: boolean;
    /**
     * Enum: `OpportunisticStartTLS`, `MandatoryStartTLS`, `NoStartTLS`. Either OpportunisticStartTLS, MandatoryStartTLS or NoStartTLS. Default is OpportunisticStartTLS.
     */
    starttlsPolicy?: string;
    /**
     * Username for SMTP authentication. Example: `smtpuser`.
     */
    username?: string;
}

export interface GetGrafanaServiceIntegration {
    /**
     * Type of the service integration. The only supported value at the moment is `readReplica`
     */
    integrationType: string;
    /**
     * Name of the source service
     */
    sourceServiceName: string;
}

export interface GetGrafanaTag {
    /**
     * Service tag key
     */
    key: string;
    /**
     * Service tag value
     */
    value: string;
}

export interface GetGrafanaTechEmail {
    /**
     * An email address to contact for technical issues
     */
    email: string;
}

export interface GetInfluxDbComponent {
    /**
     * Service component name
     */
    component: string;
    /**
     * Connection info for connecting to the service component. This is a combination of host and port.
     */
    connectionUri: string;
    /**
     * Host name for connecting to the service component
     */
    host: string;
    /**
     * Kafka authentication method. This is a value specific to the 'kafka' service component
     */
    kafkaAuthenticationMethod: string;
    /**
     * Port number for connecting to the service component
     */
    port: number;
    /**
     * Network access route
     */
    route: string;
    /**
     * Whether the endpoint is encrypted or accepts plaintext. By default endpoints are always encrypted and this property is only included for service components they may disable encryption
     */
    ssl: boolean;
    /**
     * DNS usage name
     */
    usage: string;
}

export interface GetInfluxDbInfluxdb {
    /**
     * Name of the default InfluxDB database
     */
    databaseName: string;
    /**
     * InfluxDB password
     */
    password: string;
    /**
     * InfluxDB server URIs.
     */
    uris: string[];
    /**
     * InfluxDB username
     */
    username: string;
}

export interface GetInfluxDbInfluxdbUserConfig {
    /**
     * Additional Cloud Regions for Backup Replication.
     *
     * @deprecated This property is deprecated.
     */
    additionalBackupRegions?: string;
    /**
     * Serve the web frontend using a custom CNAME pointing to the Aiven DNS name. Example: `grafana.example.org`.
     */
    customDomain?: string;
    /**
     * influxdb.conf configuration values
     */
    influxdb?: outputs.GetInfluxDbInfluxdbUserConfigInfluxdb;
    /**
     * Allow incoming connections from CIDR address block, e.g. `10.20.0.0/16`
     */
    ipFilterObjects?: outputs.GetInfluxDbInfluxdbUserConfigIpFilterObject[];
    /**
     * Allow incoming connections from CIDR address block, e.g. `10.20.0.0/16`.
     */
    ipFilterStrings?: string[];
    /**
     * Allow incoming connections from CIDR address block, e.g. `10.20.0.0/16`.
     *
     * @deprecated Deprecated. Use `ipFilterString` instead.
     */
    ipFilters?: string[];
    /**
     * Allow access to selected service ports from private networks
     */
    privateAccess?: outputs.GetInfluxDbInfluxdbUserConfigPrivateAccess;
    /**
     * Allow access to selected service components through Privatelink
     */
    privatelinkAccess?: outputs.GetInfluxDbInfluxdbUserConfigPrivatelinkAccess;
    /**
     * Name of another project to fork a service from. This has effect only when a new service is being created. Example: `anotherprojectname`.
     */
    projectToForkFrom?: string;
    /**
     * Allow access to selected service ports from the public Internet
     */
    publicAccess?: outputs.GetInfluxDbInfluxdbUserConfigPublicAccess;
    /**
     * Name of the basebackup to restore in forked service. Example: `backup-20191112t091354293891z`.
     */
    recoveryBasebackupName?: string;
    /**
     * Store logs for the service so that they are available in the HTTP API and console.
     */
    serviceLog?: boolean;
    /**
     * Name of another service to fork from. This has effect only when a new service is being created. Example: `anotherservicename`.
     */
    serviceToForkFrom?: string;
    /**
     * Use static public IP addresses.
     */
    staticIps?: boolean;
}

export interface GetInfluxDbInfluxdbUserConfigInfluxdb {
    /**
     * The maximum duration in seconds before a query is logged as a slow query. Setting this to 0 (the default) will never log slow queries.
     */
    logQueriesAfter?: number;
    /**
     * Maximum number of connections to InfluxDB. Setting this to 0 (default) means no limit. If using max_connection_limit, it is recommended to set the value to be large enough in order to not block clients unnecessarily.
     */
    maxConnectionLimit?: number;
    /**
     * The maximum number of rows returned in a non-chunked query. Setting this to 0 (the default) allows an unlimited number to be returned.
     */
    maxRowLimit?: number;
    /**
     * The maximum number of `GROUP BY time()` buckets that can be processed in a query. Setting this to 0 (the default) allows an unlimited number to be processed.
     */
    maxSelectBuckets?: number;
    /**
     * The maximum number of points that can be processed in a SELECT statement. Setting this to 0 (the default) allows an unlimited number to be processed.
     */
    maxSelectPoint?: number;
    /**
     * Whether queries should be logged before execution. May log sensitive data contained within a query.
     */
    queryLogEnabled?: boolean;
    /**
     * The maximum duration in seconds before a query is killed. Setting this to 0 (the default) will never kill slow queries.
     */
    queryTimeout?: number;
}

export interface GetInfluxDbInfluxdbUserConfigIpFilterObject {
    /**
     * Description for IP filter list entry. Example: `Production service IP range`.
     */
    description?: string;
    /**
     * CIDR address block. Example: `10.20.0.0/16`.
     */
    network: string;
}

export interface GetInfluxDbInfluxdbUserConfigPrivateAccess {
    /**
     * Allow clients to connect to influxdb with a DNS name that always resolves to the service's private IP addresses. Only available in certain network locations.
     */
    influxdb?: boolean;
}

export interface GetInfluxDbInfluxdbUserConfigPrivatelinkAccess {
    /**
     * Enable influxdb.
     */
    influxdb?: boolean;
}

export interface GetInfluxDbInfluxdbUserConfigPublicAccess {
    /**
     * Allow clients to connect to influxdb from the public internet for service nodes that are in a project VPC or another type of private network.
     */
    influxdb?: boolean;
}

export interface GetInfluxDbServiceIntegration {
    /**
     * Type of the service integration. The only supported value at the moment is `readReplica`
     */
    integrationType: string;
    /**
     * Name of the source service
     */
    sourceServiceName: string;
}

export interface GetInfluxDbTag {
    /**
     * Service tag key
     */
    key: string;
    /**
     * Service tag value
     */
    value: string;
}

export interface GetInfluxDbTechEmail {
    /**
     * An email address to contact for technical issues
     */
    email: string;
}

export interface GetKafkaComponent {
    /**
     * Service component name
     */
    component: string;
    /**
     * Connection info for connecting to the service component. This is a combination of host and port.
     */
    connectionUri: string;
    /**
     * Host name for connecting to the service component
     */
    host: string;
    /**
     * Kafka authentication method. This is a value specific to the 'kafka' service component
     */
    kafkaAuthenticationMethod: string;
    /**
     * Port number for connecting to the service component
     */
    port: number;
    /**
     * Network access route
     */
    route: string;
    /**
     * Whether the endpoint is encrypted or accepts plaintext. By default endpoints are always encrypted and this property is only included for service components they may disable encryption
     */
    ssl: boolean;
    /**
     * DNS usage name
     */
    usage: string;
}

export interface GetKafkaConnectComponent {
    /**
     * Service component name
     */
    component: string;
    /**
     * Connection info for connecting to the service component. This is a combination of host and port.
     */
    connectionUri: string;
    /**
     * Host name for connecting to the service component
     */
    host: string;
    /**
     * Kafka authentication method. This is a value specific to the 'kafka' service component
     */
    kafkaAuthenticationMethod: string;
    /**
     * Port number for connecting to the service component
     */
    port: number;
    /**
     * Network access route
     */
    route: string;
    /**
     * Whether the endpoint is encrypted or accepts plaintext. By default endpoints are always encrypted and this property is only included for service components they may disable encryption
     */
    ssl: boolean;
    /**
     * DNS usage name
     */
    usage: string;
}

export interface GetKafkaConnectKafkaConnectUserConfig {
    /**
     * Additional Cloud Regions for Backup Replication.
     *
     * @deprecated This property is deprecated.
     */
    additionalBackupRegions?: string;
    /**
     * Allow incoming connections from CIDR address block, e.g. `10.20.0.0/16`
     */
    ipFilterObjects?: outputs.GetKafkaConnectKafkaConnectUserConfigIpFilterObject[];
    /**
     * Allow incoming connections from CIDR address block, e.g. `10.20.0.0/16`.
     */
    ipFilterStrings?: string[];
    /**
     * Allow incoming connections from CIDR address block, e.g. `10.20.0.0/16`.
     *
     * @deprecated Deprecated. Use `ipFilterString` instead.
     */
    ipFilters?: string[];
    /**
     * Kafka Connect configuration values
     */
    kafkaConnect?: outputs.GetKafkaConnectKafkaConnectUserConfigKafkaConnect;
    /**
     * Allow access to selected service ports from private networks
     */
    privateAccess?: outputs.GetKafkaConnectKafkaConnectUserConfigPrivateAccess;
    /**
     * Allow access to selected service components through Privatelink
     */
    privatelinkAccess?: outputs.GetKafkaConnectKafkaConnectUserConfigPrivatelinkAccess;
    /**
     * Allow access to selected service ports from the public Internet
     */
    publicAccess?: outputs.GetKafkaConnectKafkaConnectUserConfigPublicAccess;
    secretProviders?: outputs.GetKafkaConnectKafkaConnectUserConfigSecretProvider[];
    /**
     * Store logs for the service so that they are available in the HTTP API and console.
     */
    serviceLog?: boolean;
    /**
     * Use static public IP addresses.
     */
    staticIps?: boolean;
}

export interface GetKafkaConnectKafkaConnectUserConfigIpFilterObject {
    /**
     * Description for IP filter list entry. Example: `Production service IP range`.
     */
    description?: string;
    /**
     * CIDR address block. Example: `10.20.0.0/16`.
     */
    network: string;
}

export interface GetKafkaConnectKafkaConnectUserConfigKafkaConnect {
    /**
     * Enum: `None`, `All`. Defines what client configurations can be overridden by the connector. Default is None.
     */
    connectorClientConfigOverridePolicy?: string;
    /**
     * Enum: `earliest`, `latest`. What to do when there is no initial offset in Kafka or if the current offset does not exist any more on the server. Default is earliest.
     */
    consumerAutoOffsetReset?: string;
    /**
     * Records are fetched in batches by the consumer, and if the first record batch in the first non-empty partition of the fetch is larger than this value, the record batch will still be returned to ensure that the consumer can make progress. As such, this is not a absolute maximum. Example: `52428800`.
     */
    consumerFetchMaxBytes?: number;
    /**
     * Enum: `readUncommitted`, `readCommitted`. Transaction read isolation level. readUncommitted is the default, but readCommitted can be used if consume-exactly-once behavior is desired.
     */
    consumerIsolationLevel?: string;
    /**
     * Records are fetched in batches by the consumer.If the first record batch in the first non-empty partition of the fetch is larger than this limit, the batch will still be returned to ensure that the consumer can make progress. Example: `1048576`.
     */
    consumerMaxPartitionFetchBytes?: number;
    /**
     * The maximum delay in milliseconds between invocations of poll() when using consumer group management (defaults to 300000).
     */
    consumerMaxPollIntervalMs?: number;
    /**
     * The maximum number of records returned in a single call to poll() (defaults to 500).
     */
    consumerMaxPollRecords?: number;
    /**
     * The interval at which to try committing offsets for tasks (defaults to 60000).
     */
    offsetFlushIntervalMs?: number;
    /**
     * Maximum number of milliseconds to wait for records to flush and partition offset data to be committed to offset storage before cancelling the process and restoring the offset data to be committed in a future attempt (defaults to 5000).
     */
    offsetFlushTimeoutMs?: number;
    /**
     * This setting gives the upper bound of the batch size to be sent. If there are fewer than this many bytes accumulated for this partition, the producer will `linger` for the linger.ms time waiting for more records to show up. A batch size of zero will disable batching entirely (defaults to 16384).
     */
    producerBatchSize?: number;
    /**
     * The total bytes of memory the producer can use to buffer records waiting to be sent to the broker (defaults to 33554432).
     */
    producerBufferMemory?: number;
    /**
     * Enum: `gzip`, `snappy`, `lz4`, `zstd`, `none`. Specify the default compression type for producers. This configuration accepts the standard compression codecs (`gzip`, `snappy`, `lz4`, `zstd`). It additionally accepts `none` which is the default and equivalent to no compression.
     */
    producerCompressionType?: string;
    /**
     * This setting gives the upper bound on the delay for batching: once there is batch.size worth of records for a partition it will be sent immediately regardless of this setting, however if there are fewer than this many bytes accumulated for this partition the producer will `linger` for the specified time waiting for more records to show up. Defaults to 0.
     */
    producerLingerMs?: number;
    /**
     * This setting will limit the number of record batches the producer will send in a single request to avoid sending huge requests. Example: `1048576`.
     */
    producerMaxRequestSize?: number;
    /**
     * The maximum delay that is scheduled in order to wait for the return of one or more departed workers before rebalancing and reassigning their connectors and tasks to the group. During this period the connectors and tasks of the departed workers remain unassigned. Defaults to 5 minutes.
     */
    scheduledRebalanceMaxDelayMs?: number;
    /**
     * The timeout in milliseconds used to detect failures when using Kafka’s group management facilities (defaults to 10000).
     */
    sessionTimeoutMs?: number;
}

export interface GetKafkaConnectKafkaConnectUserConfigPrivateAccess {
    /**
     * Allow clients to connect to kafkaConnect with a DNS name that always resolves to the service's private IP addresses. Only available in certain network locations.
     */
    kafkaConnect?: boolean;
    /**
     * Allow clients to connect to prometheus with a DNS name that always resolves to the service's private IP addresses. Only available in certain network locations.
     */
    prometheus?: boolean;
}

export interface GetKafkaConnectKafkaConnectUserConfigPrivatelinkAccess {
    /**
     * Enable jolokia.
     */
    jolokia?: boolean;
    /**
     * Enable kafka_connect.
     */
    kafkaConnect?: boolean;
    /**
     * Enable prometheus.
     */
    prometheus?: boolean;
}

export interface GetKafkaConnectKafkaConnectUserConfigPublicAccess {
    /**
     * Allow clients to connect to kafkaConnect from the public internet for service nodes that are in a project VPC or another type of private network.
     */
    kafkaConnect?: boolean;
    /**
     * Allow clients to connect to prometheus from the public internet for service nodes that are in a project VPC or another type of private network.
     */
    prometheus?: boolean;
}

export interface GetKafkaConnectKafkaConnectUserConfigSecretProvider {
    /**
     * AWS config for Secret Provider
     */
    aws?: outputs.GetKafkaConnectKafkaConnectUserConfigSecretProviderAws;
    /**
     * Name of the secret provider. Used to reference secrets in connector config.
     */
    name: string;
    /**
     * Vault Config for Secret Provider
     */
    vault?: outputs.GetKafkaConnectKafkaConnectUserConfigSecretProviderVault;
}

export interface GetKafkaConnectKafkaConnectUserConfigSecretProviderAws {
    /**
     * Access key used to authenticate with aws.
     */
    accessKey?: string;
    /**
     * Enum: `credentials`. Auth method of the vault secret provider.
     */
    authMethod: string;
    /**
     * Region used to lookup secrets with AWS SecretManager.
     */
    region: string;
    /**
     * Secret key used to authenticate with aws.
     */
    secretKey?: string;
}

export interface GetKafkaConnectKafkaConnectUserConfigSecretProviderVault {
    /**
     * Address of the Vault server.
     */
    address: string;
    /**
     * Enum: `token`. Auth method of the vault secret provider.
     */
    authMethod: string;
    /**
     * Enum: `1`, `2`, and newer. KV Secrets Engine version of the Vault server instance.
     */
    engineVersion?: number;
    /**
     * Prefix path depth of the secrets Engine. Default is 1. If the secrets engine path has more than one segment it has to be increased to the number of segments.
     */
    prefixPathDepth?: number;
    /**
     * Token used to authenticate with vault and auth method `token`.
     */
    token?: string;
}

export interface GetKafkaConnectServiceIntegration {
    /**
     * Type of the service integration. The only supported value at the moment is `readReplica`
     */
    integrationType: string;
    /**
     * Name of the source service
     */
    sourceServiceName: string;
}

export interface GetKafkaConnectTag {
    /**
     * Service tag key
     */
    key: string;
    /**
     * Service tag value
     */
    value: string;
}

export interface GetKafkaConnectTechEmail {
    /**
     * An email address to contact for technical issues
     */
    email: string;
}

export interface GetKafkaConnectorTask {
    /**
     * The name of the related connector.
     */
    connector: string;
    /**
     * The task ID of the task.
     */
    task: number;
}

export interface GetKafkaKafka {
    /**
     * The Kafka client certificate.
     */
    accessCert: string;
    /**
     * The Kafka client certificate key.
     */
    accessKey: string;
    /**
     * The Kafka Connect URI.
     */
    connectUri: string;
    /**
     * The Kafka REST URI.
     */
    restUri: string;
    /**
     * The Schema Registry URI.
     */
    schemaRegistryUri: string;
    /**
     * Kafka server URIs.
     */
    uris: string[];
}

export interface GetKafkaKafkaUserConfig {
    /**
     * Additional Cloud Regions for Backup Replication.
     *
     * @deprecated This property is deprecated.
     */
    additionalBackupRegions?: string;
    /**
     * Allow access to read Kafka topic messages in the Aiven Console and REST API.
     */
    aivenKafkaTopicMessages?: boolean;
    /**
     * Serve the web frontend using a custom CNAME pointing to the Aiven DNS name. Example: `grafana.example.org`.
     */
    customDomain?: string;
    /**
     * Enable follower fetching
     */
    followerFetching?: outputs.GetKafkaKafkaUserConfigFollowerFetching;
    /**
     * Allow incoming connections from CIDR address block, e.g. `10.20.0.0/16`
     */
    ipFilterObjects?: outputs.GetKafkaKafkaUserConfigIpFilterObject[];
    /**
     * Allow incoming connections from CIDR address block, e.g. `10.20.0.0/16`.
     */
    ipFilterStrings?: string[];
    /**
     * Allow incoming connections from CIDR address block, e.g. `10.20.0.0/16`.
     *
     * @deprecated Deprecated. Use `ipFilterString` instead.
     */
    ipFilters?: string[];
    /**
     * Kafka broker configuration values
     */
    kafka?: outputs.GetKafkaKafkaUserConfigKafka;
    /**
     * Kafka authentication methods
     */
    kafkaAuthenticationMethods?: outputs.GetKafkaKafkaUserConfigKafkaAuthenticationMethods;
    /**
     * Enable Kafka Connect service. Default: `false`.
     */
    kafkaConnect?: boolean;
    /**
     * Kafka Connect configuration values
     */
    kafkaConnectConfig?: outputs.GetKafkaKafkaUserConfigKafkaConnectConfig;
    kafkaConnectSecretProviders?: outputs.GetKafkaKafkaUserConfigKafkaConnectSecretProvider[];
    /**
     * Enable Kafka-REST service. Default: `false`.
     */
    kafkaRest?: boolean;
    /**
     * Enable authorization in Kafka-REST service.
     */
    kafkaRestAuthorization?: boolean;
    /**
     * Kafka REST configuration
     */
    kafkaRestConfig?: outputs.GetKafkaKafkaUserConfigKafkaRestConfig;
    /**
     * Kafka SASL mechanisms
     */
    kafkaSaslMechanisms?: outputs.GetKafkaKafkaUserConfigKafkaSaslMechanisms;
    /**
     * Enum: `3.1`, `3.2`, `3.3`, `3.4`, `3.5`, `3.6`, `3.7`, `3.8`, and newer. Kafka major version.
     */
    kafkaVersion?: string;
    /**
     * Use Letsencrypt CA for Kafka SASL via Privatelink.
     */
    letsencryptSaslPrivatelink?: boolean;
    /**
     * Allow access to selected service ports from private networks
     */
    privateAccess?: outputs.GetKafkaKafkaUserConfigPrivateAccess;
    /**
     * Allow access to selected service components through Privatelink
     */
    privatelinkAccess?: outputs.GetKafkaKafkaUserConfigPrivatelinkAccess;
    /**
     * Allow access to selected service ports from the public Internet
     */
    publicAccess?: outputs.GetKafkaKafkaUserConfigPublicAccess;
    /**
     * Enable Schema-Registry service. Default: `false`.
     */
    schemaRegistry?: boolean;
    /**
     * Schema Registry configuration
     */
    schemaRegistryConfig?: outputs.GetKafkaKafkaUserConfigSchemaRegistryConfig;
    /**
     * Store logs for the service so that they are available in the HTTP API and console.
     */
    serviceLog?: boolean;
    /**
     * Use static public IP addresses.
     */
    staticIps?: boolean;
    /**
     * Tiered storage configuration
     */
    tieredStorage?: outputs.GetKafkaKafkaUserConfigTieredStorage;
}

export interface GetKafkaKafkaUserConfigFollowerFetching {
    /**
     * Whether to enable the follower fetching functionality.
     */
    enabled?: boolean;
}

export interface GetKafkaKafkaUserConfigIpFilterObject {
    /**
     * Description for IP filter list entry. Example: `Production service IP range`.
     */
    description?: string;
    /**
     * CIDR address block. Example: `10.20.0.0/16`.
     */
    network: string;
}

export interface GetKafkaKafkaUserConfigKafka {
    /**
     * Enable auto-creation of topics. (Default: true).
     */
    autoCreateTopicsEnable?: boolean;
    /**
     * Enum: `gzip`, `snappy`, `lz4`, `zstd`, `uncompressed`, `producer`. Specify the final compression type for a given topic. This configuration accepts the standard compression codecs (`gzip`, `snappy`, `lz4`, `zstd`). It additionally accepts `uncompressed` which is equivalent to no compression; and `producer` which means retain the original compression codec set by the producer.(Default: producer).
     */
    compressionType?: string;
    /**
     * Idle connections timeout: the server socket processor threads close the connections that idle for longer than this. (Default: 600000 ms (10 minutes)). Example: `540000`.
     */
    connectionsMaxIdleMs?: number;
    /**
     * Replication factor for auto-created topics (Default: 3).
     */
    defaultReplicationFactor?: number;
    /**
     * The amount of time, in milliseconds, the group coordinator will wait for more consumers to join a new group before performing the first rebalance. A longer delay means potentially fewer rebalances, but increases the time until processing begins. The default value for this is 3 seconds. During development and testing it might be desirable to set this to 0 in order to not delay test execution time. (Default: 3000 ms (3 seconds)). Example: `3000`.
     */
    groupInitialRebalanceDelayMs?: number;
    /**
     * The maximum allowed session timeout for registered consumers. Longer timeouts give consumers more time to process messages in between heartbeats at the cost of a longer time to detect failures. Default: 1800000 ms (30 minutes). Example: `1800000`.
     */
    groupMaxSessionTimeoutMs?: number;
    /**
     * The minimum allowed session timeout for registered consumers. Longer timeouts give consumers more time to process messages in between heartbeats at the cost of a longer time to detect failures. (Default: 6000 ms (6 seconds)). Example: `6000`.
     */
    groupMinSessionTimeoutMs?: number;
    /**
     * How long are delete records retained? (Default: 86400000 (1 day)). Example: `86400000`.
     */
    logCleanerDeleteRetentionMs?: number;
    /**
     * The maximum amount of time message will remain uncompacted. Only applicable for logs that are being compacted. (Default: 9223372036854775807 ms (Long.MAX_VALUE)).
     */
    logCleanerMaxCompactionLagMs?: number;
    /**
     * Controls log compactor frequency. Larger value means more frequent compactions but also more space wasted for logs. Consider setting log.cleaner.max.compaction.lag.ms to enforce compactions sooner, instead of setting a very high value for this option. (Default: 0.5). Example: `0.5`.
     */
    logCleanerMinCleanableRatio?: number;
    /**
     * The minimum time a message will remain uncompacted in the log. Only applicable for logs that are being compacted. (Default: 0 ms).
     */
    logCleanerMinCompactionLagMs?: number;
    /**
     * Enum: `delete`, `compact`, `compact,delete`. The default cleanup policy for segments beyond the retention window (Default: delete).
     */
    logCleanupPolicy?: string;
    /**
     * The number of messages accumulated on a log partition before messages are flushed to disk (Default: 9223372036854775807 (Long.MAX_VALUE)). Example: `9223372036854775807`.
     */
    logFlushIntervalMessages?: number;
    /**
     * The maximum time in ms that a message in any topic is kept in memory (page-cache) before flushed to disk. If not set, the value in log.flush.scheduler.interval.ms is used (Default: null).
     */
    logFlushIntervalMs?: number;
    /**
     * The interval with which Kafka adds an entry to the offset index (Default: 4096 bytes (4 kibibytes)). Example: `4096`.
     */
    logIndexIntervalBytes?: number;
    /**
     * The maximum size in bytes of the offset index (Default: 10485760 (10 mebibytes)). Example: `10485760`.
     */
    logIndexSizeMaxBytes?: number;
    /**
     * The maximum size of local log segments that can grow for a partition before it gets eligible for deletion. If set to -2, the value of log.retention.bytes is used. The effective value should always be less than or equal to log.retention.bytes value. (Default: -2).
     */
    logLocalRetentionBytes?: number;
    /**
     * The number of milliseconds to keep the local log segments before it gets eligible for deletion. If set to -2, the value of log.retention.ms is used. The effective value should always be less than or equal to log.retention.ms value. (Default: -2).
     */
    logLocalRetentionMs?: number;
    /**
     * This configuration controls whether down-conversion of message formats is enabled to satisfy consume requests. (Default: true).
     */
    logMessageDownconversionEnable?: boolean;
    /**
     * The maximum difference allowed between the timestamp when a broker receives a message and the timestamp specified in the message (Default: 9223372036854775807 (Long.MAX_VALUE)).
     */
    logMessageTimestampDifferenceMaxMs?: number;
    /**
     * Enum: `CreateTime`, `LogAppendTime`. Define whether the timestamp in the message is message create time or log append time. (Default: CreateTime).
     */
    logMessageTimestampType?: string;
    /**
     * Should pre allocate file when create new segment? (Default: false).
     */
    logPreallocate?: boolean;
    /**
     * The maximum size of the log before deleting messages (Default: -1).
     */
    logRetentionBytes?: number;
    /**
     * The number of hours to keep a log file before deleting it (Default: 168 hours (1 week)).
     */
    logRetentionHours?: number;
    /**
     * The number of milliseconds to keep a log file before deleting it (in milliseconds), If not set, the value in log.retention.minutes is used. If set to -1, no time limit is applied. (Default: null, log.retention.hours applies).
     */
    logRetentionMs?: number;
    /**
     * The maximum jitter to subtract from logRollTimeMillis (in milliseconds). If not set, the value in log.roll.jitter.hours is used (Default: null).
     */
    logRollJitterMs?: number;
    /**
     * The maximum time before a new log segment is rolled out (in milliseconds). (Default: null, log.roll.hours applies (Default: 168, 7 days)).
     */
    logRollMs?: number;
    /**
     * The maximum size of a single log file (Default: 1073741824 bytes (1 gibibyte)).
     */
    logSegmentBytes?: number;
    /**
     * The amount of time to wait before deleting a file from the filesystem (Default: 60000 ms (1 minute)). Example: `60000`.
     */
    logSegmentDeleteDelayMs?: number;
    /**
     * The maximum number of connections allowed from each ip address (Default: 2147483647).
     */
    maxConnectionsPerIp?: number;
    /**
     * The maximum number of incremental fetch sessions that the broker will maintain. (Default: 1000). Example: `1000`.
     */
    maxIncrementalFetchSessionCacheSlots?: number;
    /**
     * The maximum size of message that the server can receive. (Default: 1048588 bytes (1 mebibyte + 12 bytes)). Example: `1048588`.
     */
    messageMaxBytes?: number;
    /**
     * When a producer sets acks to `all` (or `-1`), min.insync.replicas specifies the minimum number of replicas that must acknowledge a write for the write to be considered successful. (Default: 1). Example: `1`.
     */
    minInsyncReplicas?: number;
    /**
     * Number of partitions for auto-created topics (Default: 1).
     */
    numPartitions?: number;
    /**
     * Log retention window in minutes for offsets topic (Default: 10080 minutes (7 days)). Example: `10080`.
     */
    offsetsRetentionMinutes?: number;
    /**
     * The purge interval (in number of requests) of the producer request purgatory (Default: 1000).
     */
    producerPurgatoryPurgeIntervalRequests?: number;
    /**
     * The number of bytes of messages to attempt to fetch for each partition . This is not an absolute maximum, if the first record batch in the first non-empty partition of the fetch is larger than this value, the record batch will still be returned to ensure that progress can be made. (Default: 1048576 bytes (1 mebibytes)).
     */
    replicaFetchMaxBytes?: number;
    /**
     * Maximum bytes expected for the entire fetch response. Records are fetched in batches, and if the first record batch in the first non-empty partition of the fetch is larger than this value, the record batch will still be returned to ensure that progress can be made. As such, this is not an absolute maximum. (Default: 10485760 bytes (10 mebibytes)).
     */
    replicaFetchResponseMaxBytes?: number;
    /**
     * The (optional) comma-delimited setting for the broker to use to verify that the JWT was issued for one of the expected audiences. (Default: null).
     */
    saslOauthbearerExpectedAudience?: string;
    /**
     * Optional setting for the broker to use to verify that the JWT was created by the expected issuer.(Default: null).
     */
    saslOauthbearerExpectedIssuer?: string;
    /**
     * OIDC JWKS endpoint URL. By setting this the SASL SSL OAuth2/OIDC authentication is enabled. See also other options for SASL OAuth2/OIDC. (Default: null).
     */
    saslOauthbearerJwksEndpointUrl?: string;
    /**
     * Name of the scope from which to extract the subject claim from the JWT.(Default: sub).
     */
    saslOauthbearerSubClaimName?: string;
    /**
     * The maximum number of bytes in a socket request (Default: 104857600 bytes).
     */
    socketRequestMaxBytes?: number;
    /**
     * Enable verification that checks that the partition has been added to the transaction before writing transactional records to the partition. (Default: true).
     */
    transactionPartitionVerificationEnable?: boolean;
    /**
     * The interval at which to remove transactions that have expired due to transactional.id.expiration.ms passing (Default: 3600000 ms (1 hour)). Example: `3600000`.
     */
    transactionRemoveExpiredTransactionCleanupIntervalMs?: number;
    /**
     * The transaction topic segment bytes should be kept relatively small in order to facilitate faster log compaction and cache loads (Default: 104857600 bytes (100 mebibytes)). Example: `104857600`.
     */
    transactionStateLogSegmentBytes?: number;
}

export interface GetKafkaKafkaUserConfigKafkaAuthenticationMethods {
    /**
     * Enable certificate/SSL authentication. Default: `true`.
     */
    certificate?: boolean;
    /**
     * Enable SASL authentication. Default: `false`.
     */
    sasl?: boolean;
}

export interface GetKafkaKafkaUserConfigKafkaConnectConfig {
    /**
     * Enum: `None`, `All`. Defines what client configurations can be overridden by the connector. Default is None.
     */
    connectorClientConfigOverridePolicy?: string;
    /**
     * Enum: `earliest`, `latest`. What to do when there is no initial offset in Kafka or if the current offset does not exist any more on the server. Default is earliest.
     */
    consumerAutoOffsetReset?: string;
    /**
     * Records are fetched in batches by the consumer, and if the first record batch in the first non-empty partition of the fetch is larger than this value, the record batch will still be returned to ensure that the consumer can make progress. As such, this is not a absolute maximum. Example: `52428800`.
     */
    consumerFetchMaxBytes?: number;
    /**
     * Enum: `readUncommitted`, `readCommitted`. Transaction read isolation level. readUncommitted is the default, but readCommitted can be used if consume-exactly-once behavior is desired.
     */
    consumerIsolationLevel?: string;
    /**
     * Records are fetched in batches by the consumer.If the first record batch in the first non-empty partition of the fetch is larger than this limit, the batch will still be returned to ensure that the consumer can make progress. Example: `1048576`.
     */
    consumerMaxPartitionFetchBytes?: number;
    /**
     * The maximum delay in milliseconds between invocations of poll() when using consumer group management (defaults to 300000).
     */
    consumerMaxPollIntervalMs?: number;
    /**
     * The maximum number of records returned in a single call to poll() (defaults to 500).
     */
    consumerMaxPollRecords?: number;
    /**
     * The interval at which to try committing offsets for tasks (defaults to 60000).
     */
    offsetFlushIntervalMs?: number;
    /**
     * Maximum number of milliseconds to wait for records to flush and partition offset data to be committed to offset storage before cancelling the process and restoring the offset data to be committed in a future attempt (defaults to 5000).
     */
    offsetFlushTimeoutMs?: number;
    /**
     * This setting gives the upper bound of the batch size to be sent. If there are fewer than this many bytes accumulated for this partition, the producer will `linger` for the linger.ms time waiting for more records to show up. A batch size of zero will disable batching entirely (defaults to 16384).
     */
    producerBatchSize?: number;
    /**
     * The total bytes of memory the producer can use to buffer records waiting to be sent to the broker (defaults to 33554432).
     */
    producerBufferMemory?: number;
    /**
     * Enum: `gzip`, `snappy`, `lz4`, `zstd`, `none`. Specify the default compression type for producers. This configuration accepts the standard compression codecs (`gzip`, `snappy`, `lz4`, `zstd`). It additionally accepts `none` which is the default and equivalent to no compression.
     */
    producerCompressionType?: string;
    /**
     * This setting gives the upper bound on the delay for batching: once there is batch.size worth of records for a partition it will be sent immediately regardless of this setting, however if there are fewer than this many bytes accumulated for this partition the producer will `linger` for the specified time waiting for more records to show up. Defaults to 0.
     */
    producerLingerMs?: number;
    /**
     * This setting will limit the number of record batches the producer will send in a single request to avoid sending huge requests. Example: `1048576`.
     */
    producerMaxRequestSize?: number;
    /**
     * The maximum delay that is scheduled in order to wait for the return of one or more departed workers before rebalancing and reassigning their connectors and tasks to the group. During this period the connectors and tasks of the departed workers remain unassigned. Defaults to 5 minutes.
     */
    scheduledRebalanceMaxDelayMs?: number;
    /**
     * The timeout in milliseconds used to detect failures when using Kafka’s group management facilities (defaults to 10000).
     */
    sessionTimeoutMs?: number;
}

export interface GetKafkaKafkaUserConfigKafkaConnectSecretProvider {
    /**
     * AWS config for Secret Provider
     */
    aws?: outputs.GetKafkaKafkaUserConfigKafkaConnectSecretProviderAws;
    /**
     * Name of the secret provider. Used to reference secrets in connector config.
     */
    name: string;
    /**
     * Vault Config for Secret Provider
     */
    vault?: outputs.GetKafkaKafkaUserConfigKafkaConnectSecretProviderVault;
}

export interface GetKafkaKafkaUserConfigKafkaConnectSecretProviderAws {
    /**
     * Access key used to authenticate with aws.
     */
    accessKey?: string;
    /**
     * Enum: `credentials`. Auth method of the vault secret provider.
     */
    authMethod: string;
    /**
     * Region used to lookup secrets with AWS SecretManager.
     */
    region: string;
    /**
     * Secret key used to authenticate with aws.
     */
    secretKey?: string;
}

export interface GetKafkaKafkaUserConfigKafkaConnectSecretProviderVault {
    /**
     * Address of the Vault server.
     */
    address: string;
    /**
     * Enum: `token`. Auth method of the vault secret provider.
     */
    authMethod: string;
    /**
     * Enum: `1`, `2`, and newer. KV Secrets Engine version of the Vault server instance.
     */
    engineVersion?: number;
    /**
     * Prefix path depth of the secrets Engine. Default is 1. If the secrets engine path has more than one segment it has to be increased to the number of segments.
     */
    prefixPathDepth?: number;
    /**
     * Token used to authenticate with vault and auth method `token`.
     */
    token?: string;
}

export interface GetKafkaKafkaUserConfigKafkaRestConfig {
    /**
     * If true the consumer's offset will be periodically committed to Kafka in the background. Default: `true`.
     */
    consumerEnableAutoCommit?: boolean;
    /**
     * Maximum number of bytes in unencoded message keys and values by a single request. Default: `67108864`.
     */
    consumerRequestMaxBytes?: number;
    /**
     * Enum: `1000`, `15000`, `30000`. The maximum total time to wait for messages for a request if the maximum number of messages has not yet been reached. Default: `1000`.
     */
    consumerRequestTimeoutMs?: number;
    /**
     * Enum: `topicName`, `recordName`, `topicRecordName`. Name strategy to use when selecting subject for storing schemas. Default: `topicName`.
     */
    nameStrategy?: string;
    /**
     * If true, validate that given schema is registered under expected subject name by the used name strategy when producing messages. Default: `true`.
     */
    nameStrategyValidation?: boolean;
    /**
     * Enum: `all`, `-1`, `0`, `1`. The number of acknowledgments the producer requires the leader to have received before considering a request complete. If set to `all` or `-1`, the leader will wait for the full set of in-sync replicas to acknowledge the record. Default: `1`.
     */
    producerAcks?: string;
    /**
     * Enum: `gzip`, `snappy`, `lz4`, `zstd`, `none`. Specify the default compression type for producers. This configuration accepts the standard compression codecs (`gzip`, `snappy`, `lz4`, `zstd`). It additionally accepts `none` which is the default and equivalent to no compression.
     */
    producerCompressionType?: string;
    /**
     * Wait for up to the given delay to allow batching records together. Default: `0`.
     */
    producerLingerMs?: number;
    /**
     * The maximum size of a request in bytes. Note that Kafka broker can also cap the record batch size. Default: `1048576`.
     */
    producerMaxRequestSize?: number;
    /**
     * Maximum number of SimpleConsumers that can be instantiated per broker. Default: `25`.
     */
    simpleconsumerPoolSizeMax?: number;
}

export interface GetKafkaKafkaUserConfigKafkaSaslMechanisms {
    /**
     * Enable PLAIN mechanism. Default: `true`.
     */
    plain?: boolean;
    /**
     * Enable SCRAM-SHA-256 mechanism. Default: `true`.
     */
    scramSha256?: boolean;
    /**
     * Enable SCRAM-SHA-512 mechanism. Default: `true`.
     */
    scramSha512?: boolean;
}

export interface GetKafkaKafkaUserConfigPrivateAccess {
    /**
     * Allow clients to connect to kafka with a DNS name that always resolves to the service's private IP addresses. Only available in certain network locations.
     */
    kafka?: boolean;
    /**
     * Allow clients to connect to kafkaConnect with a DNS name that always resolves to the service's private IP addresses. Only available in certain network locations.
     */
    kafkaConnect?: boolean;
    /**
     * Allow clients to connect to kafkaRest with a DNS name that always resolves to the service's private IP addresses. Only available in certain network locations.
     */
    kafkaRest?: boolean;
    /**
     * Allow clients to connect to prometheus with a DNS name that always resolves to the service's private IP addresses. Only available in certain network locations.
     */
    prometheus?: boolean;
    /**
     * Allow clients to connect to schemaRegistry with a DNS name that always resolves to the service's private IP addresses. Only available in certain network locations.
     */
    schemaRegistry?: boolean;
}

export interface GetKafkaKafkaUserConfigPrivatelinkAccess {
    /**
     * Enable jolokia.
     */
    jolokia?: boolean;
    /**
     * Enable kafka.
     */
    kafka?: boolean;
    /**
     * Enable kafka_connect.
     */
    kafkaConnect?: boolean;
    /**
     * Enable kafka_rest.
     */
    kafkaRest?: boolean;
    /**
     * Enable prometheus.
     */
    prometheus?: boolean;
    /**
     * Enable schema_registry.
     */
    schemaRegistry?: boolean;
}

export interface GetKafkaKafkaUserConfigPublicAccess {
    /**
     * Allow clients to connect to kafka from the public internet for service nodes that are in a project VPC or another type of private network.
     */
    kafka?: boolean;
    /**
     * Allow clients to connect to kafkaConnect from the public internet for service nodes that are in a project VPC or another type of private network.
     */
    kafkaConnect?: boolean;
    /**
     * Allow clients to connect to kafkaRest from the public internet for service nodes that are in a project VPC or another type of private network.
     */
    kafkaRest?: boolean;
    /**
     * Allow clients to connect to prometheus from the public internet for service nodes that are in a project VPC or another type of private network.
     */
    prometheus?: boolean;
    /**
     * Allow clients to connect to schemaRegistry from the public internet for service nodes that are in a project VPC or another type of private network.
     */
    schemaRegistry?: boolean;
}

export interface GetKafkaKafkaUserConfigSchemaRegistryConfig {
    /**
     * If true, Karapace / Schema Registry on the service nodes can participate in leader election. It might be needed to disable this when the schemas topic is replicated to a secondary cluster and Karapace / Schema Registry there must not participate in leader election. Defaults to `true`.
     */
    leaderEligibility?: boolean;
    /**
     * If enabled, kafka errors which can be retried or custom errors specified for the service will not be raised, instead, a warning log is emitted. This will denoise issue tracking systems, i.e. sentry. Defaults to `true`.
     */
    retriableErrorsSilenced?: boolean;
    /**
     * If enabled, causes the Karapace schema-registry service to shutdown when there are invalid schema records in the `_schemas` topic. Defaults to `false`.
     */
    schemaReaderStrictMode?: boolean;
    /**
     * The durable single partition topic that acts as the durable log for the data. This topic must be compacted to avoid losing data due to retention policy. Please note that changing this configuration in an existing Schema Registry / Karapace setup leads to previous schemas being inaccessible, data encoded with them potentially unreadable and schema ID sequence put out of order. It's only possible to do the switch while Schema Registry / Karapace is disabled. Defaults to `_schemas`.
     */
    topicName?: string;
}

export interface GetKafkaKafkaUserConfigTieredStorage {
    /**
     * Whether to enable the tiered storage functionality.
     */
    enabled?: boolean;
    /**
     * Local cache configuration
     *
     * @deprecated This property is deprecated.
     */
    localCache?: outputs.GetKafkaKafkaUserConfigTieredStorageLocalCache;
}

export interface GetKafkaKafkaUserConfigTieredStorageLocalCache {
    /**
     * Local cache size in bytes. Example: `1073741824`.
     *
     * @deprecated This property is deprecated.
     */
    size?: number;
}

export interface GetKafkaMirrorMakerComponent {
    /**
     * Service component name
     */
    component: string;
    /**
     * Connection info for connecting to the service component. This is a combination of host and port.
     */
    connectionUri: string;
    /**
     * Host name for connecting to the service component
     */
    host: string;
    /**
     * Kafka authentication method. This is a value specific to the 'kafka' service component
     */
    kafkaAuthenticationMethod: string;
    /**
     * Port number for connecting to the service component
     */
    port: number;
    /**
     * Network access route
     */
    route: string;
    /**
     * Whether the endpoint is encrypted or accepts plaintext. By default endpoints are always encrypted and this property is only included for service components they may disable encryption
     */
    ssl: boolean;
    /**
     * DNS usage name
     */
    usage: string;
}

export interface GetKafkaMirrorMakerKafkaMirrormakerUserConfig {
    /**
     * Additional Cloud Regions for Backup Replication.
     *
     * @deprecated This property is deprecated.
     */
    additionalBackupRegions?: string;
    /**
     * Allow incoming connections from CIDR address block, e.g. `10.20.0.0/16`
     */
    ipFilterObjects?: outputs.GetKafkaMirrorMakerKafkaMirrormakerUserConfigIpFilterObject[];
    /**
     * Allow incoming connections from CIDR address block, e.g. `10.20.0.0/16`.
     */
    ipFilterStrings?: string[];
    /**
     * Allow incoming connections from CIDR address block, e.g. `10.20.0.0/16`.
     *
     * @deprecated Deprecated. Use `ipFilterString` instead.
     */
    ipFilters?: string[];
    /**
     * Kafka MirrorMaker configuration values
     */
    kafkaMirrormaker?: outputs.GetKafkaMirrorMakerKafkaMirrormakerUserConfigKafkaMirrormaker;
    /**
     * Store logs for the service so that they are available in the HTTP API and console.
     */
    serviceLog?: boolean;
    /**
     * Use static public IP addresses.
     */
    staticIps?: boolean;
}

export interface GetKafkaMirrorMakerKafkaMirrormakerUserConfigIpFilterObject {
    /**
     * Description for IP filter list entry. Example: `Production service IP range`.
     */
    description?: string;
    /**
     * CIDR address block. Example: `10.20.0.0/16`.
     */
    network: string;
}

export interface GetKafkaMirrorMakerKafkaMirrormakerUserConfigKafkaMirrormaker {
    /**
     * Timeout for administrative tasks, e.g. detecting new topics, loading of consumer group and offsets. Defaults to 60000 milliseconds (1 minute).
     */
    adminTimeoutMs?: number;
    /**
     * Whether to emit consumer group offset checkpoints to target cluster periodically (default: true).
     */
    emitCheckpointsEnabled?: boolean;
    /**
     * Frequency at which consumer group offset checkpoints are emitted (default: 60, every minute). Example: `60`.
     */
    emitCheckpointsIntervalSeconds?: number;
    /**
     * Consumer groups to replicate. Supports comma-separated group IDs and regexes. Example: `.*`.
     */
    groups?: string;
    /**
     * Exclude groups. Supports comma-separated group IDs and regexes. Excludes take precedence over includes. Example: `console-consumer-.*,connect-.*,__.*`.
     */
    groupsExclude?: string;
    /**
     * How out-of-sync a remote partition can be before it is resynced. Example: `100`.
     */
    offsetLagMax?: number;
    /**
     * Whether to periodically check for new consumer groups. Defaults to `true`.
     */
    refreshGroupsEnabled?: boolean;
    /**
     * Frequency of consumer group refresh in seconds. Defaults to 600 seconds (10 minutes).
     */
    refreshGroupsIntervalSeconds?: number;
    /**
     * Whether to periodically check for new topics and partitions. Defaults to `true`.
     */
    refreshTopicsEnabled?: boolean;
    /**
     * Frequency of topic and partitions refresh in seconds. Defaults to 600 seconds (10 minutes).
     */
    refreshTopicsIntervalSeconds?: number;
    /**
     * Whether to periodically write the translated offsets of replicated consumer groups (in the source cluster) to __consumer_offsets topic in target cluster, as long as no active consumers in that group are connected to the target cluster.
     */
    syncGroupOffsetsEnabled?: boolean;
    /**
     * Frequency at which consumer group offsets are synced (default: 60, every minute). Example: `60`.
     */
    syncGroupOffsetsIntervalSeconds?: number;
    /**
     * Whether to periodically configure remote topics to match their corresponding upstream topics.
     */
    syncTopicConfigsEnabled?: boolean;
    /**
     * `tasks.max` is set to this multiplied by the number of CPUs in the service. Default: `1`.
     */
    tasksMaxPerCpu?: number;
}

export interface GetKafkaMirrorMakerServiceIntegration {
    /**
     * Type of the service integration. The only supported value at the moment is `readReplica`
     */
    integrationType: string;
    /**
     * Name of the source service
     */
    sourceServiceName: string;
}

export interface GetKafkaMirrorMakerTag {
    /**
     * Service tag key
     */
    key: string;
    /**
     * Service tag value
     */
    value: string;
}

export interface GetKafkaMirrorMakerTechEmail {
    /**
     * An email address to contact for technical issues
     */
    email: string;
}

export interface GetKafkaServiceIntegration {
    /**
     * Type of the service integration. The only supported value at the moment is `readReplica`
     */
    integrationType: string;
    /**
     * Name of the source service
     */
    sourceServiceName: string;
}

export interface GetKafkaTag {
    /**
     * Service tag key
     */
    key: string;
    /**
     * Service tag value
     */
    value: string;
}

export interface GetKafkaTechEmail {
    /**
     * An email address to contact for technical issues
     */
    email: string;
}

export interface GetKafkaTopicConfig {
    /**
     * cleanup.policy value
     */
    cleanupPolicy?: string;
    /**
     * compression.type value
     */
    compressionType?: string;
    /**
     * delete.retention.ms value
     */
    deleteRetentionMs?: string;
    /**
     * file.delete.delay.ms value
     */
    fileDeleteDelayMs?: string;
    /**
     * flush.messages value
     */
    flushMessages?: string;
    /**
     * flush.ms value
     */
    flushMs?: string;
    /**
     * index.interval.bytes value
     */
    indexIntervalBytes?: string;
    /**
     * local.retention.bytes value
     */
    localRetentionBytes?: string;
    /**
     * local.retention.ms value
     */
    localRetentionMs?: string;
    /**
     * max.compaction.lag.ms value
     */
    maxCompactionLagMs?: string;
    /**
     * max.message.bytes value
     */
    maxMessageBytes?: string;
    /**
     * message.downconversion.enable value
     */
    messageDownconversionEnable?: boolean;
    /**
     * message.format.version value
     */
    messageFormatVersion?: string;
    /**
     * message.timestamp.difference.max.ms value
     */
    messageTimestampDifferenceMaxMs?: string;
    /**
     * message.timestamp.type value
     */
    messageTimestampType?: string;
    /**
     * min.cleanable.dirty.ratio value
     */
    minCleanableDirtyRatio?: number;
    /**
     * min.compaction.lag.ms value
     */
    minCompactionLagMs?: string;
    /**
     * min.insync.replicas value
     */
    minInsyncReplicas?: string;
    /**
     * preallocate value
     */
    preallocate?: boolean;
    /**
     * remote.storage.enable value
     */
    remoteStorageEnable?: boolean;
    /**
     * retention.bytes value
     */
    retentionBytes?: string;
    /**
     * retention.ms value
     */
    retentionMs?: string;
    /**
     * segment.bytes value
     */
    segmentBytes?: string;
    /**
     * segment.index.bytes value
     */
    segmentIndexBytes?: string;
    /**
     * segment.jitter.ms value
     */
    segmentJitterMs?: string;
    /**
     * segment.ms value
     */
    segmentMs?: string;
    /**
     * unclean.leader.election.enable value; This field is deprecated and no longer functional.
     *
     * @deprecated This field is deprecated and no longer functional.
     */
    uncleanLeaderElectionEnable?: boolean;
}

export interface GetKafkaTopicTag {
    /**
     * Tag key. Maximum length: `64`.
     */
    key: string;
    /**
     * Tag value. Maximum length: `256`.
     */
    value?: string;
}

export interface GetM3AggregatorComponent {
    /**
     * Service component name
     */
    component: string;
    /**
     * Connection info for connecting to the service component. This is a combination of host and port.
     */
    connectionUri: string;
    /**
     * Host name for connecting to the service component
     */
    host: string;
    /**
     * Kafka authentication method. This is a value specific to the 'kafka' service component
     */
    kafkaAuthenticationMethod: string;
    /**
     * Port number for connecting to the service component
     */
    port: number;
    /**
     * Network access route
     */
    route: string;
    /**
     * Whether the endpoint is encrypted or accepts plaintext. By default endpoints are always encrypted and this property is only included for service components they may disable encryption
     */
    ssl: boolean;
    /**
     * DNS usage name
     */
    usage: string;
}

export interface GetM3AggregatorM3aggregator {
    /**
     * M3 Aggregator HTTP URI.
     */
    aggregatorHttpUri: string;
    /**
     * M3 Aggregator server URIs.
     */
    uris: string[];
}

export interface GetM3AggregatorM3aggregatorUserConfig {
    /**
     * Serve the web frontend using a custom CNAME pointing to the Aiven DNS name. Example: `grafana.example.org`.
     */
    customDomain?: string;
    /**
     * Allow incoming connections from CIDR address block, e.g. `10.20.0.0/16`
     */
    ipFilterObjects?: outputs.GetM3AggregatorM3aggregatorUserConfigIpFilterObject[];
    /**
     * Allow incoming connections from CIDR address block, e.g. `10.20.0.0/16`.
     */
    ipFilterStrings?: string[];
    /**
     * Allow incoming connections from CIDR address block, e.g. `10.20.0.0/16`.
     *
     * @deprecated Deprecated. Use `ipFilterString` instead.
     */
    ipFilters?: string[];
    /**
     * Enum: `1.1`, `1.2`, `1.5`, and newer. M3 major version (deprecated, use m3aggregator_version).
     */
    m3Version?: string;
    /**
     * Enum: `1.1`, `1.2`, `1.5`, and newer. M3 major version (the minimum compatible version).
     */
    m3aggregatorVersion?: string;
    /**
     * Store logs for the service so that they are available in the HTTP API and console.
     */
    serviceLog?: boolean;
    /**
     * Use static public IP addresses.
     */
    staticIps?: boolean;
}

export interface GetM3AggregatorM3aggregatorUserConfigIpFilterObject {
    /**
     * Description for IP filter list entry. Example: `Production service IP range`.
     */
    description?: string;
    /**
     * CIDR address block. Example: `10.20.0.0/16`.
     */
    network: string;
}

export interface GetM3AggregatorServiceIntegration {
    /**
     * Type of the service integration. The only supported value at the moment is `readReplica`
     */
    integrationType: string;
    /**
     * Name of the source service
     */
    sourceServiceName: string;
}

export interface GetM3AggregatorTag {
    /**
     * Service tag key
     */
    key: string;
    /**
     * Service tag value
     */
    value: string;
}

export interface GetM3AggregatorTechEmail {
    /**
     * An email address to contact for technical issues
     */
    email: string;
}

export interface GetM3DbComponent {
    /**
     * Service component name
     */
    component: string;
    /**
     * Connection info for connecting to the service component. This is a combination of host and port.
     */
    connectionUri: string;
    /**
     * Host name for connecting to the service component
     */
    host: string;
    /**
     * Kafka authentication method. This is a value specific to the 'kafka' service component
     */
    kafkaAuthenticationMethod: string;
    /**
     * Port number for connecting to the service component
     */
    port: number;
    /**
     * Network access route
     */
    route: string;
    /**
     * Whether the endpoint is encrypted or accepts plaintext. By default endpoints are always encrypted and this property is only included for service components they may disable encryption
     */
    ssl: boolean;
    /**
     * DNS usage name
     */
    usage: string;
}

export interface GetM3DbM3db {
    /**
     * M3DB cluster URI.
     */
    httpClusterUri: string;
    /**
     * M3DB node URI.
     */
    httpNodeUri: string;
    /**
     * InfluxDB URI.
     */
    influxdbUri: string;
    /**
     * Prometheus remote read URI.
     */
    prometheusRemoteReadUri: string;
    /**
     * Prometheus remote write URI.
     */
    prometheusRemoteWriteUri: string;
    /**
     * M3DB server URIs.
     */
    uris: string[];
}

export interface GetM3DbM3dbUserConfig {
    /**
     * Additional Cloud Regions for Backup Replication.
     */
    additionalBackupRegions?: string;
    /**
     * Serve the web frontend using a custom CNAME pointing to the Aiven DNS name. Example: `grafana.example.org`.
     */
    customDomain?: string;
    /**
     * Allow incoming connections from CIDR address block, e.g. `10.20.0.0/16`
     */
    ipFilterObjects?: outputs.GetM3DbM3dbUserConfigIpFilterObject[];
    /**
     * Allow incoming connections from CIDR address block, e.g. `10.20.0.0/16`.
     */
    ipFilterStrings?: string[];
    /**
     * Allow incoming connections from CIDR address block, e.g. `10.20.0.0/16`.
     *
     * @deprecated Deprecated. Use `ipFilterString` instead.
     */
    ipFilters?: string[];
    /**
     * M3 limits
     */
    limits?: outputs.GetM3DbM3dbUserConfigLimits;
    /**
     * M3 specific configuration options
     */
    m3?: outputs.GetM3DbM3dbUserConfigM3;
    /**
     * Enum: `1.1`, `1.2`, `1.5`, and newer. M3 major version (deprecated, use m3db_version).
     */
    m3Version?: string;
    /**
     * Enables access to Graphite Carbon plaintext metrics ingestion. It can be enabled only for services inside VPCs. The metrics are written to aggregated namespaces only.
     */
    m3coordinatorEnableGraphiteCarbonIngest?: boolean;
    /**
     * Enum: `1.1`, `1.2`, `1.5`, and newer. M3 major version (the minimum compatible version).
     */
    m3dbVersion?: string;
    /**
     * List of M3 namespaces
     */
    namespaces?: outputs.GetM3DbM3dbUserConfigNamespace[];
    /**
     * Allow access to selected service ports from private networks
     */
    privateAccess?: outputs.GetM3DbM3dbUserConfigPrivateAccess;
    /**
     * Name of another project to fork a service from. This has effect only when a new service is being created. Example: `anotherprojectname`.
     */
    projectToForkFrom?: string;
    /**
     * Allow access to selected service ports from the public Internet
     */
    publicAccess?: outputs.GetM3DbM3dbUserConfigPublicAccess;
    /**
     * M3 rules
     */
    rules?: outputs.GetM3DbM3dbUserConfigRules;
    /**
     * Store logs for the service so that they are available in the HTTP API and console.
     */
    serviceLog?: boolean;
    /**
     * Name of another service to fork from. This has effect only when a new service is being created. Example: `anotherservicename`.
     */
    serviceToForkFrom?: string;
    /**
     * Use static public IP addresses.
     */
    staticIps?: boolean;
}

export interface GetM3DbM3dbUserConfigIpFilterObject {
    /**
     * Description for IP filter list entry. Example: `Production service IP range`.
     */
    description?: string;
    /**
     * CIDR address block. Example: `10.20.0.0/16`.
     */
    network: string;
}

export interface GetM3DbM3dbUserConfigLimits {
    /**
     * The maximum number of blocks that can be read in a given lookback period. Example: `20000`.
     */
    maxRecentlyQueriedSeriesBlocks?: number;
    /**
     * The maximum number of disk bytes that can be read in a given lookback period. Example: `104857600`.
     */
    maxRecentlyQueriedSeriesDiskBytesRead?: number;
    /**
     * The lookback period for `maxRecentlyQueriedSeriesBlocks` and `maxRecentlyQueriedSeriesDiskBytesRead`. Example: `15s`.
     */
    maxRecentlyQueriedSeriesLookback?: string;
    /**
     * The maximum number of docs fetched in single query. Example: `100000`.
     */
    queryDocs?: number;
    /**
     * When query limits are exceeded, whether to return error or return partial results.
     */
    queryRequireExhaustive?: boolean;
    /**
     * The maximum number of series fetched in single query. Example: `100000`.
     */
    querySeries?: number;
}

export interface GetM3DbM3dbUserConfigM3 {
    /**
     * M3 Tag Options
     */
    tagOptions?: outputs.GetM3DbM3dbUserConfigM3TagOptions;
}

export interface GetM3DbM3dbUserConfigM3TagOptions {
    /**
     * Allows for duplicate tags to appear on series (not allowed by default).
     */
    allowTagNameDuplicates?: boolean;
    /**
     * Allows for empty tags to appear on series (not allowed by default).
     */
    allowTagValueEmpty?: boolean;
}

export interface GetM3DbM3dbUserConfigNamespace {
    /**
     * The name of the namespace. Example: `default`.
     */
    name: string;
    /**
     * Namespace options
     */
    options?: outputs.GetM3DbM3dbUserConfigNamespaceOptions;
    /**
     * The resolution for an aggregated namespace. Example: `30s`.
     */
    resolution?: string;
    /**
     * Enum: `aggregated`, `unaggregated`. The type of aggregation (aggregated/unaggregated).
     */
    type: string;
}

export interface GetM3DbM3dbUserConfigNamespaceOptions {
    /**
     * Retention options
     */
    retentionOptions: outputs.GetM3DbM3dbUserConfigNamespaceOptionsRetentionOptions;
    /**
     * Controls whether M3DB will create snapshot files for this namespace.
     */
    snapshotEnabled?: boolean;
    /**
     * Controls whether M3DB will include writes to this namespace in the commitlog.
     */
    writesToCommitlog?: boolean;
}

export interface GetM3DbM3dbUserConfigNamespaceOptionsRetentionOptions {
    /**
     * Controls how long we wait before expiring stale data. Example: `5m`.
     */
    blockDataExpiryDuration?: string;
    /**
     * Controls how long to keep a block in memory before flushing to a fileset on disk. Example: `2h`.
     */
    blocksizeDuration?: string;
    /**
     * Controls how far into the future writes to the namespace will be accepted. Example: `10m`.
     */
    bufferFutureDuration?: string;
    /**
     * Controls how far into the past writes to the namespace will be accepted. Example: `10m`.
     */
    bufferPastDuration?: string;
    /**
     * Controls the duration of time that M3DB will retain data for the namespace. Example: `48h`.
     */
    retentionPeriodDuration?: string;
}

export interface GetM3DbM3dbUserConfigPrivateAccess {
    /**
     * Allow clients to connect to m3coordinator with a DNS name that always resolves to the service's private IP addresses. Only available in certain network locations.
     */
    m3coordinator?: boolean;
}

export interface GetM3DbM3dbUserConfigPublicAccess {
    /**
     * Allow clients to connect to m3coordinator from the public internet for service nodes that are in a project VPC or another type of private network.
     */
    m3coordinator?: boolean;
}

export interface GetM3DbM3dbUserConfigRules {
    /**
     * List of M3 mapping rules
     */
    mappings?: outputs.GetM3DbM3dbUserConfigRulesMapping[];
}

export interface GetM3DbM3dbUserConfigRulesMapping {
    /**
     * List of aggregations to be applied.
     */
    aggregations?: string[];
    /**
     * Only store the derived metric (as specified in the roll-up rules), if any.
     */
    drop?: boolean;
    /**
     * Matching metric names with wildcards (using __name__:wildcard) or matching tags and their (optionally wildcarded) values. For value, ! can be used at start of value for negation, and multiple filters can be supplied using space as separator. Example: `__name__:disk_* host:important-42 mount:!*&#47;sda`.
     */
    filter: string;
    /**
     * The (optional) name of the rule. Example: `important disk metrics`.
     */
    name?: string;
    /**
     * This rule will be used to store the metrics in the given namespace(s). If a namespace is target of rules, the global default aggregation will be automatically disabled. Note that specifying filters that match no namespaces whatsoever will be returned as an error. Filter the namespace by glob (=wildcards).
     *
     * @deprecated Deprecated. Use `namespacesString` instead.
     */
    namespaces?: string[];
    /**
     * This rule will be used to store the metrics in the given namespace(s). If a namespace is target of rules, the global default aggregation will be automatically disabled. Note that specifying filters that match no namespaces whatsoever will be returned as an error. Filter the namespace by exact match of retention period and resolution
     */
    namespacesObjects?: outputs.GetM3DbM3dbUserConfigRulesMappingNamespacesObject[];
    /**
     * This rule will be used to store the metrics in the given namespace(s). If a namespace is target of rules, the global default aggregation will be automatically disabled. Note that specifying filters that match no namespaces whatsoever will be returned as an error. Filter the namespace by glob (=wildcards).
     */
    namespacesStrings?: string[];
    /**
     * List of tags to be appended to matching metrics
     */
    tags?: outputs.GetM3DbM3dbUserConfigRulesMappingTag[];
}

export interface GetM3DbM3dbUserConfigRulesMappingNamespacesObject {
    /**
     * The resolution for the matching namespace. Example: `30s`.
     */
    resolution: string;
    /**
     * The retention period of the matching namespace. Example: `48h`.
     */
    retention?: string;
}

export interface GetM3DbM3dbUserConfigRulesMappingTag {
    /**
     * Name of the tag. Example: `myTag`.
     */
    name: string;
    /**
     * Value of the tag. Example: `myValue`.
     */
    value: string;
}

export interface GetM3DbServiceIntegration {
    /**
     * Type of the service integration. The only supported value at the moment is `readReplica`
     */
    integrationType: string;
    /**
     * Name of the source service
     */
    sourceServiceName: string;
}

export interface GetM3DbTag {
    /**
     * Service tag key
     */
    key: string;
    /**
     * Service tag value
     */
    value: string;
}

export interface GetM3DbTechEmail {
    /**
     * An email address to contact for technical issues
     */
    email: string;
}

export interface GetMySqlComponent {
    /**
     * Service component name
     */
    component: string;
    /**
     * Connection info for connecting to the service component. This is a combination of host and port.
     */
    connectionUri: string;
    /**
     * Host name for connecting to the service component
     */
    host: string;
    /**
     * Kafka authentication method. This is a value specific to the 'kafka' service component
     */
    kafkaAuthenticationMethod: string;
    /**
     * Port number for connecting to the service component
     */
    port: number;
    /**
     * Network access route
     */
    route: string;
    /**
     * Whether the endpoint is encrypted or accepts plaintext. By default endpoints are always encrypted and this property is only included for service components they may disable encryption
     */
    ssl: boolean;
    /**
     * DNS usage name
     */
    usage: string;
}

export interface GetMySqlMysql {
    /**
     * MySQL connection parameters
     */
    params: outputs.GetMySqlMysqlParam[];
    /**
     * MySQL replica URI for services with a replica
     */
    replicaUri: string;
    /**
     * MySQL standby connection URIs
     */
    standbyUris: string[];
    /**
     * MySQL syncing connection URIs
     */
    syncingUris: string[];
    /**
     * MySQL master connection URIs
     */
    uris: string[];
}

export interface GetMySqlMysqlParam {
    /**
     * Primary MySQL database name
     */
    databaseName: string;
    /**
     * MySQL host IP or name
     */
    host: string;
    /**
     * MySQL admin user password
     */
    password: string;
    /**
     * MySQL port
     */
    port: number;
    /**
     * MySQL sslmode setting (currently always "require")
     */
    sslmode: string;
    /**
     * MySQL admin user name
     */
    user: string;
}

export interface GetMySqlMysqlUserConfig {
    /**
     * Additional Cloud Regions for Backup Replication.
     */
    additionalBackupRegions?: string;
    /**
     * Custom password for admin user. Defaults to random string. This must be set only when a new service is being created.
     */
    adminPassword?: string;
    /**
     * Custom username for admin user. This must be set only when a new service is being created. Example: `avnadmin`.
     */
    adminUsername?: string;
    /**
     * The hour of day (in UTC) when backup for the service is started. New backup is only started if previous backup has already completed. Example: `3`.
     */
    backupHour?: number;
    /**
     * The minute of an hour when backup for the service is started. New backup is only started if previous backup has already completed. Example: `30`.
     */
    backupMinute?: number;
    /**
     * The minimum amount of time in seconds to keep binlog entries before deletion. This may be extended for services that require binlog entries for longer than the default for example if using the MySQL Debezium Kafka connector. Example: `600`.
     */
    binlogRetentionPeriod?: number;
    /**
     * Allow incoming connections from CIDR address block, e.g. `10.20.0.0/16`
     */
    ipFilterObjects?: outputs.GetMySqlMysqlUserConfigIpFilterObject[];
    /**
     * Allow incoming connections from CIDR address block, e.g. `10.20.0.0/16`.
     */
    ipFilterStrings?: string[];
    /**
     * Allow incoming connections from CIDR address block, e.g. `10.20.0.0/16`.
     *
     * @deprecated Deprecated. Use `ipFilterString` instead.
     */
    ipFilters?: string[];
    /**
     * Migrate data from existing server
     */
    migration?: outputs.GetMySqlMysqlUserConfigMigration;
    /**
     * mysql.conf configuration values
     */
    mysql?: outputs.GetMySqlMysqlUserConfigMysql;
    /**
     * Enum: `8`, and newer. MySQL major version.
     */
    mysqlVersion?: string;
    /**
     * Allow access to selected service ports from private networks
     */
    privateAccess?: outputs.GetMySqlMysqlUserConfigPrivateAccess;
    /**
     * Allow access to selected service components through Privatelink
     */
    privatelinkAccess?: outputs.GetMySqlMysqlUserConfigPrivatelinkAccess;
    /**
     * Name of another project to fork a service from. This has effect only when a new service is being created. Example: `anotherprojectname`.
     */
    projectToForkFrom?: string;
    /**
     * Allow access to selected service ports from the public Internet
     */
    publicAccess?: outputs.GetMySqlMysqlUserConfigPublicAccess;
    /**
     * Recovery target time when forking a service. This has effect only when a new service is being created. Example: `2019-01-01 23:34:45`.
     */
    recoveryTargetTime?: string;
    /**
     * Store logs for the service so that they are available in the HTTP API and console.
     */
    serviceLog?: boolean;
    /**
     * Name of another service to fork from. This has effect only when a new service is being created. Example: `anotherservicename`.
     */
    serviceToForkFrom?: string;
    /**
     * Use static public IP addresses.
     */
    staticIps?: boolean;
}

export interface GetMySqlMysqlUserConfigIpFilterObject {
    /**
     * Description for IP filter list entry. Example: `Production service IP range`.
     */
    description?: string;
    /**
     * CIDR address block. Example: `10.20.0.0/16`.
     */
    network: string;
}

export interface GetMySqlMysqlUserConfigMigration {
    /**
     * Database name for bootstrapping the initial connection. Example: `defaultdb`.
     */
    dbname?: string;
    /**
     * Hostname or IP address of the server where to migrate data from. Example: `my.server.com`.
     */
    host: string;
    /**
     * Comma-separated list of databases, which should be ignored during migration (supported by MySQL and PostgreSQL only at the moment). Example: `db1,db2`.
     */
    ignoreDbs?: string;
    /**
     * Comma-separated list of database roles, which should be ignored during migration (supported by PostgreSQL only at the moment). Example: `role1,role2`.
     */
    ignoreRoles?: string;
    /**
     * Enum: `dump`, `replication`. The migration method to be used (currently supported only by Redis, Dragonfly, MySQL and PostgreSQL service types).
     */
    method?: string;
    /**
     * Password for authentication with the server where to migrate data from. Example: `jjKk45Nnd`.
     */
    password?: string;
    /**
     * Port number of the server where to migrate data from. Example: `1234`.
     */
    port: number;
    /**
     * The server where to migrate data from is secured with SSL. Default: `true`.
     */
    ssl?: boolean;
    /**
     * User name for authentication with the server where to migrate data from. Example: `myname`.
     */
    username?: string;
}

export interface GetMySqlMysqlUserConfigMysql {
    /**
     * The number of seconds that the mysqld server waits for a connect packet before responding with Bad handshake. Example: `10`.
     */
    connectTimeout?: number;
    /**
     * Default server time zone as an offset from UTC (from -12:00 to +12:00), a time zone name, or `SYSTEM` to use the MySQL server default. Example: `+03:00`.
     */
    defaultTimeZone?: string;
    /**
     * The maximum permitted result length in bytes for the GROUP_CONCAT() function. Example: `1024`.
     */
    groupConcatMaxLen?: number;
    /**
     * The time, in seconds, before cached statistics expire. Example: `86400`.
     */
    informationSchemaStatsExpiry?: number;
    /**
     * Maximum size for the InnoDB change buffer, as a percentage of the total size of the buffer pool. Default is 25. Example: `30`.
     */
    innodbChangeBufferMaxSize?: number;
    /**
     * Specifies whether flushing a page from the InnoDB buffer pool also flushes other dirty pages in the same extent (default is 1): 0 - dirty pages in the same extent are not flushed, 1 - flush contiguous dirty pages in the same extent, 2 - flush dirty pages in the same extent. Example: `0`.
     */
    innodbFlushNeighbors?: number;
    /**
     * Minimum length of words that are stored in an InnoDB FULLTEXT index. Changing this parameter will lead to a restart of the MySQL service. Example: `3`.
     */
    innodbFtMinTokenSize?: number;
    /**
     * This option is used to specify your own InnoDB FULLTEXT index stopword list for all InnoDB tables. Example: `db_name/table_name`.
     */
    innodbFtServerStopwordTable?: string;
    /**
     * The length of time in seconds an InnoDB transaction waits for a row lock before giving up. Default is 120. Example: `50`.
     */
    innodbLockWaitTimeout?: number;
    /**
     * The size in bytes of the buffer that InnoDB uses to write to the log files on disk. Example: `16777216`.
     */
    innodbLogBufferSize?: number;
    /**
     * The upper limit in bytes on the size of the temporary log files used during online DDL operations for InnoDB tables. Example: `134217728`.
     */
    innodbOnlineAlterLogMaxSize?: number;
    /**
     * When enabled, information about all deadlocks in InnoDB user transactions is recorded in the error log. Disabled by default.
     */
    innodbPrintAllDeadlocks?: boolean;
    /**
     * The number of I/O threads for read operations in InnoDB. Default is 4. Changing this parameter will lead to a restart of the MySQL service. Example: `10`.
     */
    innodbReadIoThreads?: number;
    /**
     * When enabled a transaction timeout causes InnoDB to abort and roll back the entire transaction. Changing this parameter will lead to a restart of the MySQL service.
     */
    innodbRollbackOnTimeout?: boolean;
    /**
     * Defines the maximum number of threads permitted inside of InnoDB. Default is 0 (infinite concurrency - no limit). Example: `10`.
     */
    innodbThreadConcurrency?: number;
    /**
     * The number of I/O threads for write operations in InnoDB. Default is 4. Changing this parameter will lead to a restart of the MySQL service. Example: `10`.
     */
    innodbWriteIoThreads?: number;
    /**
     * The number of seconds the server waits for activity on an interactive connection before closing it. Example: `3600`.
     */
    interactiveTimeout?: number;
    /**
     * Enum: `TempTable`, `MEMORY`. The storage engine for in-memory internal temporary tables.
     */
    internalTmpMemStorageEngine?: string;
    /**
     * The slowQueryLogs work as SQL statements that take more than longQueryTime seconds to execute. Default is 10s. Example: `10`.
     */
    longQueryTime?: number;
    /**
     * Size of the largest message in bytes that can be received by the server. Default is 67108864 (64M). Example: `67108864`.
     */
    maxAllowedPacket?: number;
    /**
     * Limits the size of internal in-memory tables. Also set tmp_table_size. Default is 16777216 (16M). Example: `16777216`.
     */
    maxHeapTableSize?: number;
    /**
     * Start sizes of connection buffer and result buffer. Default is 16384 (16K). Changing this parameter will lead to a restart of the MySQL service. Example: `16384`.
     */
    netBufferLength?: number;
    /**
     * The number of seconds to wait for more data from a connection before aborting the read. Example: `30`.
     */
    netReadTimeout?: number;
    /**
     * The number of seconds to wait for a block to be written to a connection before aborting the write. Example: `30`.
     */
    netWriteTimeout?: number;
    /**
     * Slow query log enables capturing of slow queries. Setting slowQueryLog to false also truncates the mysql.slow_log table. Default is off.
     */
    slowQueryLog?: boolean;
    /**
     * Sort buffer size in bytes for ORDER BY optimization. Default is 262144 (256K). Example: `262144`.
     */
    sortBufferSize?: number;
    /**
     * Global SQL mode. Set to empty to use MySQL server defaults. When creating a new service and not setting this field Aiven default SQL mode (strict, SQL standard compliant) will be assigned. Example: `ANSI,TRADITIONAL`.
     */
    sqlMode?: string;
    /**
     * Require primary key to be defined for new tables or old tables modified with ALTER TABLE and fail if missing. It is recommended to always have primary keys because various functionality may break if any large table is missing them.
     */
    sqlRequirePrimaryKey?: boolean;
    /**
     * Limits the size of internal in-memory tables. Also set max_heap_table_size. Default is 16777216 (16M). Example: `16777216`.
     */
    tmpTableSize?: number;
    /**
     * The number of seconds the server waits for activity on a noninteractive connection before closing it. Example: `28800`.
     */
    waitTimeout?: number;
}

export interface GetMySqlMysqlUserConfigPrivateAccess {
    /**
     * Allow clients to connect to mysql with a DNS name that always resolves to the service's private IP addresses. Only available in certain network locations.
     */
    mysql?: boolean;
    /**
     * Allow clients to connect to mysqlx with a DNS name that always resolves to the service's private IP addresses. Only available in certain network locations.
     */
    mysqlx?: boolean;
    /**
     * Allow clients to connect to prometheus with a DNS name that always resolves to the service's private IP addresses. Only available in certain network locations.
     */
    prometheus?: boolean;
}

export interface GetMySqlMysqlUserConfigPrivatelinkAccess {
    /**
     * Enable mysql.
     */
    mysql?: boolean;
    /**
     * Enable mysqlx.
     */
    mysqlx?: boolean;
    /**
     * Enable prometheus.
     */
    prometheus?: boolean;
}

export interface GetMySqlMysqlUserConfigPublicAccess {
    /**
     * Allow clients to connect to mysql from the public internet for service nodes that are in a project VPC or another type of private network.
     */
    mysql?: boolean;
    /**
     * Allow clients to connect to mysqlx from the public internet for service nodes that are in a project VPC or another type of private network.
     */
    mysqlx?: boolean;
    /**
     * Allow clients to connect to prometheus from the public internet for service nodes that are in a project VPC or another type of private network.
     */
    prometheus?: boolean;
}

export interface GetMySqlServiceIntegration {
    /**
     * Type of the service integration. The only supported value at the moment is `readReplica`
     */
    integrationType: string;
    /**
     * Name of the source service
     */
    sourceServiceName: string;
}

export interface GetMySqlTag {
    /**
     * Service tag key
     */
    key: string;
    /**
     * Service tag value
     */
    value: string;
}

export interface GetMySqlTechEmail {
    /**
     * An email address to contact for technical issues
     */
    email: string;
}

export interface GetOpenSearchComponent {
    /**
     * Service component name
     */
    component: string;
    /**
     * Connection info for connecting to the service component. This is a combination of host and port.
     */
    connectionUri: string;
    /**
     * Host name for connecting to the service component
     */
    host: string;
    /**
     * Kafka authentication method. This is a value specific to the 'kafka' service component
     */
    kafkaAuthenticationMethod: string;
    /**
     * Port number for connecting to the service component
     */
    port: number;
    /**
     * Network access route
     */
    route: string;
    /**
     * Whether the endpoint is encrypted or accepts plaintext. By default endpoints are always encrypted and this property is only included for service components they may disable encryption
     */
    ssl: boolean;
    /**
     * DNS usage name
     */
    usage: string;
}

export interface GetOpenSearchOpensearch {
    /**
     * URI for Kibana dashboard frontend
     *
     * @deprecated This field was added by mistake and has never worked. It will be removed in future versions.
     */
    kibanaUri: string;
    /**
     * URI for OpenSearch dashboard frontend
     */
    opensearchDashboardsUri: string;
    /**
     * OpenSearch password
     */
    password: string;
    /**
     * OpenSearch server URIs.
     */
    uris: string[];
    /**
     * OpenSearch username
     */
    username: string;
}

export interface GetOpenSearchOpensearchUserConfig {
    /**
     * Additional Cloud Regions for Backup Replication.
     */
    additionalBackupRegions?: string;
    azureMigration?: outputs.GetOpenSearchOpensearchUserConfigAzureMigration;
    /**
     * Serve the web frontend using a custom CNAME pointing to the Aiven DNS name. Example: `grafana.example.org`.
     */
    customDomain?: string;
    /**
     * Disable automatic replication factor adjustment for multi-node services. By default, Aiven ensures all indexes are replicated at least to two nodes. Note: Due to potential data loss in case of losing a service node, this setting can no longer be activated.
     */
    disableReplicationFactorAdjustment?: boolean;
    gcsMigration?: outputs.GetOpenSearchOpensearchUserConfigGcsMigration;
    /**
     * Index patterns
     */
    indexPatterns?: outputs.GetOpenSearchOpensearchUserConfigIndexPattern[];
    /**
     * Index rollup settings
     */
    indexRollup?: outputs.GetOpenSearchOpensearchUserConfigIndexRollup;
    /**
     * Template settings for all new indexes
     */
    indexTemplate?: outputs.GetOpenSearchOpensearchUserConfigIndexTemplate;
    /**
     * Allow incoming connections from CIDR address block, e.g. `10.20.0.0/16`
     */
    ipFilterObjects?: outputs.GetOpenSearchOpensearchUserConfigIpFilterObject[];
    /**
     * Allow incoming connections from CIDR address block, e.g. `10.20.0.0/16`.
     */
    ipFilterStrings?: string[];
    /**
     * Allow incoming connections from CIDR address block, e.g. `10.20.0.0/16`.
     *
     * @deprecated Deprecated. Use `ipFilterString` instead.
     */
    ipFilters?: string[];
    /**
     * Aiven automation resets index.refresh_interval to default value for every index to be sure that indices are always visible to search. If it doesn't fit your case, you can disable this by setting up this flag to true.
     */
    keepIndexRefreshInterval?: boolean;
    /**
     * Use indexPatterns instead. Default: `0`.
     */
    maxIndexCount?: number;
    /**
     * OpenSearch OpenID Connect Configuration
     */
    openid?: outputs.GetOpenSearchOpensearchUserConfigOpenid;
    /**
     * OpenSearch settings
     */
    opensearch?: outputs.GetOpenSearchOpensearchUserConfigOpensearch;
    /**
     * OpenSearch Dashboards settings
     */
    opensearchDashboards?: outputs.GetOpenSearchOpensearchUserConfigOpensearchDashboards;
    /**
     * Enum: `1`, `2`, and newer. OpenSearch major version.
     */
    opensearchVersion?: string;
    /**
     * Allow access to selected service ports from private networks
     */
    privateAccess?: outputs.GetOpenSearchOpensearchUserConfigPrivateAccess;
    /**
     * Allow access to selected service components through Privatelink
     */
    privatelinkAccess?: outputs.GetOpenSearchOpensearchUserConfigPrivatelinkAccess;
    /**
     * Name of another project to fork a service from. This has effect only when a new service is being created. Example: `anotherprojectname`.
     */
    projectToForkFrom?: string;
    /**
     * Allow access to selected service ports from the public Internet
     */
    publicAccess?: outputs.GetOpenSearchOpensearchUserConfigPublicAccess;
    /**
     * Name of the basebackup to restore in forked service. Example: `backup-20191112t091354293891z`.
     */
    recoveryBasebackupName?: string;
    s3Migration?: outputs.GetOpenSearchOpensearchUserConfigS3Migration;
    /**
     * OpenSearch SAML configuration
     */
    saml?: outputs.GetOpenSearchOpensearchUserConfigSaml;
    /**
     * Store logs for the service so that they are available in the HTTP API and console.
     */
    serviceLog?: boolean;
    /**
     * Name of another service to fork from. This has effect only when a new service is being created. Example: `anotherservicename`.
     */
    serviceToForkFrom?: string;
    /**
     * Use static public IP addresses.
     */
    staticIps?: boolean;
}

export interface GetOpenSearchOpensearchUserConfigAzureMigration {
    /**
     * Azure account name.
     */
    account: string;
    /**
     * The path to the repository data within its container. The value of this setting should not start or end with a /.
     */
    basePath: string;
    /**
     * Big files can be broken down into chunks during snapshotting if needed. Should be the same as for the 3rd party repository.
     */
    chunkSize?: string;
    /**
     * When set to true metadata files are stored in compressed format.
     */
    compress?: boolean;
    /**
     * Azure container name.
     */
    container: string;
    /**
     * Defines the DNS suffix for Azure Storage endpoints.
     */
    endpointSuffix?: string;
    /**
     * A comma-delimited list of indices to restore from the snapshot. Multi-index syntax is supported. By default, a restore operation includes all data streams and indices in the snapshot. If this argument is provided, the restore operation only includes the data streams and indices that you specify. Example: `metrics*,logs*,data-20240823`.
     */
    indices?: string;
    /**
     * Azure account secret key. One of key or sasToken should be specified.
     */
    key?: string;
    /**
     * A shared access signatures (SAS) token. One of key or sasToken should be specified.
     */
    sasToken?: string;
    /**
     * The snapshot name to restore from.
     */
    snapshotName: string;
}

export interface GetOpenSearchOpensearchUserConfigGcsMigration {
    /**
     * The path to the repository data within its container. The value of this setting should not start or end with a /.
     */
    basePath: string;
    /**
     * The path to the repository data within its container.
     */
    bucket: string;
    /**
     * Big files can be broken down into chunks during snapshotting if needed. Should be the same as for the 3rd party repository.
     */
    chunkSize?: string;
    /**
     * When set to true metadata files are stored in compressed format.
     */
    compress?: boolean;
    /**
     * Google Cloud Storage credentials file content.
     */
    credentials: string;
    /**
     * A comma-delimited list of indices to restore from the snapshot. Multi-index syntax is supported. By default, a restore operation includes all data streams and indices in the snapshot. If this argument is provided, the restore operation only includes the data streams and indices that you specify. Example: `metrics*,logs*,data-20240823`.
     */
    indices?: string;
    /**
     * The snapshot name to restore from.
     */
    snapshotName: string;
}

export interface GetOpenSearchOpensearchUserConfigIndexPattern {
    /**
     * Maximum number of indexes to keep. Example: `3`.
     */
    maxIndexCount: number;
    /**
     * fnmatch pattern. Example: `logs_*_foo_*`.
     */
    pattern: string;
    /**
     * Enum: `alphabetical`, `creationDate`. Deletion sorting algorithm. Default: `creationDate`.
     */
    sortingAlgorithm?: string;
}

export interface GetOpenSearchOpensearchUserConfigIndexRollup {
    /**
     * Whether rollups are enabled in OpenSearch Dashboards. Defaults to true.
     */
    rollupDashboardsEnabled?: boolean;
    /**
     * Whether the rollup plugin is enabled. Defaults to true.
     */
    rollupEnabled?: boolean;
    /**
     * How many retries the plugin should attempt for failed rollup jobs. Defaults to 5.
     */
    rollupSearchBackoffCount?: number;
    /**
     * The backoff time between retries for failed rollup jobs. Defaults to 1000ms.
     */
    rollupSearchBackoffMillis?: number;
    /**
     * Whether OpenSearch should return all jobs that match all specified search terms. If disabled, OpenSearch returns just one, as opposed to all, of the jobs that matches the search terms. Defaults to false.
     */
    rollupSearchSearchAllJobs?: boolean;
}

export interface GetOpenSearchOpensearchUserConfigIndexTemplate {
    /**
     * The maximum number of nested JSON objects that a single document can contain across all nested types. This limit helps to prevent out of memory errors when a document contains too many nested objects. Default is 10000. Example: `10000`.
     */
    mappingNestedObjectsLimit?: number;
    /**
     * The number of replicas each primary shard has. Example: `1`.
     */
    numberOfReplicas?: number;
    /**
     * The number of primary shards that an index should have. Example: `1`.
     */
    numberOfShards?: number;
}

export interface GetOpenSearchOpensearchUserConfigIpFilterObject {
    /**
     * Description for IP filter list entry. Example: `Production service IP range`.
     */
    description?: string;
    /**
     * CIDR address block. Example: `10.20.0.0/16`.
     */
    network: string;
}

export interface GetOpenSearchOpensearchUserConfigOpenid {
    /**
     * The ID of the OpenID Connect client configured in your IdP. Required.
     */
    clientId: string;
    /**
     * The client secret of the OpenID Connect client configured in your IdP. Required.
     */
    clientSecret: string;
    /**
     * The URL of your IdP where the Security plugin can find the OpenID Connect metadata/configuration settings. Example: `https://test-account.okta.com/app/exk491jujcVc83LEX697/sso/saml/metadata`.
     */
    connectUrl: string;
    /**
     * Enables or disables OpenID Connect authentication for OpenSearch. When enabled, users can authenticate using OpenID Connect with an Identity Provider. Default: `true`.
     */
    enabled: boolean;
    /**
     * HTTP header name of the JWT token. Optional. Default is Authorization. Default: `Authorization`.
     */
    header?: string;
    /**
     * The HTTP header that stores the token. Typically the Authorization header with the Bearer schema: Authorization: Bearer <token>. Optional. Default is Authorization. Example: `preferredUsername`.
     */
    jwtHeader?: string;
    /**
     * If the token is not transmitted in the HTTP header, but as an URL parameter, define the name of the parameter here. Optional. Example: `preferredUsername`.
     */
    jwtUrlParameter?: string;
    /**
     * The maximum number of unknown key IDs in the time frame. Default is 10. Optional. Default: `10`.
     */
    refreshRateLimitCount?: number;
    /**
     * The time frame to use when checking the maximum number of unknown key IDs, in milliseconds. Optional.Default is 10000 (10 seconds). Default: `10000`.
     */
    refreshRateLimitTimeWindowMs?: number;
    /**
     * The key in the JSON payload that stores the user’s roles. The value of this key must be a comma-separated list of roles. Required only if you want to use roles in the JWT. Example: `roles`.
     */
    rolesKey?: string;
    /**
     * The scope of the identity token issued by the IdP. Optional. Default is openid profile email address phone.
     */
    scope?: string;
    /**
     * The key in the JSON payload that stores the user’s name. If not defined, the subject registered claim is used. Most IdP providers use the preferredUsername claim. Optional. Example: `preferredUsername`.
     */
    subjectKey?: string;
}

export interface GetOpenSearchOpensearchUserConfigOpensearch {
    /**
     * Explicitly allow or block automatic creation of indices. Defaults to true.
     */
    actionAutoCreateIndexEnabled?: boolean;
    /**
     * Require explicit index names when deleting.
     */
    actionDestructiveRequiresName?: boolean;
    /**
     * Opensearch Security Plugin Settings
     */
    authFailureListeners?: outputs.GetOpenSearchOpensearchUserConfigOpensearchAuthFailureListeners;
    /**
     * Controls the number of shards allowed in the cluster per data node. Example: `1000`.
     */
    clusterMaxShardsPerNode?: number;
    /**
     * How many concurrent incoming/outgoing shard recoveries (normally replicas) are allowed to happen on a node. Defaults to node cpu count * 2.
     */
    clusterRoutingAllocationNodeConcurrentRecoveries?: number;
    /**
     * Sender name placeholder to be used in Opensearch Dashboards and Opensearch keystore. Example: `alert-sender`.
     */
    emailSenderName?: string;
    /**
     * Sender password for Opensearch alerts to authenticate with SMTP server. Example: `very-secure-mail-password`.
     */
    emailSenderPassword?: string;
    /**
     * Sender username for Opensearch alerts. Example: `jane@example.com`.
     */
    emailSenderUsername?: string;
    /**
     * Enable/Disable security audit.
     */
    enableSecurityAudit?: boolean;
    /**
     * Maximum content length for HTTP requests to the OpenSearch HTTP API, in bytes.
     */
    httpMaxContentLength?: number;
    /**
     * The max size of allowed headers, in bytes. Example: `8192`.
     */
    httpMaxHeaderSize?: number;
    /**
     * The max length of an HTTP URL, in bytes. Example: `4096`.
     */
    httpMaxInitialLineLength?: number;
    /**
     * Relative amount. Maximum amount of heap memory used for field data cache. This is an expert setting; decreasing the value too much will increase overhead of loading field data; too much memory used for field data cache will decrease amount of heap available for other operations.
     */
    indicesFielddataCacheSize?: number;
    /**
     * Percentage value. Default is 10%. Total amount of heap used for indexing buffer, before writing segments to disk. This is an expert setting. Too low value will slow down indexing; too high value will increase indexing performance but causes performance issues for query performance.
     */
    indicesMemoryIndexBufferSize?: number;
    /**
     * Absolute value. Default is unbound. Doesn't work without indices.memory.index_buffer_size. Maximum amount of heap used for query cache, an absolute indices.memory.index_buffer_size maximum hard limit.
     */
    indicesMemoryMaxIndexBufferSize?: number;
    /**
     * Absolute value. Default is 48mb. Doesn't work without indices.memory.index_buffer_size. Minimum amount of heap used for query cache, an absolute indices.memory.index_buffer_size minimal hard limit.
     */
    indicesMemoryMinIndexBufferSize?: number;
    /**
     * Percentage value. Default is 10%. Maximum amount of heap used for query cache. This is an expert setting. Too low value will decrease query performance and increase performance for other operations; too high value will cause issues with other OpenSearch functionality.
     */
    indicesQueriesCacheSize?: number;
    /**
     * Maximum number of clauses Lucene BooleanQuery can have. The default value (1024) is relatively high, and increasing it may cause performance issues. Investigate other approaches first before increasing this value.
     */
    indicesQueryBoolMaxClauseCount?: number;
    /**
     * Limits total inbound and outbound recovery traffic for each node. Applies to both peer recoveries as well as snapshot recoveries (i.e., restores from a snapshot). Defaults to 40mb.
     */
    indicesRecoveryMaxBytesPerSec?: number;
    /**
     * Number of file chunks sent in parallel for each recovery. Defaults to 2.
     */
    indicesRecoveryMaxConcurrentFileChunks?: number;
    /**
     * Specifies whether ISM is enabled or not.
     */
    ismEnabled?: boolean;
    /**
     * Specifies whether audit history is enabled or not. The logs from ISM are automatically indexed to a logs document.
     */
    ismHistoryEnabled?: boolean;
    /**
     * The maximum age before rolling over the audit history index in hours. Example: `24`.
     */
    ismHistoryMaxAge?: number;
    /**
     * The maximum number of documents before rolling over the audit history index. Example: `2500000`.
     */
    ismHistoryMaxDocs?: number;
    /**
     * The time between rollover checks for the audit history index in hours. Example: `8`.
     */
    ismHistoryRolloverCheckPeriod?: number;
    /**
     * How long audit history indices are kept in days. Example: `30`.
     */
    ismHistoryRolloverRetentionPeriod?: number;
    /**
     * Enable or disable KNN memory circuit breaker. Defaults to true.
     */
    knnMemoryCircuitBreakerEnabled?: boolean;
    /**
     * Maximum amount of memory that can be used for KNN index. Defaults to 50% of the JVM heap size.
     */
    knnMemoryCircuitBreakerLimit?: number;
    /**
     * Compatibility mode sets OpenSearch to report its version as 7.10 so clients continue to work. Default is false.
     */
    overrideMainResponseVersion?: boolean;
    /**
     * Enable or disable filtering of alerting by backend roles. Requires Security plugin. Defaults to false.
     */
    pluginsAlertingFilterByBackendRoles?: boolean;
    /**
     * Whitelisted addresses for reindexing. Changing this value will cause all OpenSearch instances to restart.
     */
    reindexRemoteWhitelists?: string[];
    /**
     * Script compilation circuit breaker limits the number of inline script compilations within a period of time. Default is use-context. Example: `75/5m`.
     */
    scriptMaxCompilationsRate?: string;
    /**
     * Maximum number of aggregation buckets allowed in a single response. OpenSearch default value is used when this is not defined. Example: `10000`.
     */
    searchMaxBuckets?: number;
    /**
     * Size for the thread pool queue. See documentation for exact details.
     */
    threadPoolAnalyzeQueueSize?: number;
    /**
     * Size for the thread pool. See documentation for exact details. Do note this may have maximum value depending on CPU count - value is automatically lowered if set to higher than maximum value.
     */
    threadPoolAnalyzeSize?: number;
    /**
     * Size for the thread pool. See documentation for exact details. Do note this may have maximum value depending on CPU count - value is automatically lowered if set to higher than maximum value.
     */
    threadPoolForceMergeSize?: number;
    /**
     * Size for the thread pool queue. See documentation for exact details.
     */
    threadPoolGetQueueSize?: number;
    /**
     * Size for the thread pool. See documentation for exact details. Do note this may have maximum value depending on CPU count - value is automatically lowered if set to higher than maximum value.
     */
    threadPoolGetSize?: number;
    /**
     * Size for the thread pool queue. See documentation for exact details.
     */
    threadPoolSearchQueueSize?: number;
    /**
     * Size for the thread pool. See documentation for exact details. Do note this may have maximum value depending on CPU count - value is automatically lowered if set to higher than maximum value.
     */
    threadPoolSearchSize?: number;
    /**
     * Size for the thread pool queue. See documentation for exact details.
     */
    threadPoolSearchThrottledQueueSize?: number;
    /**
     * Size for the thread pool. See documentation for exact details. Do note this may have maximum value depending on CPU count - value is automatically lowered if set to higher than maximum value.
     */
    threadPoolSearchThrottledSize?: number;
    /**
     * Size for the thread pool queue. See documentation for exact details.
     */
    threadPoolWriteQueueSize?: number;
    /**
     * Size for the thread pool. See documentation for exact details. Do note this may have maximum value depending on CPU count - value is automatically lowered if set to higher than maximum value.
     */
    threadPoolWriteSize?: number;
}

export interface GetOpenSearchOpensearchUserConfigOpensearchAuthFailureListeners {
    internalAuthenticationBackendLimiting?: outputs.GetOpenSearchOpensearchUserConfigOpensearchAuthFailureListenersInternalAuthenticationBackendLimiting;
    /**
     * IP address rate limiting settings
     */
    ipRateLimiting?: outputs.GetOpenSearchOpensearchUserConfigOpensearchAuthFailureListenersIpRateLimiting;
}

export interface GetOpenSearchOpensearchUserConfigOpensearchAuthFailureListenersInternalAuthenticationBackendLimiting {
    /**
     * The number of login attempts allowed before login is blocked. Example: `10`.
     */
    allowedTries?: number;
    /**
     * Enum: `internal`. internal_authentication_backend_limiting.authentication_backend.
     */
    authenticationBackend?: string;
    /**
     * The duration of time that login remains blocked after a failed login. Example: `600`.
     */
    blockExpirySeconds?: number;
    /**
     * internal_authentication_backend_limiting.max_blocked_clients. Example: `100000`.
     */
    maxBlockedClients?: number;
    /**
     * The maximum number of tracked IP addresses that have failed login. Example: `100000`.
     */
    maxTrackedClients?: number;
    /**
     * The window of time in which the value for `allowedTries` is enforced. Example: `3600`.
     */
    timeWindowSeconds?: number;
    /**
     * Enum: `username`. internal_authentication_backend_limiting.type.
     */
    type?: string;
}

export interface GetOpenSearchOpensearchUserConfigOpensearchAuthFailureListenersIpRateLimiting {
    /**
     * The number of login attempts allowed before login is blocked. Example: `10`.
     */
    allowedTries?: number;
    /**
     * The duration of time that login remains blocked after a failed login. Example: `600`.
     */
    blockExpirySeconds?: number;
    /**
     * The maximum number of blocked IP addresses. Example: `100000`.
     */
    maxBlockedClients?: number;
    /**
     * The maximum number of tracked IP addresses that have failed login. Example: `100000`.
     */
    maxTrackedClients?: number;
    /**
     * The window of time in which the value for `allowedTries` is enforced. Example: `3600`.
     */
    timeWindowSeconds?: number;
    /**
     * Enum: `ip`. The type of rate limiting.
     */
    type?: string;
}

export interface GetOpenSearchOpensearchUserConfigOpensearchDashboards {
    /**
     * Enable or disable OpenSearch Dashboards. Default: `true`.
     */
    enabled?: boolean;
    /**
     * Limits the maximum amount of memory (in MiB) the OpenSearch Dashboards process can use. This sets the maxOldSpaceSize option of the nodejs running the OpenSearch Dashboards. Note: the memory reserved by OpenSearch Dashboards is not available for OpenSearch. Default: `128`.
     */
    maxOldSpaceSize?: number;
    /**
     * Timeout in milliseconds for requests made by OpenSearch Dashboards towards OpenSearch. Default: `30000`.
     */
    opensearchRequestTimeout?: number;
}

export interface GetOpenSearchOpensearchUserConfigPrivateAccess {
    /**
     * Allow clients to connect to opensearch with a DNS name that always resolves to the service's private IP addresses. Only available in certain network locations.
     */
    opensearch?: boolean;
    /**
     * Allow clients to connect to opensearchDashboards with a DNS name that always resolves to the service's private IP addresses. Only available in certain network locations.
     */
    opensearchDashboards?: boolean;
    /**
     * Allow clients to connect to prometheus with a DNS name that always resolves to the service's private IP addresses. Only available in certain network locations.
     */
    prometheus?: boolean;
}

export interface GetOpenSearchOpensearchUserConfigPrivatelinkAccess {
    /**
     * Enable opensearch.
     */
    opensearch?: boolean;
    /**
     * Enable opensearch_dashboards.
     */
    opensearchDashboards?: boolean;
    /**
     * Enable prometheus.
     */
    prometheus?: boolean;
}

export interface GetOpenSearchOpensearchUserConfigPublicAccess {
    /**
     * Allow clients to connect to opensearch from the public internet for service nodes that are in a project VPC or another type of private network.
     */
    opensearch?: boolean;
    /**
     * Allow clients to connect to opensearchDashboards from the public internet for service nodes that are in a project VPC or another type of private network.
     */
    opensearchDashboards?: boolean;
    /**
     * Allow clients to connect to prometheus from the public internet for service nodes that are in a project VPC or another type of private network.
     */
    prometheus?: boolean;
}

export interface GetOpenSearchOpensearchUserConfigS3Migration {
    /**
     * AWS Access key.
     */
    accessKey: string;
    /**
     * The path to the repository data within its container. The value of this setting should not start or end with a /.
     */
    basePath: string;
    /**
     * S3 bucket name.
     */
    bucket: string;
    /**
     * Big files can be broken down into chunks during snapshotting if needed. Should be the same as for the 3rd party repository.
     */
    chunkSize?: string;
    /**
     * When set to true metadata files are stored in compressed format.
     */
    compress?: boolean;
    /**
     * The S3 service endpoint to connect to. If you are using an S3-compatible service then you should set this to the service’s endpoint.
     */
    endpoint?: string;
    /**
     * A comma-delimited list of indices to restore from the snapshot. Multi-index syntax is supported. By default, a restore operation includes all data streams and indices in the snapshot. If this argument is provided, the restore operation only includes the data streams and indices that you specify. Example: `metrics*,logs*,data-20240823`.
     */
    indices?: string;
    /**
     * S3 region.
     */
    region: string;
    /**
     * AWS secret key.
     */
    secretKey: string;
    /**
     * When set to true files are encrypted on server side.
     */
    serverSideEncryption?: boolean;
    /**
     * The snapshot name to restore from.
     */
    snapshotName: string;
}

export interface GetOpenSearchOpensearchUserConfigSaml {
    /**
     * Enables or disables SAML-based authentication for OpenSearch. When enabled, users can authenticate using SAML with an Identity Provider. Default: `true`.
     */
    enabled: boolean;
    /**
     * The unique identifier for the Identity Provider (IdP) entity that is used for SAML authentication. This value is typically provided by the IdP. Example: `test-idp-entity-id`.
     */
    idpEntityId: string;
    /**
     * The URL of the SAML metadata for the Identity Provider (IdP). This is used to configure SAML-based authentication with the IdP. Example: `https://test-account.okta.com/app/exk491jujcVc83LEX697/sso/saml/metadata`.
     */
    idpMetadataUrl: string;
    /**
     * This parameter specifies the PEM-encoded root certificate authority (CA) content for the SAML identity provider (IdP) server verification. The root CA content is used to verify the SSL/TLS certificate presented by the server. Example: `-----BEGIN CERTIFICATE-----
     * ...
     * -----END CERTIFICATE-----
     * `.
     */
    idpPemtrustedcasContent?: string;
    /**
     * Optional. Specifies the attribute in the SAML response where role information is stored, if available. Role attributes are not required for SAML authentication, but can be included in SAML assertions by most Identity Providers (IdPs) to determine user access levels or permissions. Example: `RoleName`.
     */
    rolesKey?: string;
    /**
     * The unique identifier for the Service Provider (SP) entity that is used for SAML authentication. This value is typically provided by the SP. Example: `test-sp-entity-id`.
     */
    spEntityId: string;
    /**
     * Optional. Specifies the attribute in the SAML response where the subject identifier is stored. If not configured, the NameID attribute is used by default. Example: `NameID`.
     */
    subjectKey?: string;
}

export interface GetOpenSearchServiceIntegration {
    /**
     * Type of the service integration. The only supported value at the moment is `readReplica`
     */
    integrationType: string;
    /**
     * Name of the source service
     */
    sourceServiceName: string;
}

export interface GetOpenSearchTag {
    /**
     * Service tag key
     */
    key: string;
    /**
     * Service tag value
     */
    value: string;
}

export interface GetOpenSearchTechEmail {
    /**
     * An email address to contact for technical issues
     */
    email: string;
}

export interface GetPgComponent {
    /**
     * Service component name
     */
    component: string;
    /**
     * Connection info for connecting to the service component. This is a combination of host and port.
     */
    connectionUri: string;
    /**
     * Host name for connecting to the service component
     */
    host: string;
    /**
     * Kafka authentication method. This is a value specific to the 'kafka' service component
     */
    kafkaAuthenticationMethod: string;
    /**
     * Port number for connecting to the service component
     */
    port: number;
    /**
     * Network access route
     */
    route: string;
    /**
     * Whether the endpoint is encrypted or accepts plaintext. By default endpoints are always encrypted and this property is only included for service components they may disable encryption
     */
    ssl: boolean;
    /**
     * DNS usage name
     */
    usage: string;
}

export interface GetPgPg {
    /**
     * PgBouncer connection details for [connection pooling](https://aiven.io/docs/products/postgresql/concepts/pg-connection-pooling).
     *
     * @deprecated This field was added by mistake and has never worked. It will be removed in future versions.
     */
    bouncer: string;
    /**
     * Primary PostgreSQL database name.
     */
    dbname: string;
    /**
     * PostgreSQL primary node host IP or name.
     */
    host: string;
    /**
     * The [number of allowed connections](https://aiven.io/docs/products/postgresql/reference/pg-connection-limits). Varies based on the service plan.
     */
    maxConnections: number;
    /**
     * PostgreSQL connection parameters.
     */
    params: outputs.GetPgPgParam[];
    /**
     * PostgreSQL admin user password.
     */
    password: string;
    /**
     * PostgreSQL port.
     */
    port: number;
    /**
     * PostgreSQL replica URI for services with a replica.
     */
    replicaUri: string;
    /**
     * PostgreSQL SSL mode setting.
     */
    sslmode: string;
    /**
     * PostgreSQL standby connection URIs.
     */
    standbyUris: string[];
    /**
     * PostgreSQL syncing connection URIs.
     */
    syncingUris: string[];
    /**
     * PostgreSQL primary connection URI.
     */
    uri: string;
    /**
     * PostgreSQL primary connection URIs.
     */
    uris: string[];
    /**
     * PostgreSQL admin user name.
     */
    user: string;
}

export interface GetPgPgParam {
    /**
     * Primary PostgreSQL database name.
     */
    databaseName: string;
    /**
     * PostgreSQL host IP or name.
     */
    host: string;
    /**
     * PostgreSQL admin user password.
     */
    password: string;
    /**
     * PostgreSQL port.
     */
    port: number;
    /**
     * PostgreSQL SSL mode setting.
     */
    sslmode: string;
    /**
     * PostgreSQL admin user name.
     */
    user: string;
}

export interface GetPgPgUserConfig {
    /**
     * Additional Cloud Regions for Backup Replication.
     *
     * @deprecated This property is deprecated.
     */
    additionalBackupRegions?: string;
    /**
     * Custom password for admin user. Defaults to random string. This must be set only when a new service is being created.
     */
    adminPassword?: string;
    /**
     * Custom username for admin user. This must be set only when a new service is being created. Example: `avnadmin`.
     */
    adminUsername?: string;
    /**
     * The hour of day (in UTC) when backup for the service is started. New backup is only started if previous backup has already completed. Example: `3`.
     */
    backupHour?: number;
    /**
     * The minute of an hour when backup for the service is started. New backup is only started if previous backup has already completed. Example: `30`.
     */
    backupMinute?: number;
    /**
     * Register AAAA DNS records for the service, and allow IPv6 packets to service ports.
     */
    enableIpv6?: boolean;
    /**
     * Allow incoming connections from CIDR address block, e.g. `10.20.0.0/16`
     */
    ipFilterObjects?: outputs.GetPgPgUserConfigIpFilterObject[];
    /**
     * Allow incoming connections from CIDR address block, e.g. `10.20.0.0/16`.
     */
    ipFilterStrings?: string[];
    /**
     * Allow incoming connections from CIDR address block, e.g. `10.20.0.0/16`.
     *
     * @deprecated Deprecated. Use `ipFilterString` instead.
     */
    ipFilters?: string[];
    /**
     * Migrate data from existing server
     */
    migration?: outputs.GetPgPgUserConfigMigration;
    /**
     * postgresql.conf configuration values
     */
    pg?: outputs.GetPgPgUserConfigPg;
    /**
     * System-wide settings for the pgQualstats extension
     *
     * @deprecated This property is deprecated.
     */
    pgQualstats?: outputs.GetPgPgUserConfigPgQualstats;
    /**
     * Should the service which is being forked be a read replica (deprecated, use readReplica service integration instead).
     */
    pgReadReplica?: boolean;
    /**
     * Name of the PG Service from which to fork (deprecated, use service_to_fork_from). This has effect only when a new service is being created. Example: `anotherservicename`.
     */
    pgServiceToForkFrom?: string;
    /**
     * Enable the pgStatMonitor extension. Enabling this extension will cause the cluster to be restarted.When this extension is enabled, pgStatStatements results for utility commands are unreliable. Default: `false`.
     */
    pgStatMonitorEnable?: boolean;
    /**
     * Enum: `10`, `11`, `12`, `13`, `14`, `15`, `16`, and newer. PostgreSQL major version.
     */
    pgVersion?: string;
    /**
     * System-wide settings for the pgaudit extension
     *
     * @deprecated This property is deprecated.
     */
    pgaudit?: outputs.GetPgPgUserConfigPgaudit;
    /**
     * PGBouncer connection pooling settings
     */
    pgbouncer?: outputs.GetPgPgUserConfigPgbouncer;
    /**
     * System-wide settings for pglookout
     */
    pglookout?: outputs.GetPgPgUserConfigPglookout;
    /**
     * Allow access to selected service ports from private networks
     */
    privateAccess?: outputs.GetPgPgUserConfigPrivateAccess;
    /**
     * Allow access to selected service components through Privatelink
     */
    privatelinkAccess?: outputs.GetPgPgUserConfigPrivatelinkAccess;
    /**
     * Name of another project to fork a service from. This has effect only when a new service is being created. Example: `anotherprojectname`.
     */
    projectToForkFrom?: string;
    /**
     * Allow access to selected service ports from the public Internet
     */
    publicAccess?: outputs.GetPgPgUserConfigPublicAccess;
    /**
     * Recovery target time when forking a service. This has effect only when a new service is being created. Example: `2019-01-01 23:34:45`.
     */
    recoveryTargetTime?: string;
    /**
     * Store logs for the service so that they are available in the HTTP API and console.
     */
    serviceLog?: boolean;
    /**
     * Name of another service to fork from. This has effect only when a new service is being created. Example: `anotherservicename`.
     */
    serviceToForkFrom?: string;
    /**
     * Percentage of total RAM that the database server uses for shared memory buffers. Valid range is 20-60 (float), which corresponds to 20% - 60%. This setting adjusts the sharedBuffers configuration value. Example: `41.5`.
     */
    sharedBuffersPercentage?: number;
    /**
     * Use static public IP addresses.
     */
    staticIps?: boolean;
    /**
     * Enum: `quorum`, `off`. Synchronous replication type. Note that the service plan also needs to support synchronous replication.
     */
    synchronousReplication?: string;
    /**
     * System-wide settings for the timescaledb extension
     */
    timescaledb?: outputs.GetPgPgUserConfigTimescaledb;
    /**
     * Enum: `aiven`, `timescale`. Variant of the PostgreSQL service, may affect the features that are exposed by default.
     */
    variant?: string;
    /**
     * Sets the maximum amount of memory to be used by a query operation (such as a sort or hash table) before writing to temporary disk files, in MB. Default is 1MB + 0.075% of total RAM (up to 32MB). Example: `4`.
     */
    workMem?: number;
}

export interface GetPgPgUserConfigIpFilterObject {
    /**
     * Description for IP filter list entry. Example: `Production service IP range`.
     */
    description?: string;
    /**
     * CIDR address block. Example: `10.20.0.0/16`.
     */
    network: string;
}

export interface GetPgPgUserConfigMigration {
    /**
     * Database name for bootstrapping the initial connection. Example: `defaultdb`.
     */
    dbname?: string;
    /**
     * Hostname or IP address of the server where to migrate data from. Example: `my.server.com`.
     */
    host: string;
    /**
     * Comma-separated list of databases, which should be ignored during migration (supported by MySQL and PostgreSQL only at the moment). Example: `db1,db2`.
     */
    ignoreDbs?: string;
    /**
     * Comma-separated list of database roles, which should be ignored during migration (supported by PostgreSQL only at the moment). Example: `role1,role2`.
     */
    ignoreRoles?: string;
    /**
     * Enum: `dump`, `replication`. The migration method to be used (currently supported only by Redis, Dragonfly, MySQL and PostgreSQL service types).
     */
    method?: string;
    /**
     * Password for authentication with the server where to migrate data from. Example: `jjKk45Nnd`.
     */
    password?: string;
    /**
     * Port number of the server where to migrate data from. Example: `1234`.
     */
    port: number;
    /**
     * The server where to migrate data from is secured with SSL. Default: `true`.
     */
    ssl?: boolean;
    /**
     * User name for authentication with the server where to migrate data from. Example: `myname`.
     */
    username?: string;
}

export interface GetPgPgUserConfigPg {
    /**
     * Specifies a fraction of the table size to add to autovacuumAnalyzeThreshold when deciding whether to trigger an ANALYZE. The default is 0.2 (20% of table size).
     */
    autovacuumAnalyzeScaleFactor?: number;
    /**
     * Specifies the minimum number of inserted, updated or deleted tuples needed to trigger an ANALYZE in any one table. The default is 50 tuples.
     */
    autovacuumAnalyzeThreshold?: number;
    /**
     * Specifies the maximum age (in transactions) that a table's pg_class.relfrozenxid field can attain before a VACUUM operation is forced to prevent transaction ID wraparound within the table. Note that the system will launch autovacuum processes to prevent wraparound even when autovacuum is otherwise disabled. This parameter will cause the server to be restarted. Example: `200000000`.
     */
    autovacuumFreezeMaxAge?: number;
    /**
     * Specifies the maximum number of autovacuum processes (other than the autovacuum launcher) that may be running at any one time. The default is three. This parameter can only be set at server start.
     */
    autovacuumMaxWorkers?: number;
    /**
     * Specifies the minimum delay between autovacuum runs on any given database. The delay is measured in seconds, and the default is one minute.
     */
    autovacuumNaptime?: number;
    /**
     * Specifies the cost delay value that will be used in automatic VACUUM operations. If -1 is specified, the regular vacuumCostDelay value will be used. The default value is 20 milliseconds.
     */
    autovacuumVacuumCostDelay?: number;
    /**
     * Specifies the cost limit value that will be used in automatic VACUUM operations. If -1 is specified (which is the default), the regular vacuumCostLimit value will be used.
     */
    autovacuumVacuumCostLimit?: number;
    /**
     * Specifies a fraction of the table size to add to autovacuumVacuumThreshold when deciding whether to trigger a VACUUM. The default is 0.2 (20% of table size).
     */
    autovacuumVacuumScaleFactor?: number;
    /**
     * Specifies the minimum number of updated or deleted tuples needed to trigger a VACUUM in any one table. The default is 50 tuples.
     */
    autovacuumVacuumThreshold?: number;
    /**
     * Specifies the delay between activity rounds for the background writer in milliseconds. Default is 200. Example: `200`.
     */
    bgwriterDelay?: number;
    /**
     * Whenever more than bgwriterFlushAfter bytes have been written by the background writer, attempt to force the OS to issue these writes to the underlying storage. Specified in kilobytes, default is 512. Setting of 0 disables forced writeback. Example: `512`.
     */
    bgwriterFlushAfter?: number;
    /**
     * In each round, no more than this many buffers will be written by the background writer. Setting this to zero disables background writing. Default is 100. Example: `100`.
     */
    bgwriterLruMaxpages?: number;
    /**
     * The average recent need for new buffers is multiplied by bgwriterLruMultiplier to arrive at an estimate of the number that will be needed during the next round, (up to bgwriter_lru_maxpages). 1.0 represents a “just in time” policy of writing exactly the number of buffers predicted to be needed. Larger values provide some cushion against spikes in demand, while smaller values intentionally leave writes to be done by server processes. The default is 2.0. Example: `2.0`.
     */
    bgwriterLruMultiplier?: number;
    /**
     * This is the amount of time, in milliseconds, to wait on a lock before checking to see if there is a deadlock condition. Example: `1000`.
     */
    deadlockTimeout?: number;
    /**
     * Enum: `lz4`, `pglz`. Specifies the default TOAST compression method for values of compressible columns (the default is lz4).
     */
    defaultToastCompression?: string;
    /**
     * Time out sessions with open transactions after this number of milliseconds.
     */
    idleInTransactionSessionTimeout?: number;
    /**
     * Controls system-wide use of Just-in-Time Compilation (JIT).
     */
    jit?: boolean;
    /**
     * Causes each action executed by autovacuum to be logged if it ran for at least the specified number of milliseconds. Setting this to zero logs all autovacuum actions. Minus-one (the default) disables logging autovacuum actions.
     */
    logAutovacuumMinDuration?: number;
    /**
     * Enum: `TERSE`, `DEFAULT`, `VERBOSE`. Controls the amount of detail written in the server log for each message that is logged.
     */
    logErrorVerbosity?: string;
    /**
     * Enum: `'pid=%p,user=%u,db=%d,app=%a,client=%h '`, `'%t [%p]: [%l-1] user=%u,db=%d,app=%a,client=%h '`, `'%m [%p] %q[user=%u,db=%d,app=%a] '`, `'pid=%p,user=%u,db=%d,app=%a,client=%h,txid=%x,qid=%Q '`. Choose from one of the available log formats.
     */
    logLinePrefix?: string;
    /**
     * Log statements that take more than this number of milliseconds to run, -1 disables.
     */
    logMinDurationStatement?: number;
    /**
     * Log statements for each temporary file created larger than this number of kilobytes, -1 disables.
     */
    logTempFiles?: number;
    /**
     * PostgreSQL maximum number of files that can be open per process.
     */
    maxFilesPerProcess?: number;
    /**
     * PostgreSQL maximum locks per transaction.
     */
    maxLocksPerTransaction?: number;
    /**
     * PostgreSQL maximum logical replication workers (taken from the pool of max_parallel_workers).
     */
    maxLogicalReplicationWorkers?: number;
    /**
     * Sets the maximum number of workers that the system can support for parallel queries.
     */
    maxParallelWorkers?: number;
    /**
     * Sets the maximum number of workers that can be started by a single Gather or Gather Merge node.
     */
    maxParallelWorkersPerGather?: number;
    /**
     * PostgreSQL maximum predicate locks per transaction.
     */
    maxPredLocksPerTransaction?: number;
    /**
     * PostgreSQL maximum prepared transactions.
     */
    maxPreparedTransactions?: number;
    /**
     * PostgreSQL maximum replication slots.
     */
    maxReplicationSlots?: number;
    /**
     * PostgreSQL maximum WAL size (MB) reserved for replication slots. Default is -1 (unlimited). walKeepSize minimum WAL size setting takes precedence over this.
     */
    maxSlotWalKeepSize?: number;
    /**
     * Maximum depth of the stack in bytes.
     */
    maxStackDepth?: number;
    /**
     * Max standby archive delay in milliseconds.
     */
    maxStandbyArchiveDelay?: number;
    /**
     * Max standby streaming delay in milliseconds.
     */
    maxStandbyStreamingDelay?: number;
    /**
     * PostgreSQL maximum WAL senders.
     */
    maxWalSenders?: number;
    /**
     * Sets the maximum number of background processes that the system can support.
     */
    maxWorkerProcesses?: number;
    /**
     * Sets the time interval to run pg_partman's scheduled tasks. Example: `3600`.
     */
    pgPartmanBgwDotInterval?: number;
    /**
     * Controls which role to use for pg_partman's scheduled background tasks. Example: `myrolename`.
     */
    pgPartmanBgwDotRole?: string;
    /**
     * Enables or disables query plan monitoring.
     */
    pgStatMonitorDotPgsmEnableQueryPlan?: boolean;
    /**
     * Sets the maximum number of buckets. Example: `10`.
     */
    pgStatMonitorDotPgsmMaxBuckets?: number;
    /**
     * Enum: `all`, `top`, `none`. Controls which statements are counted. Specify top to track top-level statements (those issued directly by clients), all to also track nested statements (such as statements invoked within functions), or none to disable statement statistics collection. The default value is top.
     */
    pgStatStatementsDotTrack?: string;
    /**
     * PostgreSQL temporary file limit in KiB, -1 for unlimited. Example: `5000000`.
     */
    tempFileLimit?: number;
    /**
     * PostgreSQL service timezone. Example: `Europe/Helsinki`.
     */
    timezone?: string;
    /**
     * Specifies the number of bytes reserved to track the currently executing command for each active session. Example: `1024`.
     */
    trackActivityQuerySize?: number;
    /**
     * Enum: `off`, `on`. Record commit time of transactions.
     */
    trackCommitTimestamp?: string;
    /**
     * Enum: `all`, `pl`, `none`. Enables tracking of function call counts and time used.
     */
    trackFunctions?: string;
    /**
     * Enum: `off`, `on`. Enables timing of database I/O calls. This parameter is off by default, because it will repeatedly query the operating system for the current time, which may cause significant overhead on some platforms.
     */
    trackIoTiming?: string;
    /**
     * Terminate replication connections that are inactive for longer than this amount of time, in milliseconds. Setting this value to zero disables the timeout. Example: `60000`.
     */
    walSenderTimeout?: number;
    /**
     * WAL flush interval in milliseconds. Note that setting this value to lower than the default 200ms may negatively impact performance. Example: `50`.
     */
    walWriterDelay?: number;
}

export interface GetPgPgUserConfigPgQualstats {
    /**
     * Enable / Disable pg_qualstats. Default: `false`.
     *
     * @deprecated This property is deprecated.
     */
    enabled?: boolean;
    /**
     * Error estimation num threshold to save quals. Default: `0`.
     *
     * @deprecated This property is deprecated.
     */
    minErrEstimateNum?: number;
    /**
     * Error estimation ratio threshold to save quals. Default: `0`.
     *
     * @deprecated This property is deprecated.
     */
    minErrEstimateRatio?: number;
    /**
     * Enable / Disable pgQualstats constants tracking. Default: `true`.
     *
     * @deprecated This property is deprecated.
     */
    trackConstants?: boolean;
    /**
     * Track quals on system catalogs too. Default: `false`.
     *
     * @deprecated This property is deprecated.
     */
    trackPgCatalog?: boolean;
}

export interface GetPgPgUserConfigPgaudit {
    /**
     * Enable pgaudit extension. When enabled, pgaudit extension will be automatically installed.Otherwise, extension will be uninstalled but auditing configurations will be preserved. Default: `false`.
     *
     * @deprecated This property is deprecated.
     */
    featureEnabled?: boolean;
    /**
     * Specifies that session logging should be enabled in the casewhere all relations in a statement are in pg_catalog. Default: `true`.
     *
     * @deprecated This property is deprecated.
     */
    logCatalog?: boolean;
    /**
     * Specifies whether log messages will be visible to a client process such as psql. Default: `false`.
     *
     * @deprecated This property is deprecated.
     */
    logClient?: boolean;
    /**
     * Enum: `debug1`, `debug2`, `debug3`, `debug4`, `debug5`, `info`, `notice`, `warning`, `log`. Specifies the log level that will be used for log entries. Default: `log`.
     *
     * @deprecated This property is deprecated.
     */
    logLevel?: string;
    /**
     * Crop parameters representation and whole statements if they exceed this threshold. A (default) value of -1 disable the truncation. Default: `-1`.
     *
     * @deprecated This property is deprecated.
     */
    logMaxStringLength?: number;
    /**
     * This GUC allows to turn off logging nested statements, that is, statements that are executed as part of another ExecutorRun. Default: `true`.
     *
     * @deprecated This property is deprecated.
     */
    logNestedStatements?: boolean;
    /**
     * Specifies that audit logging should include the parameters that were passed with the statement. Default: `false`.
     *
     * @deprecated This property is deprecated.
     */
    logParameter?: boolean;
    /**
     * Specifies that parameter values longer than this setting (in bytes) should not be logged, but replaced with <long param suppressed>. Default: `0`.
     *
     * @deprecated This property is deprecated.
     */
    logParameterMaxSize?: number;
    /**
     * Specifies whether session audit logging should create a separate log entry for each relation (TABLE, VIEW, etc.) referenced in a SELECT or DML statement. Default: `false`.
     *
     * @deprecated This property is deprecated.
     */
    logRelation?: boolean;
    /**
     * Specifies that audit logging should include the rows retrieved or affected by a statement. When enabled the rows field will be included after the parameter field. Default: `false`.
     *
     * @deprecated This property is deprecated.
     */
    logRows?: boolean;
    /**
     * Specifies whether logging will include the statement text and parameters (if enabled). Default: `true`.
     *
     * @deprecated This property is deprecated.
     */
    logStatement?: boolean;
    /**
     * Specifies whether logging will include the statement text and parameters with the first log entry for a statement/substatement combination or with every entry. Default: `false`.
     *
     * @deprecated This property is deprecated.
     */
    logStatementOnce?: boolean;
    /**
     * Specifies which classes of statements will be logged by session audit logging.
     *
     * @deprecated This property is deprecated.
     */
    logs?: string[];
    /**
     * Specifies the master role to use for object audit logging.
     *
     * @deprecated This property is deprecated.
     */
    role?: string;
}

export interface GetPgPgUserConfigPgbouncer {
    /**
     * If the automatically created database pools have been unused this many seconds, they are freed. If 0 then timeout is disabled. (seconds). Default: `3600`.
     */
    autodbIdleTimeout?: number;
    /**
     * Do not allow more than this many server connections per database (regardless of user). Setting it to 0 means unlimited. Example: `0`.
     */
    autodbMaxDbConnections?: number;
    /**
     * Enum: `session`, `transaction`, `statement`. PGBouncer pool mode. Default: `transaction`.
     */
    autodbPoolMode?: string;
    /**
     * If non-zero then create automatically a pool of that size per user when a pool doesn't exist. Default: `0`.
     */
    autodbPoolSize?: number;
    /**
     * List of parameters to ignore when given in startup packet.
     */
    ignoreStartupParameters?: string[];
    /**
     * PgBouncer tracks protocol-level named prepared statements related commands sent by the client in transaction and statement pooling modes when maxPreparedStatements is set to a non-zero value. Setting it to 0 disables prepared statements. maxPreparedStatements defaults to 100, and its maximum is 3000. Default: `100`.
     */
    maxPreparedStatements?: number;
    /**
     * Add more server connections to pool if below this number. Improves behavior when usual load comes suddenly back after period of total inactivity. The value is effectively capped at the pool size. Default: `0`.
     */
    minPoolSize?: number;
    /**
     * If a server connection has been idle more than this many seconds it will be dropped. If 0 then timeout is disabled. (seconds). Default: `600`.
     */
    serverIdleTimeout?: number;
    /**
     * The pooler will close an unused server connection that has been connected longer than this. (seconds). Default: `3600`.
     */
    serverLifetime?: number;
    /**
     * Run serverResetQuery (DISCARD ALL) in all pooling modes. Default: `false`.
     */
    serverResetQueryAlways?: boolean;
}

export interface GetPgPgUserConfigPglookout {
    /**
     * Number of seconds of master unavailability before triggering database failover to standby. Default: `60`.
     */
    maxFailoverReplicationTimeLag?: number;
}

export interface GetPgPgUserConfigPrivateAccess {
    /**
     * Allow clients to connect to pg with a DNS name that always resolves to the service's private IP addresses. Only available in certain network locations.
     */
    pg?: boolean;
    /**
     * Allow clients to connect to pgbouncer with a DNS name that always resolves to the service's private IP addresses. Only available in certain network locations.
     */
    pgbouncer?: boolean;
    /**
     * Allow clients to connect to prometheus with a DNS name that always resolves to the service's private IP addresses. Only available in certain network locations.
     */
    prometheus?: boolean;
}

export interface GetPgPgUserConfigPrivatelinkAccess {
    /**
     * Enable pg.
     */
    pg?: boolean;
    /**
     * Enable pgbouncer.
     */
    pgbouncer?: boolean;
    /**
     * Enable prometheus.
     */
    prometheus?: boolean;
}

export interface GetPgPgUserConfigPublicAccess {
    /**
     * Allow clients to connect to pg from the public internet for service nodes that are in a project VPC or another type of private network.
     */
    pg?: boolean;
    /**
     * Allow clients to connect to pgbouncer from the public internet for service nodes that are in a project VPC or another type of private network.
     */
    pgbouncer?: boolean;
    /**
     * Allow clients to connect to prometheus from the public internet for service nodes that are in a project VPC or another type of private network.
     */
    prometheus?: boolean;
}

export interface GetPgPgUserConfigTimescaledb {
    /**
     * The number of background workers for timescaledb operations. You should configure this setting to the sum of your number of databases and the total number of concurrent background workers you want running at any given point in time. Default: `16`.
     */
    maxBackgroundWorkers?: number;
}

export interface GetPgServiceIntegration {
    /**
     * Type of the service integration. The only supported value at the moment is `readReplica`
     */
    integrationType: string;
    /**
     * Name of the source service
     */
    sourceServiceName: string;
}

export interface GetPgTag {
    /**
     * Service tag key
     */
    key: string;
    /**
     * Service tag value
     */
    value: string;
}

export interface GetPgTechEmail {
    /**
     * An email address to contact for technical issues
     */
    email: string;
}

export interface GetProjectTag {
    /**
     * Project tag key.
     */
    key: string;
    /**
     * Project tag value.
     */
    value: string;
}

export interface GetRedisComponent {
    /**
     * Service component name
     */
    component: string;
    /**
     * Connection info for connecting to the service component. This is a combination of host and port.
     */
    connectionUri: string;
    /**
     * Host name for connecting to the service component
     */
    host: string;
    /**
     * Kafka authentication method. This is a value specific to the 'kafka' service component
     */
    kafkaAuthenticationMethod: string;
    /**
     * Port number for connecting to the service component
     */
    port: number;
    /**
     * Network access route
     */
    route: string;
    /**
     * Whether the endpoint is encrypted or accepts plaintext. By default endpoints are always encrypted and this property is only included for service components they may disable encryption
     */
    ssl: boolean;
    /**
     * DNS usage name
     */
    usage: string;
}

export interface GetRedisRedi {
    /**
     * Redis password.
     */
    password: string;
    /**
     * Redis replica server URI.
     */
    replicaUri: string;
    /**
     * Redis slave server URIs.
     */
    slaveUris: string[];
    /**
     * Redis server URIs.
     */
    uris: string[];
}

export interface GetRedisRedisUserConfig {
    /**
     * Additional Cloud Regions for Backup Replication.
     */
    additionalBackupRegions?: string;
    /**
     * The hour of day (in UTC) when backup for the service is started. New backup is only started if previous backup has already completed. Example: `3`.
     */
    backupHour?: number;
    /**
     * The minute of an hour when backup for the service is started. New backup is only started if previous backup has already completed. Example: `30`.
     */
    backupMinute?: number;
    /**
     * Allow incoming connections from CIDR address block, e.g. `10.20.0.0/16`
     */
    ipFilterObjects?: outputs.GetRedisRedisUserConfigIpFilterObject[];
    /**
     * Allow incoming connections from CIDR address block, e.g. `10.20.0.0/16`.
     */
    ipFilterStrings?: string[];
    /**
     * Allow incoming connections from CIDR address block, e.g. `10.20.0.0/16`.
     *
     * @deprecated Deprecated. Use `ipFilterString` instead.
     */
    ipFilters?: string[];
    /**
     * Migrate data from existing server
     */
    migration?: outputs.GetRedisRedisUserConfigMigration;
    /**
     * Allow access to selected service ports from private networks
     */
    privateAccess?: outputs.GetRedisRedisUserConfigPrivateAccess;
    /**
     * Allow access to selected service components through Privatelink
     */
    privatelinkAccess?: outputs.GetRedisRedisUserConfigPrivatelinkAccess;
    /**
     * Name of another project to fork a service from. This has effect only when a new service is being created. Example: `anotherprojectname`.
     */
    projectToForkFrom?: string;
    /**
     * Allow access to selected service ports from the public Internet
     */
    publicAccess?: outputs.GetRedisRedisUserConfigPublicAccess;
    /**
     * Name of the basebackup to restore in forked service. Example: `backup-20191112t091354293891z`.
     */
    recoveryBasebackupName?: string;
    /**
     * Enum: `allchannels`, `resetchannels`. Determines default pub/sub channels' ACL for new users if ACL is not supplied. When this option is not defined, allChannels is assumed to keep backward compatibility. This option doesn't affect Redis configuration acl-pubsub-default.
     */
    redisAclChannelsDefault?: string;
    /**
     * Set Redis IO thread count. Changing this will cause a restart of the Redis service. Example: `1`.
     */
    redisIoThreads?: number;
    /**
     * LFU maxmemory-policy counter decay time in minutes. Default: `1`.
     */
    redisLfuDecayTime?: number;
    /**
     * Counter logarithm factor for volatile-lfu and allkeys-lfu maxmemory-policies. Default: `10`.
     */
    redisLfuLogFactor?: number;
    /**
     * Enum: `noeviction`, `allkeys-lru`, `volatile-lru`, `allkeys-random`, `volatile-random`, `volatile-ttl`, `volatile-lfu`, `allkeys-lfu`. Redis maxmemory-policy. Default: `noeviction`.
     */
    redisMaxmemoryPolicy?: string;
    /**
     * Set notify-keyspace-events option.
     */
    redisNotifyKeyspaceEvents?: string;
    /**
     * Set number of Redis databases. Changing this will cause a restart of the Redis service. Example: `16`.
     */
    redisNumberOfDatabases?: number;
    /**
     * Enum: `off`, `rdb`. When persistence is `rdb`, Redis does RDB dumps each 10 minutes if any key is changed. Also RDB dumps are done according to the backup schedule for backup purposes. When persistence is `off`, no RDB dumps or backups are done, so data can be lost at any moment if the service is restarted for any reason, or if the service is powered off. Also, the service can't be forked.
     */
    redisPersistence?: string;
    /**
     * Set output buffer limit for pub / sub clients in MB. The value is the hard limit, the soft limit is 1/4 of the hard limit. When setting the limit, be mindful of the available memory in the selected service plan. Example: `64`.
     */
    redisPubsubClientOutputBufferLimit?: number;
    /**
     * Require SSL to access Redis. Default: `true`.
     */
    redisSsl?: boolean;
    /**
     * Redis idle connection timeout in seconds. Default: `300`.
     */
    redisTimeout?: number;
    /**
     * Enum: `7.0`, and newer. Redis major version.
     */
    redisVersion?: string;
    /**
     * Store logs for the service so that they are available in the HTTP API and console.
     */
    serviceLog?: boolean;
    /**
     * Name of another service to fork from. This has effect only when a new service is being created. Example: `anotherservicename`.
     */
    serviceToForkFrom?: string;
    /**
     * Use static public IP addresses.
     */
    staticIps?: boolean;
}

export interface GetRedisRedisUserConfigIpFilterObject {
    /**
     * Description for IP filter list entry. Example: `Production service IP range`.
     */
    description?: string;
    /**
     * CIDR address block. Example: `10.20.0.0/16`.
     */
    network: string;
}

export interface GetRedisRedisUserConfigMigration {
    /**
     * Database name for bootstrapping the initial connection. Example: `defaultdb`.
     */
    dbname?: string;
    /**
     * Hostname or IP address of the server where to migrate data from. Example: `my.server.com`.
     */
    host: string;
    /**
     * Comma-separated list of databases, which should be ignored during migration (supported by MySQL and PostgreSQL only at the moment). Example: `db1,db2`.
     */
    ignoreDbs?: string;
    /**
     * Comma-separated list of database roles, which should be ignored during migration (supported by PostgreSQL only at the moment). Example: `role1,role2`.
     */
    ignoreRoles?: string;
    /**
     * Enum: `dump`, `replication`. The migration method to be used (currently supported only by Redis, Dragonfly, MySQL and PostgreSQL service types).
     */
    method?: string;
    /**
     * Password for authentication with the server where to migrate data from. Example: `jjKk45Nnd`.
     */
    password?: string;
    /**
     * Port number of the server where to migrate data from. Example: `1234`.
     */
    port: number;
    /**
     * The server where to migrate data from is secured with SSL. Default: `true`.
     */
    ssl?: boolean;
    /**
     * User name for authentication with the server where to migrate data from. Example: `myname`.
     */
    username?: string;
}

export interface GetRedisRedisUserConfigPrivateAccess {
    /**
     * Allow clients to connect to prometheus with a DNS name that always resolves to the service's private IP addresses. Only available in certain network locations.
     */
    prometheus?: boolean;
    /**
     * Allow clients to connect to redis with a DNS name that always resolves to the service's private IP addresses. Only available in certain network locations.
     */
    redis?: boolean;
}

export interface GetRedisRedisUserConfigPrivatelinkAccess {
    /**
     * Enable prometheus.
     */
    prometheus?: boolean;
    /**
     * Enable redis.
     */
    redis?: boolean;
}

export interface GetRedisRedisUserConfigPublicAccess {
    /**
     * Allow clients to connect to prometheus from the public internet for service nodes that are in a project VPC or another type of private network.
     */
    prometheus?: boolean;
    /**
     * Allow clients to connect to redis from the public internet for service nodes that are in a project VPC or another type of private network.
     */
    redis?: boolean;
}

export interface GetRedisServiceIntegration {
    /**
     * Type of the service integration. The only supported value at the moment is `readReplica`
     */
    integrationType: string;
    /**
     * Name of the source service
     */
    sourceServiceName: string;
}

export interface GetRedisTag {
    /**
     * Service tag key
     */
    key: string;
    /**
     * Service tag value
     */
    value: string;
}

export interface GetRedisTechEmail {
    /**
     * An email address to contact for technical issues
     */
    email: string;
}

export interface GetServiceIntegrationClickhouseKafkaUserConfig {
    /**
     * Tables to create
     */
    tables?: outputs.GetServiceIntegrationClickhouseKafkaUserConfigTable[];
}

export interface GetServiceIntegrationClickhouseKafkaUserConfigTable {
    /**
     * Enum: `smallest`, `earliest`, `beginning`, `largest`, `latest`, `end`. Action to take when there is no initial offset in offset store or the desired offset is out of range. Default: `earliest`.
     */
    autoOffsetReset?: string;
    /**
     * Table columns
     */
    columns: outputs.GetServiceIntegrationClickhouseKafkaUserConfigTableColumn[];
    /**
     * Enum: `Avro`, `CSV`, `JSONAsString`, `JSONCompactEachRow`, `JSONCompactStringsEachRow`, `JSONEachRow`, `JSONStringsEachRow`, `MsgPack`, `TSKV`, `TSV`, `TabSeparated`, `RawBLOB`, `AvroConfluent`, `Parquet`. Message data format. Default: `JSONEachRow`.
     */
    dataFormat: string;
    /**
     * Enum: `basic`, `bestEffort`, `bestEffortUs`. Method to read DateTime from text input formats. Default: `basic`.
     */
    dateTimeInputFormat?: string;
    /**
     * Kafka consumers group. Default: `clickhouse`.
     */
    groupName: string;
    /**
     * Enum: `default`, `stream`. How to handle errors for Kafka engine. Default: `default`.
     */
    handleErrorMode?: string;
    /**
     * Number of row collected by poll(s) for flushing data from Kafka. Default: `0`.
     */
    maxBlockSize?: number;
    /**
     * The maximum number of rows produced in one kafka message for row-based formats. Default: `1`.
     */
    maxRowsPerMessage?: number;
    /**
     * Name of the table. Example: `events`.
     */
    name: string;
    /**
     * The number of consumers per table per replica. Default: `1`.
     */
    numConsumers?: number;
    /**
     * Maximum amount of messages to be polled in a single Kafka poll. Default: `0`.
     */
    pollMaxBatchSize?: number;
    /**
     * Timeout in milliseconds for a single poll from Kafka. Takes the value of the streamFlushIntervalMs server setting by default (500ms). Default: `0`.
     */
    pollMaxTimeoutMs?: number;
    /**
     * Skip at least this number of broken messages from Kafka topic per block. Default: `0`.
     */
    skipBrokenMessages?: number;
    /**
     * Provide an independent thread for each consumer. All consumers run in the same thread by default. Default: `false`.
     */
    threadPerConsumer?: boolean;
    /**
     * Kafka topics
     */
    topics: outputs.GetServiceIntegrationClickhouseKafkaUserConfigTableTopic[];
}

export interface GetServiceIntegrationClickhouseKafkaUserConfigTableColumn {
    /**
     * Column name. Example: `key`.
     */
    name: string;
    /**
     * Column type. Example: `UInt64`.
     */
    type: string;
}

export interface GetServiceIntegrationClickhouseKafkaUserConfigTableTopic {
    /**
     * Name of the topic. Example: `topicName`.
     */
    name: string;
}

export interface GetServiceIntegrationClickhousePostgresqlUserConfig {
    /**
     * Databases to expose
     */
    databases?: outputs.GetServiceIntegrationClickhousePostgresqlUserConfigDatabase[];
}

export interface GetServiceIntegrationClickhousePostgresqlUserConfigDatabase {
    /**
     * PostgreSQL database to expose. Default: `defaultdb`.
     */
    database?: string;
    /**
     * PostgreSQL schema to expose. Default: `public`.
     */
    schema?: string;
}

export interface GetServiceIntegrationDatadogUserConfig {
    /**
     * Enable Datadog Database Monitoring.
     */
    datadogDbmEnabled?: boolean;
    /**
     * Enable Datadog PgBouncer Metric Tracking.
     */
    datadogPgbouncerEnabled?: boolean;
    /**
     * Custom tags provided by user
     */
    datadogTags?: outputs.GetServiceIntegrationDatadogUserConfigDatadogTag[];
    /**
     * List of custom metrics.
     */
    excludeConsumerGroups?: string[];
    /**
     * List of topics to exclude.
     */
    excludeTopics?: string[];
    /**
     * List of custom metrics.
     */
    includeConsumerGroups?: string[];
    /**
     * List of topics to include.
     */
    includeTopics?: string[];
    /**
     * List of custom metrics.
     */
    kafkaCustomMetrics?: string[];
    /**
     * Maximum number of JMX metrics to send. Example: `2000`.
     */
    maxJmxMetrics?: number;
    /**
     * List of custom metrics.
     */
    mirrormakerCustomMetrics?: string[];
    /**
     * Datadog Opensearch Options
     */
    opensearch?: outputs.GetServiceIntegrationDatadogUserConfigOpensearch;
    /**
     * Datadog Redis Options
     */
    redis?: outputs.GetServiceIntegrationDatadogUserConfigRedis;
}

export interface GetServiceIntegrationDatadogUserConfigDatadogTag {
    /**
     * Optional tag explanation. Example: `Used to tag primary replica metrics`.
     */
    comment?: string;
    /**
     * Tag format and usage are described here: https://docs.datadoghq.com/getting_started/tagging. Tags with prefix `aiven-` are reserved for Aiven. Example: `replica:primary`.
     */
    tag: string;
}

export interface GetServiceIntegrationDatadogUserConfigOpensearch {
    /**
     * Enable Datadog Opensearch Cluster Monitoring.
     */
    clusterStatsEnabled?: boolean;
    /**
     * Enable Datadog Opensearch Index Monitoring.
     */
    indexStatsEnabled?: boolean;
    /**
     * Enable Datadog Opensearch Pending Task Monitoring.
     */
    pendingTaskStatsEnabled?: boolean;
    /**
     * Enable Datadog Opensearch Primary Shard Monitoring.
     */
    pshardStatsEnabled?: boolean;
}

export interface GetServiceIntegrationDatadogUserConfigRedis {
    /**
     * Enable commandStats option in the agent's configuration. Default: `false`.
     */
    commandStatsEnabled?: boolean;
}

export interface GetServiceIntegrationEndpointDatadogUserConfig {
    /**
     * Datadog API key. Example: `848f30907c15c55d601fe45487cce9b6`.
     */
    datadogApiKey: string;
    /**
     * Custom tags provided by user
     */
    datadogTags?: outputs.GetServiceIntegrationEndpointDatadogUserConfigDatadogTag[];
    /**
     * Disable consumer group metrics.
     */
    disableConsumerStats?: boolean;
    /**
     * Number of separate instances to fetch kafka consumer statistics with. Example: `8`.
     */
    kafkaConsumerCheckInstances?: number;
    /**
     * Number of seconds that datadog will wait to get consumer statistics from brokers. Example: `60`.
     */
    kafkaConsumerStatsTimeout?: number;
    /**
     * Maximum number of partition contexts to send. Example: `32000`.
     */
    maxPartitionContexts?: number;
    /**
     * Enum: `datadoghq.com`, `datadoghq.eu`, `us3.datadoghq.com`, `us5.datadoghq.com`, `ddog-gov.com`, `ap1.datadoghq.com`. Datadog intake site. Defaults to datadoghq.com.
     */
    site?: string;
}

export interface GetServiceIntegrationEndpointDatadogUserConfigDatadogTag {
    /**
     * Optional tag explanation. Example: `Used to tag primary replica metrics`.
     */
    comment?: string;
    /**
     * Tag format and usage are described here: https://docs.datadoghq.com/getting_started/tagging. Tags with prefix `aiven-` are reserved for Aiven. Example: `replica:primary`.
     */
    tag: string;
}

export interface GetServiceIntegrationEndpointExternalAwsCloudwatchLogsUserConfig {
    /**
     * AWS access key. Required permissions are logs:CreateLogGroup, logs:CreateLogStream, logs:PutLogEvents and logs:DescribeLogStreams. Example: `AAAAAAAAAAAAAAAAAAAA`.
     */
    accessKey: string;
    /**
     * AWS CloudWatch log group name. Example: `my-log-group`.
     */
    logGroupName?: string;
    /**
     * AWS region. Example: `us-east-1`.
     */
    region: string;
    /**
     * AWS secret key. Example: `AAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAA`.
     */
    secretKey: string;
}

export interface GetServiceIntegrationEndpointExternalAwsCloudwatchMetricsUserConfig {
    /**
     * AWS access key. Required permissions are cloudwatch:PutMetricData. Example: `AAAAAAAAAAAAAAAAAAAA`.
     */
    accessKey: string;
    /**
     * AWS CloudWatch Metrics Namespace. Example: `my-metrics-namespace`.
     */
    namespace: string;
    /**
     * AWS region. Example: `us-east-1`.
     */
    region: string;
    /**
     * AWS secret key. Example: `AAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAA`.
     */
    secretKey: string;
}

export interface GetServiceIntegrationEndpointExternalAwsS3UserConfig {
    /**
     * Access Key Id. Example: `AAAAAAAAAAAAAAAAAAA`.
     */
    accessKeyId: string;
    /**
     * Secret Access Key. Example: `AAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAA`.
     */
    secretAccessKey: string;
    /**
     * S3-compatible bucket URL. Example: `https://mybucket.s3-myregion.amazonaws.com/mydataset/`.
     */
    url: string;
}

export interface GetServiceIntegrationEndpointExternalClickhouseUserConfig {
    /**
     * Hostname or IP address of the server. Example: `my.server.com`.
     */
    host: string;
    /**
     * Password. Example: `jjKk45Nnd`.
     */
    password: string;
    /**
     * Secure TCP server port. Example: `9440`.
     */
    port: number;
    /**
     * User name. Example: `default`.
     */
    username: string;
}

export interface GetServiceIntegrationEndpointExternalElasticsearchLogsUserConfig {
    /**
     * PEM encoded CA certificate. Example: `-----BEGIN CERTIFICATE-----
     * ...
     * -----END CERTIFICATE-----
     * `.
     */
    ca?: string;
    /**
     * Maximum number of days of logs to keep. Default: `3`.
     */
    indexDaysMax?: number;
    /**
     * Elasticsearch index prefix. Default: `logs`.
     */
    indexPrefix: string;
    /**
     * Elasticsearch request timeout limit. Default: `10.0`.
     */
    timeout?: number;
    /**
     * Elasticsearch connection URL. Example: `https://user:passwd@logs.example.com/`.
     */
    url: string;
}

export interface GetServiceIntegrationEndpointExternalGoogleCloudBigquery {
    /**
     * GCP project id. Example: `snappy-photon-12345`.
     */
    projectId: string;
    /**
     * This is a JSON object with the fields documented in https://cloud.google.com/iam/docs/creating-managing-service-account-keys. Example: `{"type": "serviceAccount", ...`.
     */
    serviceAccountCredentials: string;
}

export interface GetServiceIntegrationEndpointExternalGoogleCloudLoggingUserConfig {
    /**
     * Google Cloud Logging log id. Example: `syslog`.
     */
    logId: string;
    /**
     * GCP project id. Example: `snappy-photon-12345`.
     */
    projectId: string;
    /**
     * This is a JSON object with the fields documented in https://cloud.google.com/iam/docs/creating-managing-service-account-keys. Example: `{"type": "serviceAccount", ...`.
     */
    serviceAccountCredentials: string;
}

export interface GetServiceIntegrationEndpointExternalKafkaUserConfig {
    /**
     * Bootstrap servers. Example: `10.0.0.1:9092,10.0.0.2:9092`.
     */
    bootstrapServers: string;
    /**
     * Enum: `PLAIN`, `SCRAM-SHA-256`, `SCRAM-SHA-512`. SASL mechanism used for connections to the Kafka server.
     */
    saslMechanism?: string;
    /**
     * Password for SASL PLAIN mechanism in the Kafka server. Example: `admin`.
     */
    saslPlainPassword?: string;
    /**
     * Username for SASL PLAIN mechanism in the Kafka server. Example: `admin`.
     */
    saslPlainUsername?: string;
    /**
     * Enum: `PLAINTEXT`, `SSL`, `SASL_PLAINTEXT`, `SASL_SSL`. Security protocol.
     */
    securityProtocol: string;
    /**
     * PEM-encoded CA certificate. Example: `-----BEGIN CERTIFICATE-----
     * ...
     * -----END CERTIFICATE-----
     * `.
     */
    sslCaCert?: string;
    /**
     * PEM-encoded client certificate. Example: `-----BEGIN CERTIFICATE-----
     * ...
     * -----END CERTIFICATE-----
     * `.
     */
    sslClientCert?: string;
    /**
     * PEM-encoded client key. Example: `-----BEGIN PRIVATE KEY-----
     * ...
     * -----END PRIVATE KEY-----
     * `.
     */
    sslClientKey?: string;
    /**
     * Enum: `https`. The endpoint identification algorithm to validate server hostname using server certificate.
     */
    sslEndpointIdentificationAlgorithm?: string;
}

export interface GetServiceIntegrationEndpointExternalMysqlUserConfig {
    /**
     * Hostname or IP address of the server. Example: `my.server.com`.
     */
    host: string;
    /**
     * Password. Example: `jjKk45Nnd`.
     */
    password: string;
    /**
     * Port number of the server. Example: `5432`.
     */
    port: number;
    /**
     * Enum: `verify-full`. SSL Mode. Default: `verify-full`.
     */
    sslMode?: string;
    /**
     * SSL Root Cert. Example: `-----BEGIN CERTIFICATE-----
     * ...
     * -----END CERTIFICATE-----
     * `.
     */
    sslRootCert?: string;
    /**
     * User name. Example: `myname`.
     */
    username: string;
}

export interface GetServiceIntegrationEndpointExternalOpensearchLogsUserConfig {
    /**
     * PEM encoded CA certificate. Example: `-----BEGIN CERTIFICATE-----
     * ...
     * -----END CERTIFICATE-----
     * `.
     */
    ca?: string;
    /**
     * Maximum number of days of logs to keep. Default: `3`.
     */
    indexDaysMax?: number;
    /**
     * OpenSearch index prefix. Default: `logs`.
     */
    indexPrefix: string;
    /**
     * OpenSearch request timeout limit. Default: `10.0`.
     */
    timeout?: number;
    /**
     * OpenSearch connection URL. Example: `https://user:passwd@logs.example.com/`.
     */
    url: string;
}

export interface GetServiceIntegrationEndpointExternalPostgresql {
    /**
     * Default database. Example: `testdb`.
     */
    defaultDatabase?: string;
    /**
     * Hostname or IP address of the server. Example: `my.server.com`.
     */
    host: string;
    /**
     * Password. Example: `jjKk45Nnd`.
     */
    password?: string;
    /**
     * Port number of the server. Example: `5432`.
     */
    port: number;
    /**
     * Client certificate. Example: `-----BEGIN CERTIFICATE-----
     * ...
     * -----END CERTIFICATE-----
     * `.
     */
    sslClientCertificate?: string;
    /**
     * Client key. Example: `-----BEGIN PRIVATE KEY-----
     * ...
     * -----END PRIVATE KEY-----`.
     */
    sslClientKey?: string;
    /**
     * Enum: `disable`, `allow`, `prefer`, `require`, `verify-ca`, `verify-full`. SSL mode to use for the connection.  Please note that Aiven requires TLS for all connections to external PostgreSQL services. Default: `verify-full`.
     */
    sslMode?: string;
    /**
     * SSL Root Cert. Example: `-----BEGIN CERTIFICATE-----
     * ...
     * -----END CERTIFICATE-----
     * `.
     */
    sslRootCert?: string;
    /**
     * User name. Example: `myname`.
     */
    username: string;
}

export interface GetServiceIntegrationEndpointExternalSchemaRegistryUserConfig {
    /**
     * Enum: `none`, `basic`. Authentication method.
     */
    authentication: string;
    /**
     * Basic authentication password. Example: `Zm9vYg==`.
     */
    basicAuthPassword?: string;
    /**
     * Basic authentication user name. Example: `avnadmin`.
     */
    basicAuthUsername?: string;
    /**
     * Schema Registry URL. Example: `https://schema-registry.kafka.company.com:28419`.
     */
    url: string;
}

export interface GetServiceIntegrationEndpointJolokiaUserConfig {
    /**
     * Jolokia basic authentication password. Example: `yhfBNFii4C`.
     */
    basicAuthPassword?: string;
    /**
     * Jolokia basic authentication username. Example: `jol48k51`.
     */
    basicAuthUsername?: string;
}

export interface GetServiceIntegrationEndpointPrometheusUserConfig {
    /**
     * Prometheus basic authentication password. Example: `fhyFNBjj3R`.
     */
    basicAuthPassword?: string;
    /**
     * Prometheus basic authentication username. Example: `prom4851`.
     */
    basicAuthUsername?: string;
}

export interface GetServiceIntegrationEndpointRsyslogUserConfig {
    /**
     * PEM encoded CA certificate. Example: `-----BEGIN CERTIFICATE-----
     * ...
     * -----END CERTIFICATE-----
     * `.
     */
    ca?: string;
    /**
     * PEM encoded client certificate. Example: `-----BEGIN CERTIFICATE-----
     * ...
     * -----END CERTIFICATE-----
     * `.
     */
    cert?: string;
    /**
     * Enum: `rfc5424`, `rfc3164`, `custom`. Message format. Default: `rfc5424`.
     */
    format: string;
    /**
     * PEM encoded client key. Example: `-----BEGIN PRIVATE KEY-----
     * ...
     * -----END PRIVATE KEY-----
     * `.
     */
    key?: string;
    /**
     * Custom syslog message format. Example: `<%pri%>%timestamp:::date-rfc3339% %HOSTNAME% %app-name% %msg%`.
     */
    logline?: string;
    /**
     * Rsyslog max message size. Default: `8192`.
     */
    maxMessageSize?: number;
    /**
     * Rsyslog server port. Default: `514`.
     */
    port: number;
    /**
     * Structured data block for log message. Example: `TOKEN tag="LiteralValue"`.
     */
    sd?: string;
    /**
     * Rsyslog server IP address or hostname. Example: `logs.example.com`.
     */
    server: string;
    /**
     * Require TLS. Default: `true`.
     */
    tls: boolean;
}

export interface GetServiceIntegrationExternalAwsCloudwatchLogsUserConfig {
    /**
     * The list of logging fields that will be sent to the integration logging service. The MESSAGE and timestamp fields are always sent.
     */
    selectedLogFields?: string[];
}

export interface GetServiceIntegrationExternalAwsCloudwatchMetricsUserConfig {
    /**
     * Metrics to not send to AWS CloudWatch (takes precedence over extra_metrics)
     */
    droppedMetrics?: outputs.GetServiceIntegrationExternalAwsCloudwatchMetricsUserConfigDroppedMetric[];
    /**
     * Metrics to allow through to AWS CloudWatch (in addition to default metrics)
     */
    extraMetrics?: outputs.GetServiceIntegrationExternalAwsCloudwatchMetricsUserConfigExtraMetric[];
}

export interface GetServiceIntegrationExternalAwsCloudwatchMetricsUserConfigDroppedMetric {
    /**
     * Identifier of a value in the metric. Example: `used`.
     */
    field: string;
    /**
     * Identifier of the metric. Example: `java.lang:Memory`.
     */
    metric: string;
}

export interface GetServiceIntegrationExternalAwsCloudwatchMetricsUserConfigExtraMetric {
    /**
     * Identifier of a value in the metric. Example: `used`.
     */
    field: string;
    /**
     * Identifier of the metric. Example: `java.lang:Memory`.
     */
    metric: string;
}

export interface GetServiceIntegrationExternalElasticsearchLogsUserConfig {
    /**
     * The list of logging fields that will be sent to the integration logging service. The MESSAGE and timestamp fields are always sent.
     */
    selectedLogFields?: string[];
}

export interface GetServiceIntegrationExternalOpensearchLogsUserConfig {
    /**
     * The list of logging fields that will be sent to the integration logging service. The MESSAGE and timestamp fields are always sent.
     */
    selectedLogFields?: string[];
}

export interface GetServiceIntegrationKafkaConnectUserConfig {
    /**
     * Kafka Connect service configuration values
     */
    kafkaConnect?: outputs.GetServiceIntegrationKafkaConnectUserConfigKafkaConnect;
}

export interface GetServiceIntegrationKafkaConnectUserConfigKafkaConnect {
    /**
     * The name of the topic where connector and task configuration data are stored.This must be the same for all workers with the same group_id. Example: `__connect_configs`.
     */
    configStorageTopic?: string;
    /**
     * A unique string that identifies the Connect cluster group this worker belongs to. Example: `connect`.
     */
    groupId?: string;
    /**
     * The name of the topic where connector and task configuration offsets are stored.This must be the same for all workers with the same group_id. Example: `__connect_offsets`.
     */
    offsetStorageTopic?: string;
    /**
     * The name of the topic where connector and task configuration status updates are stored.This must be the same for all workers with the same group_id. Example: `__connect_status`.
     */
    statusStorageTopic?: string;
}

export interface GetServiceIntegrationKafkaLogsUserConfig {
    /**
     * Topic name. Example: `mytopic`.
     */
    kafkaTopic: string;
    /**
     * The list of logging fields that will be sent to the integration logging service. The MESSAGE and timestamp fields are always sent.
     */
    selectedLogFields?: string[];
}

export interface GetServiceIntegrationKafkaMirrormakerUserConfig {
    /**
     * The alias under which the Kafka cluster is known to MirrorMaker. Can contain the following symbols: ASCII alphanumerics, `.`, `_`, and `-`. Example: `kafka-abc`.
     */
    clusterAlias?: string;
    /**
     * Kafka MirrorMaker configuration values
     */
    kafkaMirrormaker?: outputs.GetServiceIntegrationKafkaMirrormakerUserConfigKafkaMirrormaker;
}

export interface GetServiceIntegrationKafkaMirrormakerUserConfigKafkaMirrormaker {
    /**
     * Enum: `earliest`, `latest`. Set where consumer starts to consume data. Value `earliest`: Start replication from the earliest offset. Value `latest`: Start replication from the latest offset. Default is `earliest`.
     */
    consumerAutoOffsetReset?: string;
    /**
     * The minimum amount of data the server should return for a fetch request. Example: `1024`.
     */
    consumerFetchMinBytes?: number;
    /**
     * Set consumer max.poll.records. The default is 500. Example: `500`.
     */
    consumerMaxPollRecords?: number;
    /**
     * The batch size in bytes producer will attempt to collect before publishing to broker. Example: `1024`.
     */
    producerBatchSize?: number;
    /**
     * The amount of bytes producer can use for buffering data before publishing to broker. Example: `8388608`.
     */
    producerBufferMemory?: number;
    /**
     * Enum: `gzip`, `snappy`, `lz4`, `zstd`, `none`. Specify the default compression type for producers. This configuration accepts the standard compression codecs (`gzip`, `snappy`, `lz4`, `zstd`). It additionally accepts `none` which is the default and equivalent to no compression.
     */
    producerCompressionType?: string;
    /**
     * The linger time (ms) for waiting new data to arrive for publishing. Example: `100`.
     */
    producerLingerMs?: number;
    /**
     * The maximum request size in bytes. Example: `1048576`.
     */
    producerMaxRequestSize?: number;
}

export interface GetServiceIntegrationLogsUserConfig {
    /**
     * Elasticsearch index retention limit. Default: `3`.
     */
    elasticsearchIndexDaysMax?: number;
    /**
     * Elasticsearch index prefix. Default: `logs`.
     */
    elasticsearchIndexPrefix?: string;
    /**
     * The list of logging fields that will be sent to the integration logging service. The MESSAGE and timestamp fields are always sent.
     */
    selectedLogFields?: string[];
}

export interface GetServiceIntegrationMetricsUserConfig {
    /**
     * Name of the database where to store metric datapoints. Only affects PostgreSQL destinations. Defaults to `metrics`. Note that this must be the same for all metrics integrations that write data to the same PostgreSQL service.
     */
    database?: string;
    /**
     * Number of days to keep old metrics. Only affects PostgreSQL destinations. Set to 0 for no automatic cleanup. Defaults to 30 days.
     */
    retentionDays?: number;
    /**
     * Name of a user that can be used to read metrics. This will be used for Grafana integration (if enabled) to prevent Grafana users from making undesired changes. Only affects PostgreSQL destinations. Defaults to `metricsReader`. Note that this must be the same for all metrics integrations that write data to the same PostgreSQL service.
     */
    roUsername?: string;
    /**
     * Configuration options for metrics where source service is MySQL
     */
    sourceMysql?: outputs.GetServiceIntegrationMetricsUserConfigSourceMysql;
    /**
     * Name of the user used to write metrics. Only affects PostgreSQL destinations. Defaults to `metricsWriter`. Note that this must be the same for all metrics integrations that write data to the same PostgreSQL service.
     */
    username?: string;
}

export interface GetServiceIntegrationMetricsUserConfigSourceMysql {
    /**
     * Configuration options for Telegraf MySQL input plugin
     */
    telegraf?: outputs.GetServiceIntegrationMetricsUserConfigSourceMysqlTelegraf;
}

export interface GetServiceIntegrationMetricsUserConfigSourceMysqlTelegraf {
    /**
     * Gather metrics from PERFORMANCE_SCHEMA.EVENT_WAITS.
     */
    gatherEventWaits?: boolean;
    /**
     * Gather metrics from PERFORMANCE_SCHEMA.FILE_SUMMARY_BY_EVENT_NAME.
     */
    gatherFileEventsStats?: boolean;
    /**
     * Gather metrics from PERFORMANCE_SCHEMA.TABLE_IO_WAITS_SUMMARY_BY_INDEX_USAGE.
     */
    gatherIndexIoWaits?: boolean;
    /**
     * Gather autoIncrement columns and max values from information schema.
     */
    gatherInfoSchemaAutoInc?: boolean;
    /**
     * Gather metrics from INFORMATION_SCHEMA.INNODB_METRICS.
     */
    gatherInnodbMetrics?: boolean;
    /**
     * Gather metrics from PERFORMANCE_SCHEMA.EVENTS_STATEMENTS_SUMMARY_BY_DIGEST.
     */
    gatherPerfEventsStatements?: boolean;
    /**
     * Gather thread state counts from INFORMATION_SCHEMA.PROCESSLIST.
     */
    gatherProcessList?: boolean;
    /**
     * Gather metrics from SHOW SLAVE STATUS command output.
     */
    gatherSlaveStatus?: boolean;
    /**
     * Gather metrics from PERFORMANCE_SCHEMA.TABLE_IO_WAITS_SUMMARY_BY_TABLE.
     */
    gatherTableIoWaits?: boolean;
    /**
     * Gather metrics from PERFORMANCE_SCHEMA.TABLE_LOCK_WAITS.
     */
    gatherTableLockWaits?: boolean;
    /**
     * Gather metrics from INFORMATION_SCHEMA.TABLES.
     */
    gatherTableSchema?: boolean;
    /**
     * Truncates digest text from perfEventsStatements into this many characters. Example: `120`.
     */
    perfEventsStatementsDigestTextLimit?: number;
    /**
     * Limits metrics from perf_events_statements. Example: `250`.
     */
    perfEventsStatementsLimit?: number;
    /**
     * Only include perfEventsStatements whose last seen is less than this many seconds. Example: `86400`.
     */
    perfEventsStatementsTimeLimit?: number;
}

export interface GetServiceIntegrationPrometheusUserConfig {
    /**
     * Configuration options for metrics where source service is MySQL
     */
    sourceMysql?: outputs.GetServiceIntegrationPrometheusUserConfigSourceMysql;
}

export interface GetServiceIntegrationPrometheusUserConfigSourceMysql {
    /**
     * Configuration options for Telegraf MySQL input plugin
     */
    telegraf?: outputs.GetServiceIntegrationPrometheusUserConfigSourceMysqlTelegraf;
}

export interface GetServiceIntegrationPrometheusUserConfigSourceMysqlTelegraf {
    /**
     * Gather metrics from PERFORMANCE_SCHEMA.EVENT_WAITS.
     */
    gatherEventWaits?: boolean;
    /**
     * Gather metrics from PERFORMANCE_SCHEMA.FILE_SUMMARY_BY_EVENT_NAME.
     */
    gatherFileEventsStats?: boolean;
    /**
     * Gather metrics from PERFORMANCE_SCHEMA.TABLE_IO_WAITS_SUMMARY_BY_INDEX_USAGE.
     */
    gatherIndexIoWaits?: boolean;
    /**
     * Gather autoIncrement columns and max values from information schema.
     */
    gatherInfoSchemaAutoInc?: boolean;
    /**
     * Gather metrics from INFORMATION_SCHEMA.INNODB_METRICS.
     */
    gatherInnodbMetrics?: boolean;
    /**
     * Gather metrics from PERFORMANCE_SCHEMA.EVENTS_STATEMENTS_SUMMARY_BY_DIGEST.
     */
    gatherPerfEventsStatements?: boolean;
    /**
     * Gather thread state counts from INFORMATION_SCHEMA.PROCESSLIST.
     */
    gatherProcessList?: boolean;
    /**
     * Gather metrics from SHOW SLAVE STATUS command output.
     */
    gatherSlaveStatus?: boolean;
    /**
     * Gather metrics from PERFORMANCE_SCHEMA.TABLE_IO_WAITS_SUMMARY_BY_TABLE.
     */
    gatherTableIoWaits?: boolean;
    /**
     * Gather metrics from PERFORMANCE_SCHEMA.TABLE_LOCK_WAITS.
     */
    gatherTableLockWaits?: boolean;
    /**
     * Gather metrics from INFORMATION_SCHEMA.TABLES.
     */
    gatherTableSchema?: boolean;
    /**
     * Truncates digest text from perfEventsStatements into this many characters. Example: `120`.
     */
    perfEventsStatementsDigestTextLimit?: number;
    /**
     * Limits metrics from perf_events_statements. Example: `250`.
     */
    perfEventsStatementsLimit?: number;
    /**
     * Only include perfEventsStatements whose last seen is less than this many seconds. Example: `86400`.
     */
    perfEventsStatementsTimeLimit?: number;
}

export interface GetThanosComponent {
    /**
     * Service component name
     */
    component: string;
    /**
     * Connection info for connecting to the service component. This is a combination of host and port.
     */
    connectionUri: string;
    /**
     * Host name for connecting to the service component
     */
    host: string;
    /**
     * Kafka authentication method. This is a value specific to the 'kafka' service component
     */
    kafkaAuthenticationMethod: string;
    /**
     * Port number for connecting to the service component
     */
    port: number;
    /**
     * Network access route
     */
    route: string;
    /**
     * Whether the endpoint is encrypted or accepts plaintext. By default endpoints are always encrypted and this property is only included for service components they may disable encryption
     */
    ssl: boolean;
    /**
     * DNS usage name
     */
    usage: string;
}

export interface GetThanosServiceIntegration {
    /**
     * Type of the service integration. The only supported value at the moment is `readReplica`
     */
    integrationType: string;
    /**
     * Name of the source service
     */
    sourceServiceName: string;
}

export interface GetThanosTag {
    /**
     * Service tag key
     */
    key: string;
    /**
     * Service tag value
     */
    value: string;
}

export interface GetThanosTechEmail {
    /**
     * An email address to contact for technical issues
     */
    email: string;
}

export interface GetThanosThano {
    /**
     * Query frontend URI.
     */
    queryFrontendUri: string;
    /**
     * Query URI.
     */
    queryUri: string;
    /**
     * Receiver ingesting remote write URI.
     */
    receiverIngestingRemoteWriteUri: string;
    /**
     * Receiver remote write URI.
     */
    receiverRemoteWriteUri: string;
    /**
     * Store URI.
     *
     * @deprecated This field was added by mistake and has never worked. It will be removed in future versions.
     */
    storeUri: string;
    /**
     * Thanos server URIs.
     */
    uris: string[];
}

export interface GetThanosThanosUserConfig {
    /**
     * ThanosCompactor
     */
    compactor?: outputs.GetThanosThanosUserConfigCompactor;
    /**
     * Environmental variables.
     */
    env?: {[key: string]: string};
    /**
     * Allow incoming connections from CIDR address block, e.g. `10.20.0.0/16`
     */
    ipFilterObjects?: outputs.GetThanosThanosUserConfigIpFilterObject[];
    /**
     * Allow incoming connections from CIDR address block, e.g. `10.20.0.0/16`.
     */
    ipFilterStrings?: string[];
    /**
     * Allow incoming connections from CIDR address block, e.g. `10.20.0.0/16`.
     *
     * @deprecated Deprecated. Use `ipFilterString` instead.
     */
    ipFilters?: string[];
    /**
     * After exceeding the limit a service alert is going to be raised (0 means not set).
     */
    objectStorageUsageAlertThresholdGb?: number;
    /**
     * Allow access to selected service ports from the public Internet
     */
    publicAccess?: outputs.GetThanosThanosUserConfigPublicAccess;
    /**
     * ThanosQuery
     */
    query?: outputs.GetThanosThanosUserConfigQuery;
    /**
     * ThanosQueryFrontend
     */
    queryFrontend?: outputs.GetThanosThanosUserConfigQueryFrontend;
    /**
     * Store logs for the service so that they are available in the HTTP API and console.
     */
    serviceLog?: boolean;
    /**
     * Use static public IP addresses.
     */
    staticIps?: boolean;
}

export interface GetThanosThanosUserConfigCompactor {
    /**
     * Retention time for data in days for each resolution (5m, 1h, raw).
     */
    retentionDays?: number;
}

export interface GetThanosThanosUserConfigIpFilterObject {
    /**
     * Description for IP filter list entry. Example: `Production service IP range`.
     */
    description?: string;
    /**
     * CIDR address block. Example: `10.20.0.0/16`.
     */
    network: string;
}

export interface GetThanosThanosUserConfigPublicAccess {
    /**
     * Allow clients to connect to compactor from the public internet for service nodes that are in a project VPC or another type of private network.
     */
    compactor?: boolean;
    /**
     * Allow clients to connect to query from the public internet for service nodes that are in a project VPC or another type of private network.
     */
    query?: boolean;
    /**
     * Allow clients to connect to queryFrontend from the public internet for service nodes that are in a project VPC or another type of private network.
     */
    queryFrontend?: boolean;
    /**
     * Allow clients to connect to receiverIngesting from the public internet for service nodes that are in a project VPC or another type of private network.
     */
    receiverIngesting?: boolean;
    /**
     * Allow clients to connect to receiverRouting from the public internet for service nodes that are in a project VPC or another type of private network.
     */
    receiverRouting?: boolean;
    /**
     * Allow clients to connect to store from the public internet for service nodes that are in a project VPC or another type of private network.
     */
    store?: boolean;
}

export interface GetThanosThanosUserConfigQuery {
    /**
     * Set the default evaluation interval for subqueries. Default: `1m`.
     */
    queryDefaultEvaluationInterval?: string;
    /**
     * The maximum lookback duration for retrieving metrics during expression evaluations in PromQL. PromQL always evaluates the query for a certain timestamp, and it looks back for the given amount of time to get the latest sample. If it exceeds the maximum lookback delta, it assumes the series is stale and returns none (a gap). The lookback delta should be set to at least 2 times the slowest scrape interval. If unset, it will use the promql default of 5m. Default: `5m`.
     */
    queryLookbackDelta?: string;
    /**
     * The default metadata time range duration for retrieving labels through Labels and Series API when the range parameters are not specified. The zero value means the range covers the time since the beginning. Default: `0s`.
     */
    queryMetadataDefaultTimeRange?: string;
    /**
     * Maximum time to process a query by the query node. Default: `2m`.
     */
    queryTimeout?: string;
    /**
     * The maximum samples allowed for a single Series request. The Series call fails if this limit is exceeded. Set to 0 for no limit. NOTE: For efficiency, the limit is internally implemented as 'chunks limit' considering each chunk contains a maximum of 120 samples. The default value is 100 * store.limits.request-series. Default: `0`.
     */
    storeLimitsRequestSamples?: number;
    /**
     * The maximum series allowed for a single Series request. The Series call fails if this limit is exceeded. Set to 0 for no limit. The default value is 1000 * cpu_count. Default: `0`.
     */
    storeLimitsRequestSeries?: number;
}

export interface GetThanosThanosUserConfigQueryFrontend {
    /**
     * Whether to align the query range boundaries with the step. If enabled, the query range boundaries will be aligned to the step, providing more accurate results for queries with high-resolution data. Default: `true`.
     */
    queryRangeAlignRangeWithStep?: boolean;
}

export interface GetValkeyComponent {
    /**
     * Service component name
     */
    component: string;
    /**
     * Connection info for connecting to the service component. This is a combination of host and port.
     */
    connectionUri: string;
    /**
     * Host name for connecting to the service component
     */
    host: string;
    /**
     * Kafka authentication method. This is a value specific to the 'kafka' service component
     */
    kafkaAuthenticationMethod: string;
    /**
     * Port number for connecting to the service component
     */
    port: number;
    /**
     * Network access route
     */
    route: string;
    /**
     * Whether the endpoint is encrypted or accepts plaintext. By default endpoints are always encrypted and this property is only included for service components they may disable encryption
     */
    ssl: boolean;
    /**
     * DNS usage name
     */
    usage: string;
}

export interface GetValkeyServiceIntegration {
    /**
     * Type of the service integration. The only supported value at the moment is `readReplica`
     */
    integrationType: string;
    /**
     * Name of the source service
     */
    sourceServiceName: string;
}

export interface GetValkeyTag {
    /**
     * Service tag key
     */
    key: string;
    /**
     * Service tag value
     */
    value: string;
}

export interface GetValkeyTechEmail {
    /**
     * An email address to contact for technical issues
     */
    email: string;
}

export interface GetValkeyValkey {
    /**
     * Valkey password.
     */
    password: string;
    /**
     * Valkey replica server URI.
     */
    replicaUri: string;
    /**
     * Valkey slave server URIs.
     */
    slaveUris: string[];
    /**
     * Valkey server URIs.
     */
    uris: string[];
}

export interface GetValkeyValkeyUserConfig {
    /**
     * Additional Cloud Regions for Backup Replication.
     */
    additionalBackupRegions?: string;
    /**
     * The hour of day (in UTC) when backup for the service is started. New backup is only started if previous backup has already completed. Example: `3`.
     */
    backupHour?: number;
    /**
     * The minute of an hour when backup for the service is started. New backup is only started if previous backup has already completed. Example: `30`.
     */
    backupMinute?: number;
    /**
     * Allow incoming connections from CIDR address block, e.g. `10.20.0.0/16`
     */
    ipFilterObjects?: outputs.GetValkeyValkeyUserConfigIpFilterObject[];
    /**
     * Allow incoming connections from CIDR address block, e.g. `10.20.0.0/16`.
     */
    ipFilterStrings?: string[];
    /**
     * Allow incoming connections from CIDR address block, e.g. `10.20.0.0/16`.
     *
     * @deprecated Deprecated. Use `ipFilterString` instead.
     */
    ipFilters?: string[];
    /**
     * Migrate data from existing server
     */
    migration?: outputs.GetValkeyValkeyUserConfigMigration;
    /**
     * Allow access to selected service ports from private networks
     */
    privateAccess?: outputs.GetValkeyValkeyUserConfigPrivateAccess;
    /**
     * Allow access to selected service components through Privatelink
     */
    privatelinkAccess?: outputs.GetValkeyValkeyUserConfigPrivatelinkAccess;
    /**
     * Name of another project to fork a service from. This has effect only when a new service is being created. Example: `anotherprojectname`.
     */
    projectToForkFrom?: string;
    /**
     * Allow access to selected service ports from the public Internet
     */
    publicAccess?: outputs.GetValkeyValkeyUserConfigPublicAccess;
    /**
     * Name of the basebackup to restore in forked service. Example: `backup-20191112t091354293891z`.
     */
    recoveryBasebackupName?: string;
    /**
     * Store logs for the service so that they are available in the HTTP API and console.
     */
    serviceLog?: boolean;
    /**
     * Name of another service to fork from. This has effect only when a new service is being created. Example: `anotherservicename`.
     */
    serviceToForkFrom?: string;
    /**
     * Use static public IP addresses.
     */
    staticIps?: boolean;
    /**
     * Enum: `allchannels`, `resetchannels`. Determines default pub/sub channels' ACL for new users if ACL is not supplied. When this option is not defined, allChannels is assumed to keep backward compatibility. This option doesn't affect Valkey configuration acl-pubsub-default.
     */
    valkeyAclChannelsDefault?: string;
    /**
     * Set Valkey IO thread count. Changing this will cause a restart of the Valkey service. Example: `1`.
     */
    valkeyIoThreads?: number;
    /**
     * LFU maxmemory-policy counter decay time in minutes. Default: `1`.
     */
    valkeyLfuDecayTime?: number;
    /**
     * Counter logarithm factor for volatile-lfu and allkeys-lfu maxmemory-policies. Default: `10`.
     */
    valkeyLfuLogFactor?: number;
    /**
     * Enum: `noeviction`, `allkeys-lru`, `volatile-lru`, `allkeys-random`, `volatile-random`, `volatile-ttl`, `volatile-lfu`, `allkeys-lfu`. Valkey maxmemory-policy. Default: `noeviction`.
     */
    valkeyMaxmemoryPolicy?: string;
    /**
     * Set notify-keyspace-events option.
     */
    valkeyNotifyKeyspaceEvents?: string;
    /**
     * Set number of Valkey databases. Changing this will cause a restart of the Valkey service. Example: `16`.
     */
    valkeyNumberOfDatabases?: number;
    /**
     * Enum: `off`, `rdb`. When persistence is `rdb`, Valkey does RDB dumps each 10 minutes if any key is changed. Also RDB dumps are done according to backup schedule for backup purposes. When persistence is `off`, no RDB dumps and backups are done, so data can be lost at any moment if service is restarted for any reason, or if service is powered off. Also service can't be forked.
     */
    valkeyPersistence?: string;
    /**
     * Set output buffer limit for pub / sub clients in MB. The value is the hard limit, the soft limit is 1/4 of the hard limit. When setting the limit, be mindful of the available memory in the selected service plan. Example: `64`.
     */
    valkeyPubsubClientOutputBufferLimit?: number;
    /**
     * Require SSL to access Valkey. Default: `true`.
     */
    valkeySsl?: boolean;
    /**
     * Valkey idle connection timeout in seconds. Default: `300`.
     */
    valkeyTimeout?: number;
}

export interface GetValkeyValkeyUserConfigIpFilterObject {
    /**
     * Description for IP filter list entry. Example: `Production service IP range`.
     */
    description?: string;
    /**
     * CIDR address block. Example: `10.20.0.0/16`.
     */
    network: string;
}

export interface GetValkeyValkeyUserConfigMigration {
    /**
     * Database name for bootstrapping the initial connection. Example: `defaultdb`.
     */
    dbname?: string;
    /**
     * Hostname or IP address of the server where to migrate data from. Example: `my.server.com`.
     */
    host: string;
    /**
     * Comma-separated list of databases, which should be ignored during migration (supported by MySQL and PostgreSQL only at the moment). Example: `db1,db2`.
     */
    ignoreDbs?: string;
    /**
     * Comma-separated list of database roles, which should be ignored during migration (supported by PostgreSQL only at the moment). Example: `role1,role2`.
     */
    ignoreRoles?: string;
    /**
     * Enum: `dump`, `replication`. The migration method to be used (currently supported only by Redis, Dragonfly, MySQL and PostgreSQL service types).
     */
    method?: string;
    /**
     * Password for authentication with the server where to migrate data from. Example: `jjKk45Nnd`.
     */
    password?: string;
    /**
     * Port number of the server where to migrate data from. Example: `1234`.
     */
    port: number;
    /**
     * The server where to migrate data from is secured with SSL. Default: `true`.
     */
    ssl?: boolean;
    /**
     * User name for authentication with the server where to migrate data from. Example: `myname`.
     */
    username?: string;
}

export interface GetValkeyValkeyUserConfigPrivateAccess {
    /**
     * Allow clients to connect to prometheus with a DNS name that always resolves to the service's private IP addresses. Only available in certain network locations.
     */
    prometheus?: boolean;
    /**
     * Allow clients to connect to valkey with a DNS name that always resolves to the service's private IP addresses. Only available in certain network locations.
     */
    valkey?: boolean;
}

export interface GetValkeyValkeyUserConfigPrivatelinkAccess {
    /**
     * Enable prometheus.
     */
    prometheus?: boolean;
    /**
     * Enable valkey.
     */
    valkey?: boolean;
}

export interface GetValkeyValkeyUserConfigPublicAccess {
    /**
     * Allow clients to connect to prometheus from the public internet for service nodes that are in a project VPC or another type of private network.
     */
    prometheus?: boolean;
    /**
     * Allow clients to connect to valkey from the public internet for service nodes that are in a project VPC or another type of private network.
     */
    valkey?: boolean;
}

export interface GrafanaComponent {
    /**
     * Service component name
     */
    component: string;
    /**
     * Connection info for connecting to the service component. This is a combination of host and port.
     */
    connectionUri: string;
    /**
     * Host name for connecting to the service component
     */
    host: string;
    /**
     * Kafka authentication method. This is a value specific to the 'kafka' service component
     */
    kafkaAuthenticationMethod: string;
    /**
     * Port number for connecting to the service component
     */
    port: number;
    /**
     * Network access route
     */
    route: string;
    /**
     * Whether the endpoint is encrypted or accepts plaintext. By default endpoints are always encrypted and this property is only included for service components they may disable encryption
     */
    ssl: boolean;
    /**
     * DNS usage name
     */
    usage: string;
}

export interface GrafanaGrafana {
    /**
     * Grafana server URIs.
     */
    uris: string[];
}

export interface GrafanaGrafanaUserConfig {
    /**
     * Additional Cloud Regions for Backup Replication.
     */
    additionalBackupRegions?: string;
    /**
     * Enable or disable Grafana legacy alerting functionality. This should not be enabled with unified*alerting*enabled.
     */
    alertingEnabled?: boolean;
    /**
     * Enum: `alerting`, `keepState`. Default error or timeout setting for new alerting rules.
     */
    alertingErrorOrTimeout?: string;
    /**
     * Max number of alert annotations that Grafana stores. 0 (default) keeps all alert annotations. Example: `0`.
     */
    alertingMaxAnnotationsToKeep?: number;
    /**
     * Enum: `alerting`, `noData`, `keepState`, `ok`. Default value for 'no data or null values' for new alerting rules.
     */
    alertingNodataOrNullvalues?: string;
    /**
     * Allow embedding Grafana dashboards with iframe/frame/object/embed tags. Disabled by default to limit impact of clickjacking.
     */
    allowEmbedding?: boolean;
    /**
     * Azure AD OAuth integration
     */
    authAzuread?: outputs.GrafanaGrafanaUserConfigAuthAzuread;
    /**
     * Enable or disable basic authentication form, used by Grafana built-in login.
     */
    authBasicEnabled?: boolean;
    /**
     * Generic OAuth integration
     */
    authGenericOauth?: outputs.GrafanaGrafanaUserConfigAuthGenericOauth;
    /**
     * Github Auth integration
     */
    authGithub?: outputs.GrafanaGrafanaUserConfigAuthGithub;
    /**
     * GitLab Auth integration
     */
    authGitlab?: outputs.GrafanaGrafanaUserConfigAuthGitlab;
    /**
     * Google Auth integration
     */
    authGoogle?: outputs.GrafanaGrafanaUserConfigAuthGoogle;
    /**
     * Enum: `lax`, `strict`, `none`. Cookie SameSite attribute: `strict` prevents sending cookie for cross-site requests, effectively disabling direct linking from other sites to Grafana. `lax` is the default value.
     */
    cookieSamesite?: string;
    /**
     * Serve the web frontend using a custom CNAME pointing to the Aiven DNS name. Example: `grafana.example.org`.
     */
    customDomain?: string;
    /**
     * This feature is new in Grafana 9 and is quite resource intensive. It may cause low-end plans to work more slowly while the dashboard previews are rendering.
     */
    dashboardPreviewsEnabled?: boolean;
    /**
     * Signed sequence of decimal numbers, followed by a unit suffix (ms, s, m, h, d), e.g. 30s, 1h. Example: `5s`.
     */
    dashboardsMinRefreshInterval?: string;
    /**
     * Dashboard versions to keep per dashboard. Example: `20`.
     */
    dashboardsVersionsToKeep?: number;
    /**
     * Send `X-Grafana-User` header to data source.
     */
    dataproxySendUserHeader?: boolean;
    /**
     * Timeout for data proxy requests in seconds. Example: `30`.
     */
    dataproxyTimeout?: number;
    /**
     * Grafana date format specifications
     */
    dateFormats?: outputs.GrafanaGrafanaUserConfigDateFormats;
    /**
     * Set to true to disable gravatar. Defaults to false (gravatar is enabled).
     */
    disableGravatar?: boolean;
    /**
     * Editors can manage folders, teams and dashboards created by them.
     */
    editorsCanAdmin?: boolean;
    /**
     * External image store settings
     */
    externalImageStorage?: outputs.GrafanaGrafanaUserConfigExternalImageStorage;
    /**
     * Google Analytics ID. Example: `UA-123456-4`.
     */
    googleAnalyticsUaId?: string;
    /**
     * Allow incoming connections from CIDR address block, e.g. `10.20.0.0/16`
     */
    ipFilterObjects?: outputs.GrafanaGrafanaUserConfigIpFilterObject[];
    /**
     * Allow incoming connections from CIDR address block, e.g. `10.20.0.0/16`.
     */
    ipFilterStrings?: string[];
    /**
     * Allow incoming connections from CIDR address block, e.g. `10.20.0.0/16`.
     *
     * @deprecated Deprecated. Use `ipFilterString` instead.
     */
    ipFilters?: string[];
    /**
     * Enable Grafana /metrics endpoint.
     */
    metricsEnabled?: boolean;
    /**
     * Enforce user lookup based on email instead of the unique ID provided by the IdP.
     */
    oauthAllowInsecureEmailLookup?: boolean;
    /**
     * Allow access to selected service ports from private networks
     */
    privateAccess?: outputs.GrafanaGrafanaUserConfigPrivateAccess;
    /**
     * Allow access to selected service components through Privatelink
     */
    privatelinkAccess?: outputs.GrafanaGrafanaUserConfigPrivatelinkAccess;
    /**
     * Name of another project to fork a service from. This has effect only when a new service is being created. Example: `anotherprojectname`.
     */
    projectToForkFrom?: string;
    /**
     * Allow access to selected service ports from the public Internet
     */
    publicAccess?: outputs.GrafanaGrafanaUserConfigPublicAccess;
    /**
     * Name of the basebackup to restore in forked service. Example: `backup-20191112t091354293891z`.
     */
    recoveryBasebackupName?: string;
    /**
     * Store logs for the service so that they are available in the HTTP API and console.
     */
    serviceLog?: boolean;
    /**
     * Name of another service to fork from. This has effect only when a new service is being created. Example: `anotherservicename`.
     */
    serviceToForkFrom?: string;
    /**
     * SMTP server settings
     */
    smtpServer?: outputs.GrafanaGrafanaUserConfigSmtpServer;
    /**
     * Use static public IP addresses.
     */
    staticIps?: boolean;
    /**
     * Enable or disable Grafana unified alerting functionality. By default this is enabled and any legacy alerts will be migrated on upgrade to Grafana 9+. To stay on legacy alerting, set unified*alerting*enabled to false and alertingEnabled to true. See https://grafana.com/docs/grafana/latest/alerting/set-up/migrating-alerts/ for more details.
     */
    unifiedAlertingEnabled?: boolean;
    /**
     * Auto-assign new users on signup to main organization. Defaults to false.
     */
    userAutoAssignOrg?: boolean;
    /**
     * Enum: `Viewer`, `Admin`, `Editor`. Set role for new signups. Defaults to Viewer.
     */
    userAutoAssignOrgRole?: string;
    /**
     * Users with view-only permission can edit but not save dashboards.
     */
    viewersCanEdit?: boolean;
    /**
     * Setting to enable/disable Write-Ahead Logging. The default value is false (disabled).
     */
    wal?: boolean;
}

export interface GrafanaGrafanaUserConfigAuthAzuread {
    /**
     * Automatically sign-up users on successful sign-in.
     */
    allowSignUp?: boolean;
    /**
     * Allowed domains.
     */
    allowedDomains?: string[];
    /**
     * Require users to belong to one of given groups.
     */
    allowedGroups?: string[];
    /**
     * Authorization URL. Example: `https://login.microsoftonline.com/<AZURE_TENANT_ID>/oauth2/v2.0/authorize`.
     */
    authUrl: string;
    /**
     * Client ID from provider. Example: `b1ba0bf54a4c2c0a1c29`.
     */
    clientId: string;
    /**
     * Client secret from provider. Example: `bfa6gea4f129076761dcba8ce5e1e406bd83af7b`.
     */
    clientSecret: string;
    /**
     * Token URL. Example: `https://login.microsoftonline.com/<AZURE_TENANT_ID>/oauth2/v2.0/token`.
     */
    tokenUrl: string;
}

export interface GrafanaGrafanaUserConfigAuthGenericOauth {
    /**
     * Automatically sign-up users on successful sign-in.
     */
    allowSignUp?: boolean;
    /**
     * Allowed domains.
     */
    allowedDomains?: string[];
    /**
     * Require user to be member of one of the listed organizations.
     */
    allowedOrganizations?: string[];
    /**
     * API URL. Example: `https://yourprovider.com/api`.
     */
    apiUrl: string;
    /**
     * Authorization URL. Example: `https://yourprovider.com/oauth/authorize`.
     */
    authUrl: string;
    /**
     * Allow users to bypass the login screen and automatically log in.
     */
    autoLogin?: boolean;
    /**
     * Client ID from provider. Example: `b1ba0bf54a4c2c0a1c29`.
     */
    clientId: string;
    /**
     * Client secret from provider. Example: `bfa6gea4f129076761dcba8ce5e1e406bd83af7b`.
     */
    clientSecret: string;
    /**
     * Name of the OAuth integration. Example: `My authentication`.
     */
    name?: string;
    /**
     * OAuth scopes.
     */
    scopes?: string[];
    /**
     * Token URL. Example: `https://yourprovider.com/oauth/token`.
     */
    tokenUrl: string;
    /**
     * Set to true to use refresh token and check access token expiration.
     */
    useRefreshToken?: boolean;
}

export interface GrafanaGrafanaUserConfigAuthGithub {
    /**
     * Automatically sign-up users on successful sign-in.
     */
    allowSignUp?: boolean;
    /**
     * Require users to belong to one of given organizations.
     */
    allowedOrganizations?: string[];
    /**
     * Allow users to bypass the login screen and automatically log in.
     */
    autoLogin?: boolean;
    /**
     * Client ID from provider. Example: `b1ba0bf54a4c2c0a1c29`.
     */
    clientId: string;
    /**
     * Client secret from provider. Example: `bfa6gea4f129076761dcba8ce5e1e406bd83af7b`.
     */
    clientSecret: string;
    /**
     * Stop automatically syncing user roles.
     */
    skipOrgRoleSync?: boolean;
    /**
     * Require users to belong to one of given team IDs.
     */
    teamIds?: number[];
}

export interface GrafanaGrafanaUserConfigAuthGitlab {
    /**
     * Automatically sign-up users on successful sign-in.
     */
    allowSignUp?: boolean;
    /**
     * Require users to belong to one of given groups.
     */
    allowedGroups: string[];
    /**
     * API URL. This only needs to be set when using self hosted GitLab. Example: `https://gitlab.com/api/v4`.
     */
    apiUrl?: string;
    /**
     * Authorization URL. This only needs to be set when using self hosted GitLab. Example: `https://gitlab.com/oauth/authorize`.
     */
    authUrl?: string;
    /**
     * Client ID from provider. Example: `b1ba0bf54a4c2c0a1c29`.
     */
    clientId: string;
    /**
     * Client secret from provider. Example: `bfa6gea4f129076761dcba8ce5e1e406bd83af7b`.
     */
    clientSecret: string;
    /**
     * Token URL. This only needs to be set when using self hosted GitLab. Example: `https://gitlab.com/oauth/token`.
     */
    tokenUrl?: string;
}

export interface GrafanaGrafanaUserConfigAuthGoogle {
    /**
     * Automatically sign-up users on successful sign-in.
     */
    allowSignUp?: boolean;
    /**
     * Domains allowed to sign-in to this Grafana.
     */
    allowedDomains: string[];
    /**
     * Client ID from provider. Example: `b1ba0bf54a4c2c0a1c29`.
     */
    clientId: string;
    /**
     * Client secret from provider. Example: `bfa6gea4f129076761dcba8ce5e1e406bd83af7b`.
     */
    clientSecret: string;
}

export interface GrafanaGrafanaUserConfigDateFormats {
    /**
     * Default time zone for user preferences. Value `browser` uses browser local time zone. Example: `Europe/Helsinki`.
     */
    defaultTimezone?: string;
    /**
     * Moment.js style format string for cases where full date is shown. Example: `YYYY MM DD`.
     */
    fullDate?: string;
    /**
     * Moment.js style format string used when a time requiring day accuracy is shown. Example: `MM/DD`.
     */
    intervalDay?: string;
    /**
     * Moment.js style format string used when a time requiring hour accuracy is shown. Example: `MM/DD HH:mm`.
     */
    intervalHour?: string;
    /**
     * Moment.js style format string used when a time requiring minute accuracy is shown. Example: `HH:mm`.
     */
    intervalMinute?: string;
    /**
     * Moment.js style format string used when a time requiring month accuracy is shown. Example: `YYYY-MM`.
     */
    intervalMonth?: string;
    /**
     * Moment.js style format string used when a time requiring second accuracy is shown. Example: `HH:mm:ss`.
     */
    intervalSecond?: string;
    /**
     * Moment.js style format string used when a time requiring year accuracy is shown. Example: `YYYY`.
     */
    intervalYear?: string;
}

export interface GrafanaGrafanaUserConfigExternalImageStorage {
    /**
     * S3 access key. Requires permissions to the S3 bucket for the s3:PutObject and s3:PutObjectAcl actions. Example: `AAAAAAAAAAAAAAAAAAA`.
     */
    accessKey: string;
    /**
     * Bucket URL for S3. Example: `https://grafana.s3-ap-southeast-2.amazonaws.com/`.
     */
    bucketUrl: string;
    /**
     * Enum: `s3`. Provider type.
     */
    provider: string;
    /**
     * S3 secret key. Example: `AAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAA`.
     */
    secretKey: string;
}

export interface GrafanaGrafanaUserConfigIpFilterObject {
    /**
     * Description for IP filter list entry. Example: `Production service IP range`.
     */
    description?: string;
    /**
     * CIDR address block. Example: `10.20.0.0/16`.
     */
    network: string;
}

export interface GrafanaGrafanaUserConfigPrivateAccess {
    /**
     * Allow clients to connect to grafana with a DNS name that always resolves to the service's private IP addresses. Only available in certain network locations.
     */
    grafana?: boolean;
}

export interface GrafanaGrafanaUserConfigPrivatelinkAccess {
    /**
     * Enable grafana.
     */
    grafana?: boolean;
}

export interface GrafanaGrafanaUserConfigPublicAccess {
    /**
     * Allow clients to connect to grafana from the public internet for service nodes that are in a project VPC or another type of private network.
     */
    grafana?: boolean;
}

export interface GrafanaGrafanaUserConfigSmtpServer {
    /**
     * Address used for sending emails. Example: `yourgrafanauser@yourdomain.example.com`.
     */
    fromAddress: string;
    /**
     * Name used in outgoing emails, defaults to Grafana.
     */
    fromName?: string;
    /**
     * Server hostname or IP. Example: `smtp.example.com`.
     */
    host: string;
    /**
     * Password for SMTP authentication. Example: `ein0eemeev5eeth3Ahfu`.
     */
    password?: string;
    /**
     * SMTP server port. Example: `25`.
     */
    port: number;
    /**
     * Skip verifying server certificate. Defaults to false.
     */
    skipVerify?: boolean;
    /**
     * Enum: `OpportunisticStartTLS`, `MandatoryStartTLS`, `NoStartTLS`. Either OpportunisticStartTLS, MandatoryStartTLS or NoStartTLS. Default is OpportunisticStartTLS.
     */
    starttlsPolicy?: string;
    /**
     * Username for SMTP authentication. Example: `smtpuser`.
     */
    username?: string;
}

export interface GrafanaServiceIntegration {
    /**
     * Type of the service integration. The only supported value at the moment is `readReplica`
     */
    integrationType: string;
    /**
     * Name of the source service
     */
    sourceServiceName: string;
}

export interface GrafanaTag {
    /**
     * Service tag key
     */
    key: string;
    /**
     * Service tag value
     */
    value: string;
}

export interface GrafanaTechEmail {
    /**
     * An email address to contact for technical issues
     */
    email: string;
}

export interface InfluxDbComponent {
    /**
     * Service component name
     */
    component: string;
    /**
     * Connection info for connecting to the service component. This is a combination of host and port.
     */
    connectionUri: string;
    /**
     * Host name for connecting to the service component
     */
    host: string;
    /**
     * Kafka authentication method. This is a value specific to the 'kafka' service component
     */
    kafkaAuthenticationMethod: string;
    /**
     * Port number for connecting to the service component
     */
    port: number;
    /**
     * Network access route
     */
    route: string;
    /**
     * Whether the endpoint is encrypted or accepts plaintext. By default endpoints are always encrypted and this property is only included for service components they may disable encryption
     */
    ssl: boolean;
    /**
     * DNS usage name
     */
    usage: string;
}

export interface InfluxDbInfluxdb {
    /**
     * Name of the default InfluxDB database
     */
    databaseName: string;
    /**
     * InfluxDB password
     */
    password: string;
    /**
     * InfluxDB server URIs.
     */
    uris: string[];
    /**
     * InfluxDB username
     */
    username: string;
}

export interface InfluxDbInfluxdbUserConfig {
    /**
     * Additional Cloud Regions for Backup Replication.
     *
     * @deprecated This property is deprecated.
     */
    additionalBackupRegions?: string;
    /**
     * Serve the web frontend using a custom CNAME pointing to the Aiven DNS name. Example: `grafana.example.org`.
     */
    customDomain?: string;
    /**
     * influxdb.conf configuration values
     */
    influxdb?: outputs.InfluxDbInfluxdbUserConfigInfluxdb;
    /**
     * Allow incoming connections from CIDR address block, e.g. `10.20.0.0/16`
     */
    ipFilterObjects?: outputs.InfluxDbInfluxdbUserConfigIpFilterObject[];
    /**
     * Allow incoming connections from CIDR address block, e.g. `10.20.0.0/16`.
     */
    ipFilterStrings?: string[];
    /**
     * Allow incoming connections from CIDR address block, e.g. `10.20.0.0/16`.
     *
     * @deprecated Deprecated. Use `ipFilterString` instead.
     */
    ipFilters?: string[];
    /**
     * Allow access to selected service ports from private networks
     */
    privateAccess?: outputs.InfluxDbInfluxdbUserConfigPrivateAccess;
    /**
     * Allow access to selected service components through Privatelink
     */
    privatelinkAccess?: outputs.InfluxDbInfluxdbUserConfigPrivatelinkAccess;
    /**
     * Name of another project to fork a service from. This has effect only when a new service is being created. Example: `anotherprojectname`.
     */
    projectToForkFrom?: string;
    /**
     * Allow access to selected service ports from the public Internet
     */
    publicAccess?: outputs.InfluxDbInfluxdbUserConfigPublicAccess;
    /**
     * Name of the basebackup to restore in forked service. Example: `backup-20191112t091354293891z`.
     */
    recoveryBasebackupName?: string;
    /**
     * Store logs for the service so that they are available in the HTTP API and console.
     */
    serviceLog?: boolean;
    /**
     * Name of another service to fork from. This has effect only when a new service is being created. Example: `anotherservicename`.
     */
    serviceToForkFrom?: string;
    /**
     * Use static public IP addresses.
     */
    staticIps?: boolean;
}

export interface InfluxDbInfluxdbUserConfigInfluxdb {
    /**
     * The maximum duration in seconds before a query is logged as a slow query. Setting this to 0 (the default) will never log slow queries.
     */
    logQueriesAfter?: number;
    /**
     * Maximum number of connections to InfluxDB. Setting this to 0 (default) means no limit. If using max_connection_limit, it is recommended to set the value to be large enough in order to not block clients unnecessarily.
     */
    maxConnectionLimit?: number;
    /**
     * The maximum number of rows returned in a non-chunked query. Setting this to 0 (the default) allows an unlimited number to be returned.
     */
    maxRowLimit?: number;
    /**
     * The maximum number of `GROUP BY time()` buckets that can be processed in a query. Setting this to 0 (the default) allows an unlimited number to be processed.
     */
    maxSelectBuckets?: number;
    /**
     * The maximum number of points that can be processed in a SELECT statement. Setting this to 0 (the default) allows an unlimited number to be processed.
     */
    maxSelectPoint?: number;
    /**
     * Whether queries should be logged before execution. May log sensitive data contained within a query.
     */
    queryLogEnabled?: boolean;
    /**
     * The maximum duration in seconds before a query is killed. Setting this to 0 (the default) will never kill slow queries.
     */
    queryTimeout?: number;
}

export interface InfluxDbInfluxdbUserConfigIpFilterObject {
    /**
     * Description for IP filter list entry. Example: `Production service IP range`.
     */
    description?: string;
    /**
     * CIDR address block. Example: `10.20.0.0/16`.
     */
    network: string;
}

export interface InfluxDbInfluxdbUserConfigPrivateAccess {
    /**
     * Allow clients to connect to influxdb with a DNS name that always resolves to the service's private IP addresses. Only available in certain network locations.
     */
    influxdb?: boolean;
}

export interface InfluxDbInfluxdbUserConfigPrivatelinkAccess {
    /**
     * Enable influxdb.
     */
    influxdb?: boolean;
}

export interface InfluxDbInfluxdbUserConfigPublicAccess {
    /**
     * Allow clients to connect to influxdb from the public internet for service nodes that are in a project VPC or another type of private network.
     */
    influxdb?: boolean;
}

export interface InfluxDbServiceIntegration {
    /**
     * Type of the service integration. The only supported value at the moment is `readReplica`
     */
    integrationType: string;
    /**
     * Name of the source service
     */
    sourceServiceName: string;
}

export interface InfluxDbTag {
    /**
     * Service tag key
     */
    key: string;
    /**
     * Service tag value
     */
    value: string;
}

export interface InfluxDbTechEmail {
    /**
     * An email address to contact for technical issues
     */
    email: string;
}

export interface KafkaComponent {
    /**
     * Service component name
     */
    component: string;
    /**
     * Connection info for connecting to the service component. This is a combination of host and port.
     */
    connectionUri: string;
    /**
     * Host name for connecting to the service component
     */
    host: string;
    /**
     * Kafka authentication method. This is a value specific to the 'kafka' service component
     */
    kafkaAuthenticationMethod: string;
    /**
     * Port number for connecting to the service component
     */
    port: number;
    /**
     * Network access route
     */
    route: string;
    /**
     * Whether the endpoint is encrypted or accepts plaintext. By default endpoints are always encrypted and this property is only included for service components they may disable encryption
     */
    ssl: boolean;
    /**
     * DNS usage name
     */
    usage: string;
}

export interface KafkaConnectComponent {
    /**
     * Service component name
     */
    component: string;
    /**
     * Connection info for connecting to the service component. This is a combination of host and port.
     */
    connectionUri: string;
    /**
     * Host name for connecting to the service component
     */
    host: string;
    /**
     * Kafka authentication method. This is a value specific to the 'kafka' service component
     */
    kafkaAuthenticationMethod: string;
    /**
     * Port number for connecting to the service component
     */
    port: number;
    /**
     * Network access route
     */
    route: string;
    /**
     * Whether the endpoint is encrypted or accepts plaintext. By default endpoints are always encrypted and this property is only included for service components they may disable encryption
     */
    ssl: boolean;
    /**
     * DNS usage name
     */
    usage: string;
}

export interface KafkaConnectKafkaConnectUserConfig {
    /**
     * Additional Cloud Regions for Backup Replication.
     *
     * @deprecated This property is deprecated.
     */
    additionalBackupRegions?: string;
    /**
     * Allow incoming connections from CIDR address block, e.g. `10.20.0.0/16`
     */
    ipFilterObjects?: outputs.KafkaConnectKafkaConnectUserConfigIpFilterObject[];
    /**
     * Allow incoming connections from CIDR address block, e.g. `10.20.0.0/16`.
     */
    ipFilterStrings?: string[];
    /**
     * Allow incoming connections from CIDR address block, e.g. `10.20.0.0/16`.
     *
     * @deprecated Deprecated. Use `ipFilterString` instead.
     */
    ipFilters?: string[];
    /**
     * Kafka Connect configuration values
     */
    kafkaConnect?: outputs.KafkaConnectKafkaConnectUserConfigKafkaConnect;
    /**
     * Allow access to selected service ports from private networks
     */
    privateAccess?: outputs.KafkaConnectKafkaConnectUserConfigPrivateAccess;
    /**
     * Allow access to selected service components through Privatelink
     */
    privatelinkAccess?: outputs.KafkaConnectKafkaConnectUserConfigPrivatelinkAccess;
    /**
     * Allow access to selected service ports from the public Internet
     */
    publicAccess?: outputs.KafkaConnectKafkaConnectUserConfigPublicAccess;
    secretProviders?: outputs.KafkaConnectKafkaConnectUserConfigSecretProvider[];
    /**
     * Store logs for the service so that they are available in the HTTP API and console.
     */
    serviceLog?: boolean;
    /**
     * Use static public IP addresses.
     */
    staticIps?: boolean;
}

export interface KafkaConnectKafkaConnectUserConfigIpFilterObject {
    /**
     * Description for IP filter list entry. Example: `Production service IP range`.
     */
    description?: string;
    /**
     * CIDR address block. Example: `10.20.0.0/16`.
     */
    network: string;
}

export interface KafkaConnectKafkaConnectUserConfigKafkaConnect {
    /**
     * Enum: `None`, `All`. Defines what client configurations can be overridden by the connector. Default is None.
     */
    connectorClientConfigOverridePolicy?: string;
    /**
     * Enum: `earliest`, `latest`. What to do when there is no initial offset in Kafka or if the current offset does not exist any more on the server. Default is earliest.
     */
    consumerAutoOffsetReset?: string;
    /**
     * Records are fetched in batches by the consumer, and if the first record batch in the first non-empty partition of the fetch is larger than this value, the record batch will still be returned to ensure that the consumer can make progress. As such, this is not a absolute maximum. Example: `52428800`.
     */
    consumerFetchMaxBytes?: number;
    /**
     * Enum: `readUncommitted`, `readCommitted`. Transaction read isolation level. read*uncommitted is the default, but read*committed can be used if consume-exactly-once behavior is desired.
     */
    consumerIsolationLevel?: string;
    /**
     * Records are fetched in batches by the consumer.If the first record batch in the first non-empty partition of the fetch is larger than this limit, the batch will still be returned to ensure that the consumer can make progress. Example: `1048576`.
     */
    consumerMaxPartitionFetchBytes?: number;
    /**
     * The maximum delay in milliseconds between invocations of poll() when using consumer group management (defaults to 300000).
     */
    consumerMaxPollIntervalMs?: number;
    /**
     * The maximum number of records returned in a single call to poll() (defaults to 500).
     */
    consumerMaxPollRecords?: number;
    /**
     * The interval at which to try committing offsets for tasks (defaults to 60000).
     */
    offsetFlushIntervalMs?: number;
    /**
     * Maximum number of milliseconds to wait for records to flush and partition offset data to be committed to offset storage before cancelling the process and restoring the offset data to be committed in a future attempt (defaults to 5000).
     */
    offsetFlushTimeoutMs?: number;
    /**
     * This setting gives the upper bound of the batch size to be sent. If there are fewer than this many bytes accumulated for this partition, the producer will `linger` for the linger.ms time waiting for more records to show up. A batch size of zero will disable batching entirely (defaults to 16384).
     */
    producerBatchSize?: number;
    /**
     * The total bytes of memory the producer can use to buffer records waiting to be sent to the broker (defaults to 33554432).
     */
    producerBufferMemory?: number;
    /**
     * Enum: `gzip`, `snappy`, `lz4`, `zstd`, `none`. Specify the default compression type for producers. This configuration accepts the standard compression codecs (`gzip`, `snappy`, `lz4`, `zstd`). It additionally accepts `none` which is the default and equivalent to no compression.
     */
    producerCompressionType?: string;
    /**
     * This setting gives the upper bound on the delay for batching: once there is batch.size worth of records for a partition it will be sent immediately regardless of this setting, however if there are fewer than this many bytes accumulated for this partition the producer will `linger` for the specified time waiting for more records to show up. Defaults to 0.
     */
    producerLingerMs?: number;
    /**
     * This setting will limit the number of record batches the producer will send in a single request to avoid sending huge requests. Example: `1048576`.
     */
    producerMaxRequestSize?: number;
    /**
     * The maximum delay that is scheduled in order to wait for the return of one or more departed workers before rebalancing and reassigning their connectors and tasks to the group. During this period the connectors and tasks of the departed workers remain unassigned. Defaults to 5 minutes.
     */
    scheduledRebalanceMaxDelayMs?: number;
    /**
     * The timeout in milliseconds used to detect failures when using Kafka’s group management facilities (defaults to 10000).
     */
    sessionTimeoutMs?: number;
}

export interface KafkaConnectKafkaConnectUserConfigPrivateAccess {
    /**
     * Allow clients to connect to kafkaConnect with a DNS name that always resolves to the service's private IP addresses. Only available in certain network locations.
     */
    kafkaConnect?: boolean;
    /**
     * Allow clients to connect to prometheus with a DNS name that always resolves to the service's private IP addresses. Only available in certain network locations.
     */
    prometheus?: boolean;
}

export interface KafkaConnectKafkaConnectUserConfigPrivatelinkAccess {
    /**
     * Enable jolokia.
     */
    jolokia?: boolean;
    /**
     * Enable kafka_connect.
     */
    kafkaConnect?: boolean;
    /**
     * Enable prometheus.
     */
    prometheus?: boolean;
}

export interface KafkaConnectKafkaConnectUserConfigPublicAccess {
    /**
     * Allow clients to connect to kafkaConnect from the public internet for service nodes that are in a project VPC or another type of private network.
     */
    kafkaConnect?: boolean;
    /**
     * Allow clients to connect to prometheus from the public internet for service nodes that are in a project VPC or another type of private network.
     */
    prometheus?: boolean;
}

export interface KafkaConnectKafkaConnectUserConfigSecretProvider {
    /**
     * AWS config for Secret Provider
     */
    aws?: outputs.KafkaConnectKafkaConnectUserConfigSecretProviderAws;
    /**
     * Name of the secret provider. Used to reference secrets in connector config.
     */
    name: string;
    /**
     * Vault Config for Secret Provider
     */
    vault?: outputs.KafkaConnectKafkaConnectUserConfigSecretProviderVault;
}

export interface KafkaConnectKafkaConnectUserConfigSecretProviderAws {
    /**
     * Access key used to authenticate with aws.
     */
    accessKey?: string;
    /**
     * Enum: `credentials`. Auth method of the vault secret provider.
     */
    authMethod: string;
    /**
     * Region used to lookup secrets with AWS SecretManager.
     */
    region: string;
    /**
     * Secret key used to authenticate with aws.
     */
    secretKey?: string;
}

export interface KafkaConnectKafkaConnectUserConfigSecretProviderVault {
    /**
     * Address of the Vault server.
     */
    address: string;
    /**
     * Enum: `token`. Auth method of the vault secret provider.
     */
    authMethod: string;
    /**
     * Enum: `1`, `2`, and newer. KV Secrets Engine version of the Vault server instance.
     */
    engineVersion?: number;
    /**
     * Prefix path depth of the secrets Engine. Default is 1. If the secrets engine path has more than one segment it has to be increased to the number of segments.
     */
    prefixPathDepth?: number;
    /**
     * Token used to authenticate with vault and auth method `token`.
     */
    token?: string;
}

export interface KafkaConnectServiceIntegration {
    /**
     * Type of the service integration. The only supported value at the moment is `readReplica`
     */
    integrationType: string;
    /**
     * Name of the source service
     */
    sourceServiceName: string;
}

export interface KafkaConnectTag {
    /**
     * Service tag key
     */
    key: string;
    /**
     * Service tag value
     */
    value: string;
}

export interface KafkaConnectTechEmail {
    /**
     * An email address to contact for technical issues
     */
    email: string;
}

export interface KafkaConnectorTask {
    /**
     * The name of the related connector.
     */
    connector: string;
    /**
     * The task ID of the task.
     */
    task: number;
}

export interface KafkaKafka {
    /**
     * The Kafka client certificate.
     */
    accessCert: string;
    /**
     * The Kafka client certificate key.
     */
    accessKey: string;
    /**
     * The Kafka Connect URI.
     */
    connectUri: string;
    /**
     * The Kafka REST URI.
     */
    restUri: string;
    /**
     * The Schema Registry URI.
     */
    schemaRegistryUri: string;
    /**
     * Kafka server URIs.
     */
    uris: string[];
}

export interface KafkaKafkaUserConfig {
    /**
     * Additional Cloud Regions for Backup Replication.
     *
     * @deprecated This property is deprecated.
     */
    additionalBackupRegions?: string;
    /**
     * Allow access to read Kafka topic messages in the Aiven Console and REST API.
     */
    aivenKafkaTopicMessages?: boolean;
    /**
     * Serve the web frontend using a custom CNAME pointing to the Aiven DNS name. Example: `grafana.example.org`.
     */
    customDomain?: string;
    /**
     * Enable follower fetching
     */
    followerFetching?: outputs.KafkaKafkaUserConfigFollowerFetching;
    /**
     * Allow incoming connections from CIDR address block, e.g. `10.20.0.0/16`
     */
    ipFilterObjects?: outputs.KafkaKafkaUserConfigIpFilterObject[];
    /**
     * Allow incoming connections from CIDR address block, e.g. `10.20.0.0/16`.
     */
    ipFilterStrings?: string[];
    /**
     * Allow incoming connections from CIDR address block, e.g. `10.20.0.0/16`.
     *
     * @deprecated Deprecated. Use `ipFilterString` instead.
     */
    ipFilters?: string[];
    /**
     * Kafka broker configuration values
     */
    kafka?: outputs.KafkaKafkaUserConfigKafka;
    /**
     * Kafka authentication methods
     */
    kafkaAuthenticationMethods?: outputs.KafkaKafkaUserConfigKafkaAuthenticationMethods;
    /**
     * Enable Kafka Connect service. Default: `false`.
     */
    kafkaConnect?: boolean;
    /**
     * Kafka Connect configuration values
     */
    kafkaConnectConfig?: outputs.KafkaKafkaUserConfigKafkaConnectConfig;
    kafkaConnectSecretProviders?: outputs.KafkaKafkaUserConfigKafkaConnectSecretProvider[];
    /**
     * Enable Kafka-REST service. Default: `false`.
     */
    kafkaRest?: boolean;
    /**
     * Enable authorization in Kafka-REST service.
     */
    kafkaRestAuthorization?: boolean;
    /**
     * Kafka REST configuration
     */
    kafkaRestConfig?: outputs.KafkaKafkaUserConfigKafkaRestConfig;
    /**
     * Kafka SASL mechanisms
     */
    kafkaSaslMechanisms?: outputs.KafkaKafkaUserConfigKafkaSaslMechanisms;
    /**
     * Enum: `3.1`, `3.2`, `3.3`, `3.4`, `3.5`, `3.6`, `3.7`, `3.8`, and newer. Kafka major version.
     */
    kafkaVersion?: string;
    /**
     * Use Letsencrypt CA for Kafka SASL via Privatelink.
     */
    letsencryptSaslPrivatelink?: boolean;
    /**
     * Allow access to selected service ports from private networks
     */
    privateAccess?: outputs.KafkaKafkaUserConfigPrivateAccess;
    /**
     * Allow access to selected service components through Privatelink
     */
    privatelinkAccess?: outputs.KafkaKafkaUserConfigPrivatelinkAccess;
    /**
     * Allow access to selected service ports from the public Internet
     */
    publicAccess?: outputs.KafkaKafkaUserConfigPublicAccess;
    /**
     * Enable Schema-Registry service. Default: `false`.
     */
    schemaRegistry?: boolean;
    /**
     * Schema Registry configuration
     */
    schemaRegistryConfig?: outputs.KafkaKafkaUserConfigSchemaRegistryConfig;
    /**
     * Store logs for the service so that they are available in the HTTP API and console.
     */
    serviceLog?: boolean;
    /**
     * Use static public IP addresses.
     */
    staticIps?: boolean;
    /**
     * Tiered storage configuration
     */
    tieredStorage?: outputs.KafkaKafkaUserConfigTieredStorage;
}

export interface KafkaKafkaUserConfigFollowerFetching {
    /**
     * Whether to enable the follower fetching functionality.
     */
    enabled?: boolean;
}

export interface KafkaKafkaUserConfigIpFilterObject {
    /**
     * Description for IP filter list entry. Example: `Production service IP range`.
     */
    description?: string;
    /**
     * CIDR address block. Example: `10.20.0.0/16`.
     */
    network: string;
}

export interface KafkaKafkaUserConfigKafka {
    /**
     * Enable auto-creation of topics. (Default: true).
     */
    autoCreateTopicsEnable?: boolean;
    /**
     * Enum: `gzip`, `snappy`, `lz4`, `zstd`, `uncompressed`, `producer`. Specify the final compression type for a given topic. This configuration accepts the standard compression codecs (`gzip`, `snappy`, `lz4`, `zstd`). It additionally accepts `uncompressed` which is equivalent to no compression; and `producer` which means retain the original compression codec set by the producer.(Default: producer).
     */
    compressionType?: string;
    /**
     * Idle connections timeout: the server socket processor threads close the connections that idle for longer than this. (Default: 600000 ms (10 minutes)). Example: `540000`.
     */
    connectionsMaxIdleMs?: number;
    /**
     * Replication factor for auto-created topics (Default: 3).
     */
    defaultReplicationFactor?: number;
    /**
     * The amount of time, in milliseconds, the group coordinator will wait for more consumers to join a new group before performing the first rebalance. A longer delay means potentially fewer rebalances, but increases the time until processing begins. The default value for this is 3 seconds. During development and testing it might be desirable to set this to 0 in order to not delay test execution time. (Default: 3000 ms (3 seconds)). Example: `3000`.
     */
    groupInitialRebalanceDelayMs?: number;
    /**
     * The maximum allowed session timeout for registered consumers. Longer timeouts give consumers more time to process messages in between heartbeats at the cost of a longer time to detect failures. Default: 1800000 ms (30 minutes). Example: `1800000`.
     */
    groupMaxSessionTimeoutMs?: number;
    /**
     * The minimum allowed session timeout for registered consumers. Longer timeouts give consumers more time to process messages in between heartbeats at the cost of a longer time to detect failures. (Default: 6000 ms (6 seconds)). Example: `6000`.
     */
    groupMinSessionTimeoutMs?: number;
    /**
     * How long are delete records retained? (Default: 86400000 (1 day)). Example: `86400000`.
     */
    logCleanerDeleteRetentionMs?: number;
    /**
     * The maximum amount of time message will remain uncompacted. Only applicable for logs that are being compacted. (Default: 9223372036854775807 ms (Long.MAX_VALUE)).
     */
    logCleanerMaxCompactionLagMs?: number;
    /**
     * Controls log compactor frequency. Larger value means more frequent compactions but also more space wasted for logs. Consider setting log.cleaner.max.compaction.lag.ms to enforce compactions sooner, instead of setting a very high value for this option. (Default: 0.5). Example: `0.5`.
     */
    logCleanerMinCleanableRatio?: number;
    /**
     * The minimum time a message will remain uncompacted in the log. Only applicable for logs that are being compacted. (Default: 0 ms).
     */
    logCleanerMinCompactionLagMs?: number;
    /**
     * Enum: `delete`, `compact`, `compact,delete`. The default cleanup policy for segments beyond the retention window (Default: delete).
     */
    logCleanupPolicy?: string;
    /**
     * The number of messages accumulated on a log partition before messages are flushed to disk (Default: 9223372036854775807 (Long.MAX_VALUE)). Example: `9223372036854775807`.
     */
    logFlushIntervalMessages?: number;
    /**
     * The maximum time in ms that a message in any topic is kept in memory (page-cache) before flushed to disk. If not set, the value in log.flush.scheduler.interval.ms is used (Default: null).
     */
    logFlushIntervalMs?: number;
    /**
     * The interval with which Kafka adds an entry to the offset index (Default: 4096 bytes (4 kibibytes)). Example: `4096`.
     */
    logIndexIntervalBytes?: number;
    /**
     * The maximum size in bytes of the offset index (Default: 10485760 (10 mebibytes)). Example: `10485760`.
     */
    logIndexSizeMaxBytes?: number;
    /**
     * The maximum size of local log segments that can grow for a partition before it gets eligible for deletion. If set to -2, the value of log.retention.bytes is used. The effective value should always be less than or equal to log.retention.bytes value. (Default: -2).
     */
    logLocalRetentionBytes?: number;
    /**
     * The number of milliseconds to keep the local log segments before it gets eligible for deletion. If set to -2, the value of log.retention.ms is used. The effective value should always be less than or equal to log.retention.ms value. (Default: -2).
     */
    logLocalRetentionMs?: number;
    /**
     * This configuration controls whether down-conversion of message formats is enabled to satisfy consume requests. (Default: true).
     */
    logMessageDownconversionEnable?: boolean;
    /**
     * The maximum difference allowed between the timestamp when a broker receives a message and the timestamp specified in the message (Default: 9223372036854775807 (Long.MAX_VALUE)).
     */
    logMessageTimestampDifferenceMaxMs?: number;
    /**
     * Enum: `CreateTime`, `LogAppendTime`. Define whether the timestamp in the message is message create time or log append time. (Default: CreateTime).
     */
    logMessageTimestampType?: string;
    /**
     * Should pre allocate file when create new segment? (Default: false).
     */
    logPreallocate?: boolean;
    /**
     * The maximum size of the log before deleting messages (Default: -1).
     */
    logRetentionBytes?: number;
    /**
     * The number of hours to keep a log file before deleting it (Default: 168 hours (1 week)).
     */
    logRetentionHours?: number;
    /**
     * The number of milliseconds to keep a log file before deleting it (in milliseconds), If not set, the value in log.retention.minutes is used. If set to -1, no time limit is applied. (Default: null, log.retention.hours applies).
     */
    logRetentionMs?: number;
    /**
     * The maximum jitter to subtract from logRollTimeMillis (in milliseconds). If not set, the value in log.roll.jitter.hours is used (Default: null).
     */
    logRollJitterMs?: number;
    /**
     * The maximum time before a new log segment is rolled out (in milliseconds). (Default: null, log.roll.hours applies (Default: 168, 7 days)).
     */
    logRollMs?: number;
    /**
     * The maximum size of a single log file (Default: 1073741824 bytes (1 gibibyte)).
     */
    logSegmentBytes?: number;
    /**
     * The amount of time to wait before deleting a file from the filesystem (Default: 60000 ms (1 minute)). Example: `60000`.
     */
    logSegmentDeleteDelayMs?: number;
    /**
     * The maximum number of connections allowed from each ip address (Default: 2147483647).
     */
    maxConnectionsPerIp?: number;
    /**
     * The maximum number of incremental fetch sessions that the broker will maintain. (Default: 1000). Example: `1000`.
     */
    maxIncrementalFetchSessionCacheSlots?: number;
    /**
     * The maximum size of message that the server can receive. (Default: 1048588 bytes (1 mebibyte + 12 bytes)). Example: `1048588`.
     */
    messageMaxBytes?: number;
    /**
     * When a producer sets acks to `all` (or `-1`), min.insync.replicas specifies the minimum number of replicas that must acknowledge a write for the write to be considered successful. (Default: 1). Example: `1`.
     */
    minInsyncReplicas?: number;
    /**
     * Number of partitions for auto-created topics (Default: 1).
     */
    numPartitions?: number;
    /**
     * Log retention window in minutes for offsets topic (Default: 10080 minutes (7 days)). Example: `10080`.
     */
    offsetsRetentionMinutes?: number;
    /**
     * The purge interval (in number of requests) of the producer request purgatory (Default: 1000).
     */
    producerPurgatoryPurgeIntervalRequests?: number;
    /**
     * The number of bytes of messages to attempt to fetch for each partition . This is not an absolute maximum, if the first record batch in the first non-empty partition of the fetch is larger than this value, the record batch will still be returned to ensure that progress can be made. (Default: 1048576 bytes (1 mebibytes)).
     */
    replicaFetchMaxBytes?: number;
    /**
     * Maximum bytes expected for the entire fetch response. Records are fetched in batches, and if the first record batch in the first non-empty partition of the fetch is larger than this value, the record batch will still be returned to ensure that progress can be made. As such, this is not an absolute maximum. (Default: 10485760 bytes (10 mebibytes)).
     */
    replicaFetchResponseMaxBytes?: number;
    /**
     * The (optional) comma-delimited setting for the broker to use to verify that the JWT was issued for one of the expected audiences. (Default: null).
     */
    saslOauthbearerExpectedAudience?: string;
    /**
     * Optional setting for the broker to use to verify that the JWT was created by the expected issuer.(Default: null).
     */
    saslOauthbearerExpectedIssuer?: string;
    /**
     * OIDC JWKS endpoint URL. By setting this the SASL SSL OAuth2/OIDC authentication is enabled. See also other options for SASL OAuth2/OIDC. (Default: null).
     */
    saslOauthbearerJwksEndpointUrl?: string;
    /**
     * Name of the scope from which to extract the subject claim from the JWT.(Default: sub).
     */
    saslOauthbearerSubClaimName?: string;
    /**
     * The maximum number of bytes in a socket request (Default: 104857600 bytes).
     */
    socketRequestMaxBytes?: number;
    /**
     * Enable verification that checks that the partition has been added to the transaction before writing transactional records to the partition. (Default: true).
     */
    transactionPartitionVerificationEnable?: boolean;
    /**
     * The interval at which to remove transactions that have expired due to transactional.id.expiration.ms passing (Default: 3600000 ms (1 hour)). Example: `3600000`.
     */
    transactionRemoveExpiredTransactionCleanupIntervalMs?: number;
    /**
     * The transaction topic segment bytes should be kept relatively small in order to facilitate faster log compaction and cache loads (Default: 104857600 bytes (100 mebibytes)). Example: `104857600`.
     */
    transactionStateLogSegmentBytes?: number;
}

export interface KafkaKafkaUserConfigKafkaAuthenticationMethods {
    /**
     * Enable certificate/SSL authentication. Default: `true`.
     */
    certificate?: boolean;
    /**
     * Enable SASL authentication. Default: `false`.
     */
    sasl?: boolean;
}

export interface KafkaKafkaUserConfigKafkaConnectConfig {
    /**
     * Enum: `None`, `All`. Defines what client configurations can be overridden by the connector. Default is None.
     */
    connectorClientConfigOverridePolicy?: string;
    /**
     * Enum: `earliest`, `latest`. What to do when there is no initial offset in Kafka or if the current offset does not exist any more on the server. Default is earliest.
     */
    consumerAutoOffsetReset?: string;
    /**
     * Records are fetched in batches by the consumer, and if the first record batch in the first non-empty partition of the fetch is larger than this value, the record batch will still be returned to ensure that the consumer can make progress. As such, this is not a absolute maximum. Example: `52428800`.
     */
    consumerFetchMaxBytes?: number;
    /**
     * Enum: `readUncommitted`, `readCommitted`. Transaction read isolation level. read*uncommitted is the default, but read*committed can be used if consume-exactly-once behavior is desired.
     */
    consumerIsolationLevel?: string;
    /**
     * Records are fetched in batches by the consumer.If the first record batch in the first non-empty partition of the fetch is larger than this limit, the batch will still be returned to ensure that the consumer can make progress. Example: `1048576`.
     */
    consumerMaxPartitionFetchBytes?: number;
    /**
     * The maximum delay in milliseconds between invocations of poll() when using consumer group management (defaults to 300000).
     */
    consumerMaxPollIntervalMs?: number;
    /**
     * The maximum number of records returned in a single call to poll() (defaults to 500).
     */
    consumerMaxPollRecords?: number;
    /**
     * The interval at which to try committing offsets for tasks (defaults to 60000).
     */
    offsetFlushIntervalMs?: number;
    /**
     * Maximum number of milliseconds to wait for records to flush and partition offset data to be committed to offset storage before cancelling the process and restoring the offset data to be committed in a future attempt (defaults to 5000).
     */
    offsetFlushTimeoutMs?: number;
    /**
     * This setting gives the upper bound of the batch size to be sent. If there are fewer than this many bytes accumulated for this partition, the producer will `linger` for the linger.ms time waiting for more records to show up. A batch size of zero will disable batching entirely (defaults to 16384).
     */
    producerBatchSize?: number;
    /**
     * The total bytes of memory the producer can use to buffer records waiting to be sent to the broker (defaults to 33554432).
     */
    producerBufferMemory?: number;
    /**
     * Enum: `gzip`, `snappy`, `lz4`, `zstd`, `none`. Specify the default compression type for producers. This configuration accepts the standard compression codecs (`gzip`, `snappy`, `lz4`, `zstd`). It additionally accepts `none` which is the default and equivalent to no compression.
     */
    producerCompressionType?: string;
    /**
     * This setting gives the upper bound on the delay for batching: once there is batch.size worth of records for a partition it will be sent immediately regardless of this setting, however if there are fewer than this many bytes accumulated for this partition the producer will `linger` for the specified time waiting for more records to show up. Defaults to 0.
     */
    producerLingerMs?: number;
    /**
     * This setting will limit the number of record batches the producer will send in a single request to avoid sending huge requests. Example: `1048576`.
     */
    producerMaxRequestSize?: number;
    /**
     * The maximum delay that is scheduled in order to wait for the return of one or more departed workers before rebalancing and reassigning their connectors and tasks to the group. During this period the connectors and tasks of the departed workers remain unassigned. Defaults to 5 minutes.
     */
    scheduledRebalanceMaxDelayMs?: number;
    /**
     * The timeout in milliseconds used to detect failures when using Kafka’s group management facilities (defaults to 10000).
     */
    sessionTimeoutMs?: number;
}

export interface KafkaKafkaUserConfigKafkaConnectSecretProvider {
    /**
     * AWS config for Secret Provider
     */
    aws?: outputs.KafkaKafkaUserConfigKafkaConnectSecretProviderAws;
    /**
     * Name of the secret provider. Used to reference secrets in connector config.
     */
    name: string;
    /**
     * Vault Config for Secret Provider
     */
    vault?: outputs.KafkaKafkaUserConfigKafkaConnectSecretProviderVault;
}

export interface KafkaKafkaUserConfigKafkaConnectSecretProviderAws {
    /**
     * Access key used to authenticate with aws.
     */
    accessKey?: string;
    /**
     * Enum: `credentials`. Auth method of the vault secret provider.
     */
    authMethod: string;
    /**
     * Region used to lookup secrets with AWS SecretManager.
     */
    region: string;
    /**
     * Secret key used to authenticate with aws.
     */
    secretKey?: string;
}

export interface KafkaKafkaUserConfigKafkaConnectSecretProviderVault {
    /**
     * Address of the Vault server.
     */
    address: string;
    /**
     * Enum: `token`. Auth method of the vault secret provider.
     */
    authMethod: string;
    /**
     * Enum: `1`, `2`, and newer. KV Secrets Engine version of the Vault server instance.
     */
    engineVersion?: number;
    /**
     * Prefix path depth of the secrets Engine. Default is 1. If the secrets engine path has more than one segment it has to be increased to the number of segments.
     */
    prefixPathDepth?: number;
    /**
     * Token used to authenticate with vault and auth method `token`.
     */
    token?: string;
}

export interface KafkaKafkaUserConfigKafkaRestConfig {
    /**
     * If true the consumer's offset will be periodically committed to Kafka in the background. Default: `true`.
     */
    consumerEnableAutoCommit?: boolean;
    /**
     * Maximum number of bytes in unencoded message keys and values by a single request. Default: `67108864`.
     */
    consumerRequestMaxBytes?: number;
    /**
     * Enum: `1000`, `15000`, `30000`. The maximum total time to wait for messages for a request if the maximum number of messages has not yet been reached. Default: `1000`.
     */
    consumerRequestTimeoutMs?: number;
    /**
     * Enum: `topicName`, `recordName`, `topicRecordName`. Name strategy to use when selecting subject for storing schemas. Default: `topicName`.
     */
    nameStrategy?: string;
    /**
     * If true, validate that given schema is registered under expected subject name by the used name strategy when producing messages. Default: `true`.
     */
    nameStrategyValidation?: boolean;
    /**
     * Enum: `all`, `-1`, `0`, `1`. The number of acknowledgments the producer requires the leader to have received before considering a request complete. If set to `all` or `-1`, the leader will wait for the full set of in-sync replicas to acknowledge the record. Default: `1`.
     */
    producerAcks?: string;
    /**
     * Enum: `gzip`, `snappy`, `lz4`, `zstd`, `none`. Specify the default compression type for producers. This configuration accepts the standard compression codecs (`gzip`, `snappy`, `lz4`, `zstd`). It additionally accepts `none` which is the default and equivalent to no compression.
     */
    producerCompressionType?: string;
    /**
     * Wait for up to the given delay to allow batching records together. Default: `0`.
     */
    producerLingerMs?: number;
    /**
     * The maximum size of a request in bytes. Note that Kafka broker can also cap the record batch size. Default: `1048576`.
     */
    producerMaxRequestSize?: number;
    /**
     * Maximum number of SimpleConsumers that can be instantiated per broker. Default: `25`.
     */
    simpleconsumerPoolSizeMax?: number;
}

export interface KafkaKafkaUserConfigKafkaSaslMechanisms {
    /**
     * Enable PLAIN mechanism. Default: `true`.
     */
    plain?: boolean;
    /**
     * Enable SCRAM-SHA-256 mechanism. Default: `true`.
     */
    scramSha256?: boolean;
    /**
     * Enable SCRAM-SHA-512 mechanism. Default: `true`.
     */
    scramSha512?: boolean;
}

export interface KafkaKafkaUserConfigPrivateAccess {
    /**
     * Allow clients to connect to kafka with a DNS name that always resolves to the service's private IP addresses. Only available in certain network locations.
     */
    kafka?: boolean;
    /**
     * Allow clients to connect to kafkaConnect with a DNS name that always resolves to the service's private IP addresses. Only available in certain network locations.
     */
    kafkaConnect?: boolean;
    /**
     * Allow clients to connect to kafkaRest with a DNS name that always resolves to the service's private IP addresses. Only available in certain network locations.
     */
    kafkaRest?: boolean;
    /**
     * Allow clients to connect to prometheus with a DNS name that always resolves to the service's private IP addresses. Only available in certain network locations.
     */
    prometheus?: boolean;
    /**
     * Allow clients to connect to schemaRegistry with a DNS name that always resolves to the service's private IP addresses. Only available in certain network locations.
     */
    schemaRegistry?: boolean;
}

export interface KafkaKafkaUserConfigPrivatelinkAccess {
    /**
     * Enable jolokia.
     */
    jolokia?: boolean;
    /**
     * Enable kafka.
     */
    kafka?: boolean;
    /**
     * Enable kafka_connect.
     */
    kafkaConnect?: boolean;
    /**
     * Enable kafka_rest.
     */
    kafkaRest?: boolean;
    /**
     * Enable prometheus.
     */
    prometheus?: boolean;
    /**
     * Enable schema_registry.
     */
    schemaRegistry?: boolean;
}

export interface KafkaKafkaUserConfigPublicAccess {
    /**
     * Allow clients to connect to kafka from the public internet for service nodes that are in a project VPC or another type of private network.
     */
    kafka?: boolean;
    /**
     * Allow clients to connect to kafkaConnect from the public internet for service nodes that are in a project VPC or another type of private network.
     */
    kafkaConnect?: boolean;
    /**
     * Allow clients to connect to kafkaRest from the public internet for service nodes that are in a project VPC or another type of private network.
     */
    kafkaRest?: boolean;
    /**
     * Allow clients to connect to prometheus from the public internet for service nodes that are in a project VPC or another type of private network.
     */
    prometheus?: boolean;
    /**
     * Allow clients to connect to schemaRegistry from the public internet for service nodes that are in a project VPC or another type of private network.
     */
    schemaRegistry?: boolean;
}

export interface KafkaKafkaUserConfigSchemaRegistryConfig {
    /**
     * If true, Karapace / Schema Registry on the service nodes can participate in leader election. It might be needed to disable this when the schemas topic is replicated to a secondary cluster and Karapace / Schema Registry there must not participate in leader election. Defaults to `true`.
     */
    leaderEligibility?: boolean;
    /**
     * If enabled, kafka errors which can be retried or custom errors specified for the service will not be raised, instead, a warning log is emitted. This will denoise issue tracking systems, i.e. sentry. Defaults to `true`.
     */
    retriableErrorsSilenced?: boolean;
    /**
     * If enabled, causes the Karapace schema-registry service to shutdown when there are invalid schema records in the `_schemas` topic. Defaults to `false`.
     */
    schemaReaderStrictMode?: boolean;
    /**
     * The durable single partition topic that acts as the durable log for the data. This topic must be compacted to avoid losing data due to retention policy. Please note that changing this configuration in an existing Schema Registry / Karapace setup leads to previous schemas being inaccessible, data encoded with them potentially unreadable and schema ID sequence put out of order. It's only possible to do the switch while Schema Registry / Karapace is disabled. Defaults to `_schemas`.
     */
    topicName?: string;
}

export interface KafkaKafkaUserConfigTieredStorage {
    /**
     * Whether to enable the tiered storage functionality.
     */
    enabled?: boolean;
    /**
     * Local cache configuration
     *
     * @deprecated This property is deprecated.
     */
    localCache?: outputs.KafkaKafkaUserConfigTieredStorageLocalCache;
}

export interface KafkaKafkaUserConfigTieredStorageLocalCache {
    /**
     * Local cache size in bytes. Example: `1073741824`.
     *
     * @deprecated This property is deprecated.
     */
    size?: number;
}

export interface KafkaMirrorMakerComponent {
    /**
     * Service component name
     */
    component: string;
    /**
     * Connection info for connecting to the service component. This is a combination of host and port.
     */
    connectionUri: string;
    /**
     * Host name for connecting to the service component
     */
    host: string;
    /**
     * Kafka authentication method. This is a value specific to the 'kafka' service component
     */
    kafkaAuthenticationMethod: string;
    /**
     * Port number for connecting to the service component
     */
    port: number;
    /**
     * Network access route
     */
    route: string;
    /**
     * Whether the endpoint is encrypted or accepts plaintext. By default endpoints are always encrypted and this property is only included for service components they may disable encryption
     */
    ssl: boolean;
    /**
     * DNS usage name
     */
    usage: string;
}

export interface KafkaMirrorMakerKafkaMirrormakerUserConfig {
    /**
     * Additional Cloud Regions for Backup Replication.
     *
     * @deprecated This property is deprecated.
     */
    additionalBackupRegions?: string;
    /**
     * Allow incoming connections from CIDR address block, e.g. `10.20.0.0/16`
     */
    ipFilterObjects?: outputs.KafkaMirrorMakerKafkaMirrormakerUserConfigIpFilterObject[];
    /**
     * Allow incoming connections from CIDR address block, e.g. `10.20.0.0/16`.
     */
    ipFilterStrings?: string[];
    /**
     * Allow incoming connections from CIDR address block, e.g. `10.20.0.0/16`.
     *
     * @deprecated Deprecated. Use `ipFilterString` instead.
     */
    ipFilters?: string[];
    /**
     * Kafka MirrorMaker configuration values
     */
    kafkaMirrormaker?: outputs.KafkaMirrorMakerKafkaMirrormakerUserConfigKafkaMirrormaker;
    /**
     * Store logs for the service so that they are available in the HTTP API and console.
     */
    serviceLog?: boolean;
    /**
     * Use static public IP addresses.
     */
    staticIps?: boolean;
}

export interface KafkaMirrorMakerKafkaMirrormakerUserConfigIpFilterObject {
    /**
     * Description for IP filter list entry. Example: `Production service IP range`.
     */
    description?: string;
    /**
     * CIDR address block. Example: `10.20.0.0/16`.
     */
    network: string;
}

export interface KafkaMirrorMakerKafkaMirrormakerUserConfigKafkaMirrormaker {
    /**
     * Timeout for administrative tasks, e.g. detecting new topics, loading of consumer group and offsets. Defaults to 60000 milliseconds (1 minute).
     */
    adminTimeoutMs?: number;
    /**
     * Whether to emit consumer group offset checkpoints to target cluster periodically (default: true).
     */
    emitCheckpointsEnabled?: boolean;
    /**
     * Frequency at which consumer group offset checkpoints are emitted (default: 60, every minute). Example: `60`.
     */
    emitCheckpointsIntervalSeconds?: number;
    /**
     * Consumer groups to replicate. Supports comma-separated group IDs and regexes. Example: `.*`.
     */
    groups?: string;
    /**
     * Exclude groups. Supports comma-separated group IDs and regexes. Excludes take precedence over includes. Example: `console-consumer-.*,connect-.*,__.*`.
     */
    groupsExclude?: string;
    /**
     * How out-of-sync a remote partition can be before it is resynced. Example: `100`.
     */
    offsetLagMax?: number;
    /**
     * Whether to periodically check for new consumer groups. Defaults to `true`.
     */
    refreshGroupsEnabled?: boolean;
    /**
     * Frequency of consumer group refresh in seconds. Defaults to 600 seconds (10 minutes).
     */
    refreshGroupsIntervalSeconds?: number;
    /**
     * Whether to periodically check for new topics and partitions. Defaults to `true`.
     */
    refreshTopicsEnabled?: boolean;
    /**
     * Frequency of topic and partitions refresh in seconds. Defaults to 600 seconds (10 minutes).
     */
    refreshTopicsIntervalSeconds?: number;
    /**
     * Whether to periodically write the translated offsets of replicated consumer groups (in the source cluster) to _*consumer*offsets topic in target cluster, as long as no active consumers in that group are connected to the target cluster.
     */
    syncGroupOffsetsEnabled?: boolean;
    /**
     * Frequency at which consumer group offsets are synced (default: 60, every minute). Example: `60`.
     */
    syncGroupOffsetsIntervalSeconds?: number;
    /**
     * Whether to periodically configure remote topics to match their corresponding upstream topics.
     */
    syncTopicConfigsEnabled?: boolean;
    /**
     * `tasks.max` is set to this multiplied by the number of CPUs in the service. Default: `1`.
     */
    tasksMaxPerCpu?: number;
}

export interface KafkaMirrorMakerServiceIntegration {
    /**
     * Type of the service integration. The only supported value at the moment is `readReplica`
     */
    integrationType: string;
    /**
     * Name of the source service
     */
    sourceServiceName: string;
}

export interface KafkaMirrorMakerTag {
    /**
     * Service tag key
     */
    key: string;
    /**
     * Service tag value
     */
    value: string;
}

export interface KafkaMirrorMakerTechEmail {
    /**
     * An email address to contact for technical issues
     */
    email: string;
}

export interface KafkaServiceIntegration {
    /**
     * Type of the service integration. The only supported value at the moment is `readReplica`
     */
    integrationType: string;
    /**
     * Name of the source service
     */
    sourceServiceName: string;
}

export interface KafkaTag {
    /**
     * Service tag key
     */
    key: string;
    /**
     * Service tag value
     */
    value: string;
}

export interface KafkaTechEmail {
    /**
     * An email address to contact for technical issues
     */
    email: string;
}

export interface KafkaTopicConfig {
    /**
     * cleanup.policy value
     */
    cleanupPolicy?: string;
    /**
     * compression.type value
     */
    compressionType?: string;
    /**
     * delete.retention.ms value
     */
    deleteRetentionMs?: string;
    /**
     * file.delete.delay.ms value
     */
    fileDeleteDelayMs?: string;
    /**
     * flush.messages value
     */
    flushMessages?: string;
    /**
     * flush.ms value
     */
    flushMs?: string;
    /**
     * index.interval.bytes value
     */
    indexIntervalBytes?: string;
    /**
     * local.retention.bytes value
     */
    localRetentionBytes?: string;
    /**
     * local.retention.ms value
     */
    localRetentionMs?: string;
    /**
     * max.compaction.lag.ms value
     */
    maxCompactionLagMs?: string;
    /**
     * max.message.bytes value
     */
    maxMessageBytes?: string;
    /**
     * message.downconversion.enable value
     */
    messageDownconversionEnable?: boolean;
    /**
     * message.format.version value
     */
    messageFormatVersion?: string;
    /**
     * message.timestamp.difference.max.ms value
     */
    messageTimestampDifferenceMaxMs?: string;
    /**
     * message.timestamp.type value
     */
    messageTimestampType?: string;
    /**
     * min.cleanable.dirty.ratio value
     */
    minCleanableDirtyRatio?: number;
    /**
     * min.compaction.lag.ms value
     */
    minCompactionLagMs?: string;
    /**
     * min.insync.replicas value
     */
    minInsyncReplicas?: string;
    /**
     * preallocate value
     */
    preallocate?: boolean;
    /**
     * remote.storage.enable value
     */
    remoteStorageEnable?: boolean;
    /**
     * retention.bytes value
     */
    retentionBytes?: string;
    /**
     * retention.ms value
     */
    retentionMs?: string;
    /**
     * segment.bytes value
     */
    segmentBytes?: string;
    /**
     * segment.index.bytes value
     */
    segmentIndexBytes?: string;
    /**
     * segment.jitter.ms value
     */
    segmentJitterMs?: string;
    /**
     * segment.ms value
     */
    segmentMs?: string;
    /**
     * unclean.leader.election.enable value; This field is deprecated and no longer functional.
     *
     * @deprecated This field is deprecated and no longer functional.
     */
    uncleanLeaderElectionEnable?: boolean;
}

export interface KafkaTopicTag {
    /**
     * Tag key. Maximum length: `64`.
     */
    key: string;
    /**
     * Tag value. Maximum length: `256`.
     */
    value?: string;
}

export interface M3AggregatorComponent {
    /**
     * Service component name
     */
    component: string;
    /**
     * Connection info for connecting to the service component. This is a combination of host and port.
     */
    connectionUri: string;
    /**
     * Host name for connecting to the service component
     */
    host: string;
    /**
     * Kafka authentication method. This is a value specific to the 'kafka' service component
     */
    kafkaAuthenticationMethod: string;
    /**
     * Port number for connecting to the service component
     */
    port: number;
    /**
     * Network access route
     */
    route: string;
    /**
     * Whether the endpoint is encrypted or accepts plaintext. By default endpoints are always encrypted and this property is only included for service components they may disable encryption
     */
    ssl: boolean;
    /**
     * DNS usage name
     */
    usage: string;
}

export interface M3AggregatorM3aggregator {
    /**
     * M3 Aggregator HTTP URI.
     */
    aggregatorHttpUri: string;
    /**
     * M3 Aggregator server URIs.
     */
    uris: string[];
}

export interface M3AggregatorM3aggregatorUserConfig {
    /**
     * Serve the web frontend using a custom CNAME pointing to the Aiven DNS name. Example: `grafana.example.org`.
     */
    customDomain?: string;
    /**
     * Allow incoming connections from CIDR address block, e.g. `10.20.0.0/16`
     */
    ipFilterObjects?: outputs.M3AggregatorM3aggregatorUserConfigIpFilterObject[];
    /**
     * Allow incoming connections from CIDR address block, e.g. `10.20.0.0/16`.
     */
    ipFilterStrings?: string[];
    /**
     * Allow incoming connections from CIDR address block, e.g. `10.20.0.0/16`.
     *
     * @deprecated Deprecated. Use `ipFilterString` instead.
     */
    ipFilters?: string[];
    /**
     * Enum: `1.1`, `1.2`, `1.5`, and newer. M3 major version (deprecated, use m3aggregator_version).
     */
    m3Version?: string;
    /**
     * Enum: `1.1`, `1.2`, `1.5`, and newer. M3 major version (the minimum compatible version).
     */
    m3aggregatorVersion?: string;
    /**
     * Store logs for the service so that they are available in the HTTP API and console.
     */
    serviceLog?: boolean;
    /**
     * Use static public IP addresses.
     */
    staticIps?: boolean;
}

export interface M3AggregatorM3aggregatorUserConfigIpFilterObject {
    /**
     * Description for IP filter list entry. Example: `Production service IP range`.
     */
    description?: string;
    /**
     * CIDR address block. Example: `10.20.0.0/16`.
     */
    network: string;
}

export interface M3AggregatorServiceIntegration {
    /**
     * Type of the service integration. The only supported value at the moment is `readReplica`
     */
    integrationType: string;
    /**
     * Name of the source service
     */
    sourceServiceName: string;
}

export interface M3AggregatorTag {
    /**
     * Service tag key
     */
    key: string;
    /**
     * Service tag value
     */
    value: string;
}

export interface M3AggregatorTechEmail {
    /**
     * An email address to contact for technical issues
     */
    email: string;
}

export interface M3DbComponent {
    /**
     * Service component name
     */
    component: string;
    /**
     * Connection info for connecting to the service component. This is a combination of host and port.
     */
    connectionUri: string;
    /**
     * Host name for connecting to the service component
     */
    host: string;
    /**
     * Kafka authentication method. This is a value specific to the 'kafka' service component
     */
    kafkaAuthenticationMethod: string;
    /**
     * Port number for connecting to the service component
     */
    port: number;
    /**
     * Network access route
     */
    route: string;
    /**
     * Whether the endpoint is encrypted or accepts plaintext. By default endpoints are always encrypted and this property is only included for service components they may disable encryption
     */
    ssl: boolean;
    /**
     * DNS usage name
     */
    usage: string;
}

export interface M3DbM3db {
    /**
     * M3DB cluster URI.
     */
    httpClusterUri: string;
    /**
     * M3DB node URI.
     */
    httpNodeUri: string;
    /**
     * InfluxDB URI.
     */
    influxdbUri: string;
    /**
     * Prometheus remote read URI.
     */
    prometheusRemoteReadUri: string;
    /**
     * Prometheus remote write URI.
     */
    prometheusRemoteWriteUri: string;
    /**
     * M3DB server URIs.
     */
    uris: string[];
}

export interface M3DbM3dbUserConfig {
    /**
     * Additional Cloud Regions for Backup Replication.
     */
    additionalBackupRegions?: string;
    /**
     * Serve the web frontend using a custom CNAME pointing to the Aiven DNS name. Example: `grafana.example.org`.
     */
    customDomain?: string;
    /**
     * Allow incoming connections from CIDR address block, e.g. `10.20.0.0/16`
     */
    ipFilterObjects?: outputs.M3DbM3dbUserConfigIpFilterObject[];
    /**
     * Allow incoming connections from CIDR address block, e.g. `10.20.0.0/16`.
     */
    ipFilterStrings?: string[];
    /**
     * Allow incoming connections from CIDR address block, e.g. `10.20.0.0/16`.
     *
     * @deprecated Deprecated. Use `ipFilterString` instead.
     */
    ipFilters?: string[];
    /**
     * M3 limits
     */
    limits?: outputs.M3DbM3dbUserConfigLimits;
    /**
     * M3 specific configuration options
     */
    m3?: outputs.M3DbM3dbUserConfigM3;
    /**
     * Enum: `1.1`, `1.2`, `1.5`, and newer. M3 major version (deprecated, use m3db_version).
     */
    m3Version?: string;
    /**
     * Enables access to Graphite Carbon plaintext metrics ingestion. It can be enabled only for services inside VPCs. The metrics are written to aggregated namespaces only.
     */
    m3coordinatorEnableGraphiteCarbonIngest?: boolean;
    /**
     * Enum: `1.1`, `1.2`, `1.5`, and newer. M3 major version (the minimum compatible version).
     */
    m3dbVersion?: string;
    /**
     * List of M3 namespaces
     */
    namespaces?: outputs.M3DbM3dbUserConfigNamespace[];
    /**
     * Allow access to selected service ports from private networks
     */
    privateAccess?: outputs.M3DbM3dbUserConfigPrivateAccess;
    /**
     * Name of another project to fork a service from. This has effect only when a new service is being created. Example: `anotherprojectname`.
     */
    projectToForkFrom?: string;
    /**
     * Allow access to selected service ports from the public Internet
     */
    publicAccess?: outputs.M3DbM3dbUserConfigPublicAccess;
    /**
     * M3 rules
     */
    rules?: outputs.M3DbM3dbUserConfigRules;
    /**
     * Store logs for the service so that they are available in the HTTP API and console.
     */
    serviceLog?: boolean;
    /**
     * Name of another service to fork from. This has effect only when a new service is being created. Example: `anotherservicename`.
     */
    serviceToForkFrom?: string;
    /**
     * Use static public IP addresses.
     */
    staticIps?: boolean;
}

export interface M3DbM3dbUserConfigIpFilterObject {
    /**
     * Description for IP filter list entry. Example: `Production service IP range`.
     */
    description?: string;
    /**
     * CIDR address block. Example: `10.20.0.0/16`.
     */
    network: string;
}

export interface M3DbM3dbUserConfigLimits {
    /**
     * The maximum number of blocks that can be read in a given lookback period. Example: `20000`.
     */
    maxRecentlyQueriedSeriesBlocks?: number;
    /**
     * The maximum number of disk bytes that can be read in a given lookback period. Example: `104857600`.
     */
    maxRecentlyQueriedSeriesDiskBytesRead?: number;
    /**
     * The lookback period for `maxRecentlyQueriedSeriesBlocks` and `maxRecentlyQueriedSeriesDiskBytesRead`. Example: `15s`.
     */
    maxRecentlyQueriedSeriesLookback?: string;
    /**
     * The maximum number of docs fetched in single query. Example: `100000`.
     */
    queryDocs?: number;
    /**
     * When query limits are exceeded, whether to return error or return partial results.
     */
    queryRequireExhaustive?: boolean;
    /**
     * The maximum number of series fetched in single query. Example: `100000`.
     */
    querySeries?: number;
}

export interface M3DbM3dbUserConfigM3 {
    /**
     * M3 Tag Options
     */
    tagOptions?: outputs.M3DbM3dbUserConfigM3TagOptions;
}

export interface M3DbM3dbUserConfigM3TagOptions {
    /**
     * Allows for duplicate tags to appear on series (not allowed by default).
     */
    allowTagNameDuplicates?: boolean;
    /**
     * Allows for empty tags to appear on series (not allowed by default).
     */
    allowTagValueEmpty?: boolean;
}

export interface M3DbM3dbUserConfigNamespace {
    /**
     * The name of the namespace. Example: `default`.
     */
    name: string;
    /**
     * Namespace options
     */
    options?: outputs.M3DbM3dbUserConfigNamespaceOptions;
    /**
     * The resolution for an aggregated namespace. Example: `30s`.
     */
    resolution?: string;
    /**
     * Enum: `aggregated`, `unaggregated`. The type of aggregation (aggregated/unaggregated).
     */
    type: string;
}

export interface M3DbM3dbUserConfigNamespaceOptions {
    /**
     * Retention options
     */
    retentionOptions: outputs.M3DbM3dbUserConfigNamespaceOptionsRetentionOptions;
    /**
     * Controls whether M3DB will create snapshot files for this namespace.
     */
    snapshotEnabled?: boolean;
    /**
     * Controls whether M3DB will include writes to this namespace in the commitlog.
     */
    writesToCommitlog?: boolean;
}

export interface M3DbM3dbUserConfigNamespaceOptionsRetentionOptions {
    /**
     * Controls how long we wait before expiring stale data. Example: `5m`.
     */
    blockDataExpiryDuration?: string;
    /**
     * Controls how long to keep a block in memory before flushing to a fileset on disk. Example: `2h`.
     */
    blocksizeDuration?: string;
    /**
     * Controls how far into the future writes to the namespace will be accepted. Example: `10m`.
     */
    bufferFutureDuration?: string;
    /**
     * Controls how far into the past writes to the namespace will be accepted. Example: `10m`.
     */
    bufferPastDuration?: string;
    /**
     * Controls the duration of time that M3DB will retain data for the namespace. Example: `48h`.
     */
    retentionPeriodDuration?: string;
}

export interface M3DbM3dbUserConfigPrivateAccess {
    /**
     * Allow clients to connect to m3coordinator with a DNS name that always resolves to the service's private IP addresses. Only available in certain network locations.
     */
    m3coordinator?: boolean;
}

export interface M3DbM3dbUserConfigPublicAccess {
    /**
     * Allow clients to connect to m3coordinator from the public internet for service nodes that are in a project VPC or another type of private network.
     */
    m3coordinator?: boolean;
}

export interface M3DbM3dbUserConfigRules {
    /**
     * List of M3 mapping rules
     */
    mappings?: outputs.M3DbM3dbUserConfigRulesMapping[];
}

export interface M3DbM3dbUserConfigRulesMapping {
    /**
     * List of aggregations to be applied.
     */
    aggregations?: string[];
    /**
     * Only store the derived metric (as specified in the roll-up rules), if any.
     */
    drop?: boolean;
    /**
     * Matching metric names with wildcards (using **name**:wildcard) or matching tags and their (optionally wildcarded) values. For value, ! can be used at start of value for negation, and multiple filters can be supplied using space as separator. Example: `__name__:disk_* host:important-42 mount:!*&#47;sda`.
     */
    filter: string;
    /**
     * The (optional) name of the rule. Example: `important disk metrics`.
     */
    name?: string;
    /**
     * This rule will be used to store the metrics in the given namespace(s). If a namespace is target of rules, the global default aggregation will be automatically disabled. Note that specifying filters that match no namespaces whatsoever will be returned as an error. Filter the namespace by glob (=wildcards).
     *
     * @deprecated Deprecated. Use `namespacesString` instead.
     */
    namespaces?: string[];
    /**
     * This rule will be used to store the metrics in the given namespace(s). If a namespace is target of rules, the global default aggregation will be automatically disabled. Note that specifying filters that match no namespaces whatsoever will be returned as an error. Filter the namespace by exact match of retention period and resolution
     */
    namespacesObjects?: outputs.M3DbM3dbUserConfigRulesMappingNamespacesObject[];
    /**
     * This rule will be used to store the metrics in the given namespace(s). If a namespace is target of rules, the global default aggregation will be automatically disabled. Note that specifying filters that match no namespaces whatsoever will be returned as an error. Filter the namespace by glob (=wildcards).
     */
    namespacesStrings?: string[];
    /**
     * List of tags to be appended to matching metrics
     */
    tags?: outputs.M3DbM3dbUserConfigRulesMappingTag[];
}

export interface M3DbM3dbUserConfigRulesMappingNamespacesObject {
    /**
     * The resolution for the matching namespace. Example: `30s`.
     */
    resolution: string;
    /**
     * The retention period of the matching namespace. Example: `48h`.
     */
    retention?: string;
}

export interface M3DbM3dbUserConfigRulesMappingTag {
    /**
     * Name of the tag. Example: `myTag`.
     */
    name: string;
    /**
     * Value of the tag. Example: `myValue`.
     */
    value: string;
}

export interface M3DbServiceIntegration {
    /**
     * Type of the service integration. The only supported value at the moment is `readReplica`
     */
    integrationType: string;
    /**
     * Name of the source service
     */
    sourceServiceName: string;
}

export interface M3DbTag {
    /**
     * Service tag key
     */
    key: string;
    /**
     * Service tag value
     */
    value: string;
}

export interface M3DbTechEmail {
    /**
     * An email address to contact for technical issues
     */
    email: string;
}

export interface MySqlComponent {
    /**
     * Service component name
     */
    component: string;
    /**
     * Connection info for connecting to the service component. This is a combination of host and port.
     */
    connectionUri: string;
    /**
     * Host name for connecting to the service component
     */
    host: string;
    /**
     * Kafka authentication method. This is a value specific to the 'kafka' service component
     */
    kafkaAuthenticationMethod: string;
    /**
     * Port number for connecting to the service component
     */
    port: number;
    /**
     * Network access route
     */
    route: string;
    /**
     * Whether the endpoint is encrypted or accepts plaintext. By default endpoints are always encrypted and this property is only included for service components they may disable encryption
     */
    ssl: boolean;
    /**
     * DNS usage name
     */
    usage: string;
}

export interface MySqlMysql {
    /**
     * MySQL connection parameters
     */
    params: outputs.MySqlMysqlParam[];
    /**
     * MySQL replica URI for services with a replica
     */
    replicaUri: string;
    /**
     * MySQL standby connection URIs
     */
    standbyUris: string[];
    /**
     * MySQL syncing connection URIs
     */
    syncingUris: string[];
    /**
     * MySQL master connection URIs
     */
    uris: string[];
}

export interface MySqlMysqlParam {
    /**
     * Primary MySQL database name
     */
    databaseName: string;
    /**
     * MySQL host IP or name
     */
    host: string;
    /**
     * MySQL admin user password
     */
    password: string;
    /**
     * MySQL port
     */
    port: number;
    /**
     * MySQL sslmode setting (currently always "require")
     */
    sslmode: string;
    /**
     * MySQL admin user name
     */
    user: string;
}

export interface MySqlMysqlUserConfig {
    /**
     * Additional Cloud Regions for Backup Replication.
     */
    additionalBackupRegions?: string;
    /**
     * Custom password for admin user. Defaults to random string. This must be set only when a new service is being created.
     */
    adminPassword?: string;
    /**
     * Custom username for admin user. This must be set only when a new service is being created. Example: `avnadmin`.
     */
    adminUsername?: string;
    /**
     * The hour of day (in UTC) when backup for the service is started. New backup is only started if previous backup has already completed. Example: `3`.
     */
    backupHour?: number;
    /**
     * The minute of an hour when backup for the service is started. New backup is only started if previous backup has already completed. Example: `30`.
     */
    backupMinute?: number;
    /**
     * The minimum amount of time in seconds to keep binlog entries before deletion. This may be extended for services that require binlog entries for longer than the default for example if using the MySQL Debezium Kafka connector. Example: `600`.
     */
    binlogRetentionPeriod?: number;
    /**
     * Allow incoming connections from CIDR address block, e.g. `10.20.0.0/16`
     */
    ipFilterObjects?: outputs.MySqlMysqlUserConfigIpFilterObject[];
    /**
     * Allow incoming connections from CIDR address block, e.g. `10.20.0.0/16`.
     */
    ipFilterStrings?: string[];
    /**
     * Allow incoming connections from CIDR address block, e.g. `10.20.0.0/16`.
     *
     * @deprecated Deprecated. Use `ipFilterString` instead.
     */
    ipFilters?: string[];
    /**
     * Migrate data from existing server
     */
    migration?: outputs.MySqlMysqlUserConfigMigration;
    /**
     * mysql.conf configuration values
     */
    mysql?: outputs.MySqlMysqlUserConfigMysql;
    /**
     * Enum: `8`, and newer. MySQL major version.
     */
    mysqlVersion?: string;
    /**
     * Allow access to selected service ports from private networks
     */
    privateAccess?: outputs.MySqlMysqlUserConfigPrivateAccess;
    /**
     * Allow access to selected service components through Privatelink
     */
    privatelinkAccess?: outputs.MySqlMysqlUserConfigPrivatelinkAccess;
    /**
     * Name of another project to fork a service from. This has effect only when a new service is being created. Example: `anotherprojectname`.
     */
    projectToForkFrom?: string;
    /**
     * Allow access to selected service ports from the public Internet
     */
    publicAccess?: outputs.MySqlMysqlUserConfigPublicAccess;
    /**
     * Recovery target time when forking a service. This has effect only when a new service is being created. Example: `2019-01-01 23:34:45`.
     */
    recoveryTargetTime?: string;
    /**
     * Store logs for the service so that they are available in the HTTP API and console.
     */
    serviceLog?: boolean;
    /**
     * Name of another service to fork from. This has effect only when a new service is being created. Example: `anotherservicename`.
     */
    serviceToForkFrom?: string;
    /**
     * Use static public IP addresses.
     */
    staticIps?: boolean;
}

export interface MySqlMysqlUserConfigIpFilterObject {
    /**
     * Description for IP filter list entry. Example: `Production service IP range`.
     */
    description?: string;
    /**
     * CIDR address block. Example: `10.20.0.0/16`.
     */
    network: string;
}

export interface MySqlMysqlUserConfigMigration {
    /**
     * Database name for bootstrapping the initial connection. Example: `defaultdb`.
     */
    dbname?: string;
    /**
     * Hostname or IP address of the server where to migrate data from. Example: `my.server.com`.
     */
    host: string;
    /**
     * Comma-separated list of databases, which should be ignored during migration (supported by MySQL and PostgreSQL only at the moment). Example: `db1,db2`.
     */
    ignoreDbs?: string;
    /**
     * Comma-separated list of database roles, which should be ignored during migration (supported by PostgreSQL only at the moment). Example: `role1,role2`.
     */
    ignoreRoles?: string;
    /**
     * Enum: `dump`, `replication`. The migration method to be used (currently supported only by Redis, Dragonfly, MySQL and PostgreSQL service types).
     */
    method?: string;
    /**
     * Password for authentication with the server where to migrate data from. Example: `jjKk45Nnd`.
     */
    password?: string;
    /**
     * Port number of the server where to migrate data from. Example: `1234`.
     */
    port: number;
    /**
     * The server where to migrate data from is secured with SSL. Default: `true`.
     */
    ssl?: boolean;
    /**
     * User name for authentication with the server where to migrate data from. Example: `myname`.
     */
    username?: string;
}

export interface MySqlMysqlUserConfigMysql {
    /**
     * The number of seconds that the mysqld server waits for a connect packet before responding with Bad handshake. Example: `10`.
     */
    connectTimeout?: number;
    /**
     * Default server time zone as an offset from UTC (from -12:00 to +12:00), a time zone name, or `SYSTEM` to use the MySQL server default. Example: `+03:00`.
     */
    defaultTimeZone?: string;
    /**
     * The maximum permitted result length in bytes for the GROUP_CONCAT() function. Example: `1024`.
     */
    groupConcatMaxLen?: number;
    /**
     * The time, in seconds, before cached statistics expire. Example: `86400`.
     */
    informationSchemaStatsExpiry?: number;
    /**
     * Maximum size for the InnoDB change buffer, as a percentage of the total size of the buffer pool. Default is 25. Example: `30`.
     */
    innodbChangeBufferMaxSize?: number;
    /**
     * Specifies whether flushing a page from the InnoDB buffer pool also flushes other dirty pages in the same extent (default is 1): 0 - dirty pages in the same extent are not flushed, 1 - flush contiguous dirty pages in the same extent, 2 - flush dirty pages in the same extent. Example: `0`.
     */
    innodbFlushNeighbors?: number;
    /**
     * Minimum length of words that are stored in an InnoDB FULLTEXT index. Changing this parameter will lead to a restart of the MySQL service. Example: `3`.
     */
    innodbFtMinTokenSize?: number;
    /**
     * This option is used to specify your own InnoDB FULLTEXT index stopword list for all InnoDB tables. Example: `db_name/table_name`.
     */
    innodbFtServerStopwordTable?: string;
    /**
     * The length of time in seconds an InnoDB transaction waits for a row lock before giving up. Default is 120. Example: `50`.
     */
    innodbLockWaitTimeout?: number;
    /**
     * The size in bytes of the buffer that InnoDB uses to write to the log files on disk. Example: `16777216`.
     */
    innodbLogBufferSize?: number;
    /**
     * The upper limit in bytes on the size of the temporary log files used during online DDL operations for InnoDB tables. Example: `134217728`.
     */
    innodbOnlineAlterLogMaxSize?: number;
    /**
     * When enabled, information about all deadlocks in InnoDB user transactions is recorded in the error log. Disabled by default.
     */
    innodbPrintAllDeadlocks?: boolean;
    /**
     * The number of I/O threads for read operations in InnoDB. Default is 4. Changing this parameter will lead to a restart of the MySQL service. Example: `10`.
     */
    innodbReadIoThreads?: number;
    /**
     * When enabled a transaction timeout causes InnoDB to abort and roll back the entire transaction. Changing this parameter will lead to a restart of the MySQL service.
     */
    innodbRollbackOnTimeout?: boolean;
    /**
     * Defines the maximum number of threads permitted inside of InnoDB. Default is 0 (infinite concurrency - no limit). Example: `10`.
     */
    innodbThreadConcurrency?: number;
    /**
     * The number of I/O threads for write operations in InnoDB. Default is 4. Changing this parameter will lead to a restart of the MySQL service. Example: `10`.
     */
    innodbWriteIoThreads?: number;
    /**
     * The number of seconds the server waits for activity on an interactive connection before closing it. Example: `3600`.
     */
    interactiveTimeout?: number;
    /**
     * Enum: `TempTable`, `MEMORY`. The storage engine for in-memory internal temporary tables.
     */
    internalTmpMemStorageEngine?: string;
    /**
     * The slow*query*logs work as SQL statements that take more than long*query*time seconds to execute. Default is 10s. Example: `10`.
     */
    longQueryTime?: number;
    /**
     * Size of the largest message in bytes that can be received by the server. Default is 67108864 (64M). Example: `67108864`.
     */
    maxAllowedPacket?: number;
    /**
     * Limits the size of internal in-memory tables. Also set tmp*table*size. Default is 16777216 (16M). Example: `16777216`.
     */
    maxHeapTableSize?: number;
    /**
     * Start sizes of connection buffer and result buffer. Default is 16384 (16K). Changing this parameter will lead to a restart of the MySQL service. Example: `16384`.
     */
    netBufferLength?: number;
    /**
     * The number of seconds to wait for more data from a connection before aborting the read. Example: `30`.
     */
    netReadTimeout?: number;
    /**
     * The number of seconds to wait for a block to be written to a connection before aborting the write. Example: `30`.
     */
    netWriteTimeout?: number;
    /**
     * Slow query log enables capturing of slow queries. Setting slow*query*log to false also truncates the mysql.slow_log table. Default is off.
     */
    slowQueryLog?: boolean;
    /**
     * Sort buffer size in bytes for ORDER BY optimization. Default is 262144 (256K). Example: `262144`.
     */
    sortBufferSize?: number;
    /**
     * Global SQL mode. Set to empty to use MySQL server defaults. When creating a new service and not setting this field Aiven default SQL mode (strict, SQL standard compliant) will be assigned. Example: `ANSI,TRADITIONAL`.
     */
    sqlMode?: string;
    /**
     * Require primary key to be defined for new tables or old tables modified with ALTER TABLE and fail if missing. It is recommended to always have primary keys because various functionality may break if any large table is missing them.
     */
    sqlRequirePrimaryKey?: boolean;
    /**
     * Limits the size of internal in-memory tables. Also set max*heap*table_size. Default is 16777216 (16M). Example: `16777216`.
     */
    tmpTableSize?: number;
    /**
     * The number of seconds the server waits for activity on a noninteractive connection before closing it. Example: `28800`.
     */
    waitTimeout?: number;
}

export interface MySqlMysqlUserConfigPrivateAccess {
    /**
     * Allow clients to connect to mysql with a DNS name that always resolves to the service's private IP addresses. Only available in certain network locations.
     */
    mysql?: boolean;
    /**
     * Allow clients to connect to mysqlx with a DNS name that always resolves to the service's private IP addresses. Only available in certain network locations.
     */
    mysqlx?: boolean;
    /**
     * Allow clients to connect to prometheus with a DNS name that always resolves to the service's private IP addresses. Only available in certain network locations.
     */
    prometheus?: boolean;
}

export interface MySqlMysqlUserConfigPrivatelinkAccess {
    /**
     * Enable mysql.
     */
    mysql?: boolean;
    /**
     * Enable mysqlx.
     */
    mysqlx?: boolean;
    /**
     * Enable prometheus.
     */
    prometheus?: boolean;
}

export interface MySqlMysqlUserConfigPublicAccess {
    /**
     * Allow clients to connect to mysql from the public internet for service nodes that are in a project VPC or another type of private network.
     */
    mysql?: boolean;
    /**
     * Allow clients to connect to mysqlx from the public internet for service nodes that are in a project VPC or another type of private network.
     */
    mysqlx?: boolean;
    /**
     * Allow clients to connect to prometheus from the public internet for service nodes that are in a project VPC or another type of private network.
     */
    prometheus?: boolean;
}

export interface MySqlServiceIntegration {
    /**
     * Type of the service integration. The only supported value at the moment is `readReplica`
     */
    integrationType: string;
    /**
     * Name of the source service
     */
    sourceServiceName: string;
}

export interface MySqlTag {
    /**
     * Service tag key
     */
    key: string;
    /**
     * Service tag value
     */
    value: string;
}

export interface MySqlTechEmail {
    /**
     * An email address to contact for technical issues
     */
    email: string;
}

export interface OpenSearchComponent {
    /**
     * Service component name
     */
    component: string;
    /**
     * Connection info for connecting to the service component. This is a combination of host and port.
     */
    connectionUri: string;
    /**
     * Host name for connecting to the service component
     */
    host: string;
    /**
     * Kafka authentication method. This is a value specific to the 'kafka' service component
     */
    kafkaAuthenticationMethod: string;
    /**
     * Port number for connecting to the service component
     */
    port: number;
    /**
     * Network access route
     */
    route: string;
    /**
     * Whether the endpoint is encrypted or accepts plaintext. By default endpoints are always encrypted and this property is only included for service components they may disable encryption
     */
    ssl: boolean;
    /**
     * DNS usage name
     */
    usage: string;
}

export interface OpenSearchOpensearch {
    /**
     * URI for Kibana dashboard frontend
     *
     * @deprecated This field was added by mistake and has never worked. It will be removed in future versions.
     */
    kibanaUri: string;
    /**
     * URI for OpenSearch dashboard frontend
     */
    opensearchDashboardsUri: string;
    /**
     * OpenSearch password
     */
    password: string;
    /**
     * OpenSearch server URIs.
     */
    uris: string[];
    /**
     * OpenSearch username
     */
    username: string;
}

export interface OpenSearchOpensearchUserConfig {
    /**
     * Additional Cloud Regions for Backup Replication.
     */
    additionalBackupRegions?: string;
    azureMigration?: outputs.OpenSearchOpensearchUserConfigAzureMigration;
    /**
     * Serve the web frontend using a custom CNAME pointing to the Aiven DNS name. Example: `grafana.example.org`.
     */
    customDomain?: string;
    /**
     * Disable automatic replication factor adjustment for multi-node services. By default, Aiven ensures all indexes are replicated at least to two nodes. Note: Due to potential data loss in case of losing a service node, this setting can no longer be activated.
     */
    disableReplicationFactorAdjustment?: boolean;
    gcsMigration?: outputs.OpenSearchOpensearchUserConfigGcsMigration;
    /**
     * Index patterns
     */
    indexPatterns?: outputs.OpenSearchOpensearchUserConfigIndexPattern[];
    /**
     * Index rollup settings
     */
    indexRollup?: outputs.OpenSearchOpensearchUserConfigIndexRollup;
    /**
     * Template settings for all new indexes
     */
    indexTemplate?: outputs.OpenSearchOpensearchUserConfigIndexTemplate;
    /**
     * Allow incoming connections from CIDR address block, e.g. `10.20.0.0/16`
     */
    ipFilterObjects?: outputs.OpenSearchOpensearchUserConfigIpFilterObject[];
    /**
     * Allow incoming connections from CIDR address block, e.g. `10.20.0.0/16`.
     */
    ipFilterStrings?: string[];
    /**
     * Allow incoming connections from CIDR address block, e.g. `10.20.0.0/16`.
     *
     * @deprecated Deprecated. Use `ipFilterString` instead.
     */
    ipFilters?: string[];
    /**
     * Aiven automation resets index.refresh_interval to default value for every index to be sure that indices are always visible to search. If it doesn't fit your case, you can disable this by setting up this flag to true.
     */
    keepIndexRefreshInterval?: boolean;
    /**
     * Use indexPatterns instead. Default: `0`.
     */
    maxIndexCount?: number;
    /**
     * OpenSearch OpenID Connect Configuration
     */
    openid?: outputs.OpenSearchOpensearchUserConfigOpenid;
    /**
     * OpenSearch settings
     */
    opensearch?: outputs.OpenSearchOpensearchUserConfigOpensearch;
    /**
     * OpenSearch Dashboards settings
     */
    opensearchDashboards?: outputs.OpenSearchOpensearchUserConfigOpensearchDashboards;
    /**
     * Enum: `1`, `2`, and newer. OpenSearch major version.
     */
    opensearchVersion?: string;
    /**
     * Allow access to selected service ports from private networks
     */
    privateAccess?: outputs.OpenSearchOpensearchUserConfigPrivateAccess;
    /**
     * Allow access to selected service components through Privatelink
     */
    privatelinkAccess?: outputs.OpenSearchOpensearchUserConfigPrivatelinkAccess;
    /**
     * Name of another project to fork a service from. This has effect only when a new service is being created. Example: `anotherprojectname`.
     */
    projectToForkFrom?: string;
    /**
     * Allow access to selected service ports from the public Internet
     */
    publicAccess?: outputs.OpenSearchOpensearchUserConfigPublicAccess;
    /**
     * Name of the basebackup to restore in forked service. Example: `backup-20191112t091354293891z`.
     */
    recoveryBasebackupName?: string;
    s3Migration?: outputs.OpenSearchOpensearchUserConfigS3Migration;
    /**
     * OpenSearch SAML configuration
     */
    saml?: outputs.OpenSearchOpensearchUserConfigSaml;
    /**
     * Store logs for the service so that they are available in the HTTP API and console.
     */
    serviceLog?: boolean;
    /**
     * Name of another service to fork from. This has effect only when a new service is being created. Example: `anotherservicename`.
     */
    serviceToForkFrom?: string;
    /**
     * Use static public IP addresses.
     */
    staticIps?: boolean;
}

export interface OpenSearchOpensearchUserConfigAzureMigration {
    /**
     * Azure account name.
     */
    account: string;
    /**
     * The path to the repository data within its container. The value of this setting should not start or end with a /.
     */
    basePath: string;
    /**
     * Big files can be broken down into chunks during snapshotting if needed. Should be the same as for the 3rd party repository.
     */
    chunkSize?: string;
    /**
     * When set to true metadata files are stored in compressed format.
     */
    compress?: boolean;
    /**
     * Azure container name.
     */
    container: string;
    /**
     * Defines the DNS suffix for Azure Storage endpoints.
     */
    endpointSuffix?: string;
    /**
     * A comma-delimited list of indices to restore from the snapshot. Multi-index syntax is supported. By default, a restore operation includes all data streams and indices in the snapshot. If this argument is provided, the restore operation only includes the data streams and indices that you specify. Example: `metrics*,logs*,data-20240823`.
     */
    indices?: string;
    /**
     * Azure account secret key. One of key or sasToken should be specified.
     */
    key?: string;
    /**
     * A shared access signatures (SAS) token. One of key or sasToken should be specified.
     */
    sasToken?: string;
    /**
     * The snapshot name to restore from.
     */
    snapshotName: string;
}

export interface OpenSearchOpensearchUserConfigGcsMigration {
    /**
     * The path to the repository data within its container. The value of this setting should not start or end with a /.
     */
    basePath: string;
    /**
     * The path to the repository data within its container.
     */
    bucket: string;
    /**
     * Big files can be broken down into chunks during snapshotting if needed. Should be the same as for the 3rd party repository.
     */
    chunkSize?: string;
    /**
     * When set to true metadata files are stored in compressed format.
     */
    compress?: boolean;
    /**
     * Google Cloud Storage credentials file content.
     */
    credentials: string;
    /**
     * A comma-delimited list of indices to restore from the snapshot. Multi-index syntax is supported. By default, a restore operation includes all data streams and indices in the snapshot. If this argument is provided, the restore operation only includes the data streams and indices that you specify. Example: `metrics*,logs*,data-20240823`.
     */
    indices?: string;
    /**
     * The snapshot name to restore from.
     */
    snapshotName: string;
}

export interface OpenSearchOpensearchUserConfigIndexPattern {
    /**
     * Maximum number of indexes to keep. Example: `3`.
     */
    maxIndexCount: number;
    /**
     * fnmatch pattern. Example: `logs_*_foo_*`.
     */
    pattern: string;
    /**
     * Enum: `alphabetical`, `creationDate`. Deletion sorting algorithm. Default: `creationDate`.
     */
    sortingAlgorithm?: string;
}

export interface OpenSearchOpensearchUserConfigIndexRollup {
    /**
     * Whether rollups are enabled in OpenSearch Dashboards. Defaults to true.
     */
    rollupDashboardsEnabled?: boolean;
    /**
     * Whether the rollup plugin is enabled. Defaults to true.
     */
    rollupEnabled?: boolean;
    /**
     * How many retries the plugin should attempt for failed rollup jobs. Defaults to 5.
     */
    rollupSearchBackoffCount?: number;
    /**
     * The backoff time between retries for failed rollup jobs. Defaults to 1000ms.
     */
    rollupSearchBackoffMillis?: number;
    /**
     * Whether OpenSearch should return all jobs that match all specified search terms. If disabled, OpenSearch returns just one, as opposed to all, of the jobs that matches the search terms. Defaults to false.
     */
    rollupSearchSearchAllJobs?: boolean;
}

export interface OpenSearchOpensearchUserConfigIndexTemplate {
    /**
     * The maximum number of nested JSON objects that a single document can contain across all nested types. This limit helps to prevent out of memory errors when a document contains too many nested objects. Default is 10000. Example: `10000`.
     */
    mappingNestedObjectsLimit?: number;
    /**
     * The number of replicas each primary shard has. Example: `1`.
     */
    numberOfReplicas?: number;
    /**
     * The number of primary shards that an index should have. Example: `1`.
     */
    numberOfShards?: number;
}

export interface OpenSearchOpensearchUserConfigIpFilterObject {
    /**
     * Description for IP filter list entry. Example: `Production service IP range`.
     */
    description?: string;
    /**
     * CIDR address block. Example: `10.20.0.0/16`.
     */
    network: string;
}

export interface OpenSearchOpensearchUserConfigOpenid {
    /**
     * The ID of the OpenID Connect client configured in your IdP. Required.
     */
    clientId: string;
    /**
     * The client secret of the OpenID Connect client configured in your IdP. Required.
     */
    clientSecret: string;
    /**
     * The URL of your IdP where the Security plugin can find the OpenID Connect metadata/configuration settings. Example: `https://test-account.okta.com/app/exk491jujcVc83LEX697/sso/saml/metadata`.
     */
    connectUrl: string;
    /**
     * Enables or disables OpenID Connect authentication for OpenSearch. When enabled, users can authenticate using OpenID Connect with an Identity Provider. Default: `true`.
     */
    enabled: boolean;
    /**
     * HTTP header name of the JWT token. Optional. Default is Authorization. Default: `Authorization`.
     */
    header?: string;
    /**
     * The HTTP header that stores the token. Typically the Authorization header with the Bearer schema: Authorization: Bearer \n\n. Optional. Default is Authorization. Example: `preferredUsername`.
     */
    jwtHeader?: string;
    /**
     * If the token is not transmitted in the HTTP header, but as an URL parameter, define the name of the parameter here. Optional. Example: `preferredUsername`.
     */
    jwtUrlParameter?: string;
    /**
     * The maximum number of unknown key IDs in the time frame. Default is 10. Optional. Default: `10`.
     */
    refreshRateLimitCount?: number;
    /**
     * The time frame to use when checking the maximum number of unknown key IDs, in milliseconds. Optional.Default is 10000 (10 seconds). Default: `10000`.
     */
    refreshRateLimitTimeWindowMs?: number;
    /**
     * The key in the JSON payload that stores the user’s roles. The value of this key must be a comma-separated list of roles. Required only if you want to use roles in the JWT. Example: `roles`.
     */
    rolesKey?: string;
    /**
     * The scope of the identity token issued by the IdP. Optional. Default is openid profile email address phone.
     */
    scope?: string;
    /**
     * The key in the JSON payload that stores the user’s name. If not defined, the subject registered claim is used. Most IdP providers use the preferredUsername claim. Optional. Example: `preferredUsername`.
     */
    subjectKey?: string;
}

export interface OpenSearchOpensearchUserConfigOpensearch {
    /**
     * Explicitly allow or block automatic creation of indices. Defaults to true.
     */
    actionAutoCreateIndexEnabled?: boolean;
    /**
     * Require explicit index names when deleting.
     */
    actionDestructiveRequiresName?: boolean;
    /**
     * Opensearch Security Plugin Settings
     */
    authFailureListeners?: outputs.OpenSearchOpensearchUserConfigOpensearchAuthFailureListeners;
    /**
     * Controls the number of shards allowed in the cluster per data node. Example: `1000`.
     */
    clusterMaxShardsPerNode?: number;
    /**
     * How many concurrent incoming/outgoing shard recoveries (normally replicas) are allowed to happen on a node. Defaults to node cpu count * 2.
     */
    clusterRoutingAllocationNodeConcurrentRecoveries?: number;
    /**
     * Sender name placeholder to be used in Opensearch Dashboards and Opensearch keystore. Example: `alert-sender`.
     */
    emailSenderName?: string;
    /**
     * Sender password for Opensearch alerts to authenticate with SMTP server. Example: `very-secure-mail-password`.
     */
    emailSenderPassword?: string;
    /**
     * Sender username for Opensearch alerts. Example: `jane@example.com`.
     */
    emailSenderUsername?: string;
    /**
     * Enable/Disable security audit.
     */
    enableSecurityAudit?: boolean;
    /**
     * Maximum content length for HTTP requests to the OpenSearch HTTP API, in bytes.
     */
    httpMaxContentLength?: number;
    /**
     * The max size of allowed headers, in bytes. Example: `8192`.
     */
    httpMaxHeaderSize?: number;
    /**
     * The max length of an HTTP URL, in bytes. Example: `4096`.
     */
    httpMaxInitialLineLength?: number;
    /**
     * Relative amount. Maximum amount of heap memory used for field data cache. This is an expert setting; decreasing the value too much will increase overhead of loading field data; too much memory used for field data cache will decrease amount of heap available for other operations.
     */
    indicesFielddataCacheSize?: number;
    /**
     * Percentage value. Default is 10%. Total amount of heap used for indexing buffer, before writing segments to disk. This is an expert setting. Too low value will slow down indexing; too high value will increase indexing performance but causes performance issues for query performance.
     */
    indicesMemoryIndexBufferSize?: number;
    /**
     * Absolute value. Default is unbound. Doesn't work without indices.memory.index*buffer*size. Maximum amount of heap used for query cache, an absolute indices.memory.index*buffer*size maximum hard limit.
     */
    indicesMemoryMaxIndexBufferSize?: number;
    /**
     * Absolute value. Default is 48mb. Doesn't work without indices.memory.index*buffer*size. Minimum amount of heap used for query cache, an absolute indices.memory.index*buffer*size minimal hard limit.
     */
    indicesMemoryMinIndexBufferSize?: number;
    /**
     * Percentage value. Default is 10%. Maximum amount of heap used for query cache. This is an expert setting. Too low value will decrease query performance and increase performance for other operations; too high value will cause issues with other OpenSearch functionality.
     */
    indicesQueriesCacheSize?: number;
    /**
     * Maximum number of clauses Lucene BooleanQuery can have. The default value (1024) is relatively high, and increasing it may cause performance issues. Investigate other approaches first before increasing this value.
     */
    indicesQueryBoolMaxClauseCount?: number;
    /**
     * Limits total inbound and outbound recovery traffic for each node. Applies to both peer recoveries as well as snapshot recoveries (i.e., restores from a snapshot). Defaults to 40mb.
     */
    indicesRecoveryMaxBytesPerSec?: number;
    /**
     * Number of file chunks sent in parallel for each recovery. Defaults to 2.
     */
    indicesRecoveryMaxConcurrentFileChunks?: number;
    /**
     * Specifies whether ISM is enabled or not.
     */
    ismEnabled?: boolean;
    /**
     * Specifies whether audit history is enabled or not. The logs from ISM are automatically indexed to a logs document.
     */
    ismHistoryEnabled?: boolean;
    /**
     * The maximum age before rolling over the audit history index in hours. Example: `24`.
     */
    ismHistoryMaxAge?: number;
    /**
     * The maximum number of documents before rolling over the audit history index. Example: `2500000`.
     */
    ismHistoryMaxDocs?: number;
    /**
     * The time between rollover checks for the audit history index in hours. Example: `8`.
     */
    ismHistoryRolloverCheckPeriod?: number;
    /**
     * How long audit history indices are kept in days. Example: `30`.
     */
    ismHistoryRolloverRetentionPeriod?: number;
    /**
     * Enable or disable KNN memory circuit breaker. Defaults to true.
     */
    knnMemoryCircuitBreakerEnabled?: boolean;
    /**
     * Maximum amount of memory that can be used for KNN index. Defaults to 50% of the JVM heap size.
     */
    knnMemoryCircuitBreakerLimit?: number;
    /**
     * Compatibility mode sets OpenSearch to report its version as 7.10 so clients continue to work. Default is false.
     */
    overrideMainResponseVersion?: boolean;
    /**
     * Enable or disable filtering of alerting by backend roles. Requires Security plugin. Defaults to false.
     */
    pluginsAlertingFilterByBackendRoles?: boolean;
    /**
     * Whitelisted addresses for reindexing. Changing this value will cause all OpenSearch instances to restart.
     */
    reindexRemoteWhitelists?: string[];
    /**
     * Script compilation circuit breaker limits the number of inline script compilations within a period of time. Default is use-context. Example: `75/5m`.
     */
    scriptMaxCompilationsRate?: string;
    /**
     * Maximum number of aggregation buckets allowed in a single response. OpenSearch default value is used when this is not defined. Example: `10000`.
     */
    searchMaxBuckets?: number;
    /**
     * Size for the thread pool queue. See documentation for exact details.
     */
    threadPoolAnalyzeQueueSize?: number;
    /**
     * Size for the thread pool. See documentation for exact details. Do note this may have maximum value depending on CPU count - value is automatically lowered if set to higher than maximum value.
     */
    threadPoolAnalyzeSize?: number;
    /**
     * Size for the thread pool. See documentation for exact details. Do note this may have maximum value depending on CPU count - value is automatically lowered if set to higher than maximum value.
     */
    threadPoolForceMergeSize?: number;
    /**
     * Size for the thread pool queue. See documentation for exact details.
     */
    threadPoolGetQueueSize?: number;
    /**
     * Size for the thread pool. See documentation for exact details. Do note this may have maximum value depending on CPU count - value is automatically lowered if set to higher than maximum value.
     */
    threadPoolGetSize?: number;
    /**
     * Size for the thread pool queue. See documentation for exact details.
     */
    threadPoolSearchQueueSize?: number;
    /**
     * Size for the thread pool. See documentation for exact details. Do note this may have maximum value depending on CPU count - value is automatically lowered if set to higher than maximum value.
     */
    threadPoolSearchSize?: number;
    /**
     * Size for the thread pool queue. See documentation for exact details.
     */
    threadPoolSearchThrottledQueueSize?: number;
    /**
     * Size for the thread pool. See documentation for exact details. Do note this may have maximum value depending on CPU count - value is automatically lowered if set to higher than maximum value.
     */
    threadPoolSearchThrottledSize?: number;
    /**
     * Size for the thread pool queue. See documentation for exact details.
     */
    threadPoolWriteQueueSize?: number;
    /**
     * Size for the thread pool. See documentation for exact details. Do note this may have maximum value depending on CPU count - value is automatically lowered if set to higher than maximum value.
     */
    threadPoolWriteSize?: number;
}

export interface OpenSearchOpensearchUserConfigOpensearchAuthFailureListeners {
    internalAuthenticationBackendLimiting?: outputs.OpenSearchOpensearchUserConfigOpensearchAuthFailureListenersInternalAuthenticationBackendLimiting;
    /**
     * IP address rate limiting settings
     */
    ipRateLimiting?: outputs.OpenSearchOpensearchUserConfigOpensearchAuthFailureListenersIpRateLimiting;
}

export interface OpenSearchOpensearchUserConfigOpensearchAuthFailureListenersInternalAuthenticationBackendLimiting {
    /**
     * The number of login attempts allowed before login is blocked. Example: `10`.
     */
    allowedTries?: number;
    /**
     * Enum: `internal`. internal*authentication*backend*limiting.authentication*backend.
     */
    authenticationBackend?: string;
    /**
     * The duration of time that login remains blocked after a failed login. Example: `600`.
     */
    blockExpirySeconds?: number;
    /**
     * internal*authentication*backend*limiting.max*blocked_clients. Example: `100000`.
     */
    maxBlockedClients?: number;
    /**
     * The maximum number of tracked IP addresses that have failed login. Example: `100000`.
     */
    maxTrackedClients?: number;
    /**
     * The window of time in which the value for `allowedTries` is enforced. Example: `3600`.
     */
    timeWindowSeconds?: number;
    /**
     * Enum: `username`. internal*authentication*backend_limiting.type.
     */
    type?: string;
}

export interface OpenSearchOpensearchUserConfigOpensearchAuthFailureListenersIpRateLimiting {
    /**
     * The number of login attempts allowed before login is blocked. Example: `10`.
     */
    allowedTries?: number;
    /**
     * The duration of time that login remains blocked after a failed login. Example: `600`.
     */
    blockExpirySeconds?: number;
    /**
     * The maximum number of blocked IP addresses. Example: `100000`.
     */
    maxBlockedClients?: number;
    /**
     * The maximum number of tracked IP addresses that have failed login. Example: `100000`.
     */
    maxTrackedClients?: number;
    /**
     * The window of time in which the value for `allowedTries` is enforced. Example: `3600`.
     */
    timeWindowSeconds?: number;
    /**
     * Enum: `ip`. The type of rate limiting.
     */
    type?: string;
}

export interface OpenSearchOpensearchUserConfigOpensearchDashboards {
    /**
     * Enable or disable OpenSearch Dashboards. Default: `true`.
     */
    enabled?: boolean;
    /**
     * Limits the maximum amount of memory (in MiB) the OpenSearch Dashboards process can use. This sets the max*old*space_size option of the nodejs running the OpenSearch Dashboards. Note: the memory reserved by OpenSearch Dashboards is not available for OpenSearch. Default: `128`.
     */
    maxOldSpaceSize?: number;
    /**
     * Timeout in milliseconds for requests made by OpenSearch Dashboards towards OpenSearch. Default: `30000`.
     */
    opensearchRequestTimeout?: number;
}

export interface OpenSearchOpensearchUserConfigPrivateAccess {
    /**
     * Allow clients to connect to opensearch with a DNS name that always resolves to the service's private IP addresses. Only available in certain network locations.
     */
    opensearch?: boolean;
    /**
     * Allow clients to connect to opensearchDashboards with a DNS name that always resolves to the service's private IP addresses. Only available in certain network locations.
     */
    opensearchDashboards?: boolean;
    /**
     * Allow clients to connect to prometheus with a DNS name that always resolves to the service's private IP addresses. Only available in certain network locations.
     */
    prometheus?: boolean;
}

export interface OpenSearchOpensearchUserConfigPrivatelinkAccess {
    /**
     * Enable opensearch.
     */
    opensearch?: boolean;
    /**
     * Enable opensearch_dashboards.
     */
    opensearchDashboards?: boolean;
    /**
     * Enable prometheus.
     */
    prometheus?: boolean;
}

export interface OpenSearchOpensearchUserConfigPublicAccess {
    /**
     * Allow clients to connect to opensearch from the public internet for service nodes that are in a project VPC or another type of private network.
     */
    opensearch?: boolean;
    /**
     * Allow clients to connect to opensearchDashboards from the public internet for service nodes that are in a project VPC or another type of private network.
     */
    opensearchDashboards?: boolean;
    /**
     * Allow clients to connect to prometheus from the public internet for service nodes that are in a project VPC or another type of private network.
     */
    prometheus?: boolean;
}

export interface OpenSearchOpensearchUserConfigS3Migration {
    /**
     * AWS Access key.
     */
    accessKey: string;
    /**
     * The path to the repository data within its container. The value of this setting should not start or end with a /.
     */
    basePath: string;
    /**
     * S3 bucket name.
     */
    bucket: string;
    /**
     * Big files can be broken down into chunks during snapshotting if needed. Should be the same as for the 3rd party repository.
     */
    chunkSize?: string;
    /**
     * When set to true metadata files are stored in compressed format.
     */
    compress?: boolean;
    /**
     * The S3 service endpoint to connect to. If you are using an S3-compatible service then you should set this to the service’s endpoint.
     */
    endpoint?: string;
    /**
     * A comma-delimited list of indices to restore from the snapshot. Multi-index syntax is supported. By default, a restore operation includes all data streams and indices in the snapshot. If this argument is provided, the restore operation only includes the data streams and indices that you specify. Example: `metrics*,logs*,data-20240823`.
     */
    indices?: string;
    /**
     * S3 region.
     */
    region: string;
    /**
     * AWS secret key.
     */
    secretKey: string;
    /**
     * When set to true files are encrypted on server side.
     */
    serverSideEncryption?: boolean;
    /**
     * The snapshot name to restore from.
     */
    snapshotName: string;
}

export interface OpenSearchOpensearchUserConfigSaml {
    /**
     * Enables or disables SAML-based authentication for OpenSearch. When enabled, users can authenticate using SAML with an Identity Provider. Default: `true`.
     */
    enabled: boolean;
    /**
     * The unique identifier for the Identity Provider (IdP) entity that is used for SAML authentication. This value is typically provided by the IdP. Example: `test-idp-entity-id`.
     */
    idpEntityId: string;
    /**
     * The URL of the SAML metadata for the Identity Provider (IdP). This is used to configure SAML-based authentication with the IdP. Example: `https://test-account.okta.com/app/exk491jujcVc83LEX697/sso/saml/metadata`.
     */
    idpMetadataUrl: string;
    /**
     * This parameter specifies the PEM-encoded root certificate authority (CA) content for the SAML identity provider (IdP) server verification. The root CA content is used to verify the SSL/TLS certificate presented by the server. Example: `-----BEGIN CERTIFICATE-----
     * ...
     * -----END CERTIFICATE-----
     * `.
     */
    idpPemtrustedcasContent?: string;
    /**
     * Optional. Specifies the attribute in the SAML response where role information is stored, if available. Role attributes are not required for SAML authentication, but can be included in SAML assertions by most Identity Providers (IdPs) to determine user access levels or permissions. Example: `RoleName`.
     */
    rolesKey?: string;
    /**
     * The unique identifier for the Service Provider (SP) entity that is used for SAML authentication. This value is typically provided by the SP. Example: `test-sp-entity-id`.
     */
    spEntityId: string;
    /**
     * Optional. Specifies the attribute in the SAML response where the subject identifier is stored. If not configured, the NameID attribute is used by default. Example: `NameID`.
     */
    subjectKey?: string;
}

export interface OpenSearchServiceIntegration {
    /**
     * Type of the service integration. The only supported value at the moment is `readReplica`
     */
    integrationType: string;
    /**
     * Name of the source service
     */
    sourceServiceName: string;
}

export interface OpenSearchTag {
    /**
     * Service tag key
     */
    key: string;
    /**
     * Service tag value
     */
    value: string;
}

export interface OpenSearchTechEmail {
    /**
     * An email address to contact for technical issues
     */
    email: string;
}

export interface OrganizationGroupProjectTimeouts {
    /**
     * A string that can be [parsed as a duration](https://pkg.go.dev/time#ParseDuration) consisting of numbers and unit suffixes, such as "30s" or "2h45m". Valid time units are "s" (seconds), "m" (minutes), "h" (hours).
     */
    create?: string;
    /**
     * A string that can be [parsed as a duration](https://pkg.go.dev/time#ParseDuration) consisting of numbers and unit suffixes, such as "30s" or "2h45m". Valid time units are "s" (seconds), "m" (minutes), "h" (hours). Setting a timeout for a Delete operation is only applicable if changes are saved into state before the destroy operation occurs.
     */
    delete?: string;
    /**
     * A string that can be [parsed as a duration](https://pkg.go.dev/time#ParseDuration) consisting of numbers and unit suffixes, such as "30s" or "2h45m". Valid time units are "s" (seconds), "m" (minutes), "h" (hours). Read operations occur during any refresh or planning operation when refresh is enabled.
     */
    read?: string;
    /**
     * A string that can be [parsed as a duration](https://pkg.go.dev/time#ParseDuration) consisting of numbers and unit suffixes, such as "30s" or "2h45m". Valid time units are "s" (seconds), "m" (minutes), "h" (hours).
     */
    update?: string;
}

export interface OrganizationPermissionPermission {
    /**
     * Time created.
     */
    createTime: string;
    /**
     * List of permissions. The possible values are `admin`, `developer`, `operator` and `readOnly`.
     */
    permissions: string[];
    /**
     * ID of the user or group.
     */
    principalId: string;
    /**
     * The type of principal. The possible values are `user` and `userGroup`.
     */
    principalType: string;
    /**
     * Time updated.
     */
    updateTime: string;
}

export interface OrganizationTimeouts {
    /**
     * A string that can be [parsed as a duration](https://pkg.go.dev/time#ParseDuration) consisting of numbers and unit suffixes, such as "30s" or "2h45m". Valid time units are "s" (seconds), "m" (minutes), "h" (hours).
     */
    create?: string;
    /**
     * A string that can be [parsed as a duration](https://pkg.go.dev/time#ParseDuration) consisting of numbers and unit suffixes, such as "30s" or "2h45m". Valid time units are "s" (seconds), "m" (minutes), "h" (hours). Setting a timeout for a Delete operation is only applicable if changes are saved into state before the destroy operation occurs.
     */
    delete?: string;
    /**
     * A string that can be [parsed as a duration](https://pkg.go.dev/time#ParseDuration) consisting of numbers and unit suffixes, such as "30s" or "2h45m". Valid time units are "s" (seconds), "m" (minutes), "h" (hours). Read operations occur during any refresh or planning operation when refresh is enabled.
     */
    read?: string;
    /**
     * A string that can be [parsed as a duration](https://pkg.go.dev/time#ParseDuration) consisting of numbers and unit suffixes, such as "30s" or "2h45m". Valid time units are "s" (seconds), "m" (minutes), "h" (hours).
     */
    update?: string;
}

export interface OrganizationUserGroupMemberTimeouts {
    /**
     * A string that can be [parsed as a duration](https://pkg.go.dev/time#ParseDuration) consisting of numbers and unit suffixes, such as "30s" or "2h45m". Valid time units are "s" (seconds), "m" (minutes), "h" (hours).
     */
    create?: string;
    /**
     * A string that can be [parsed as a duration](https://pkg.go.dev/time#ParseDuration) consisting of numbers and unit suffixes, such as "30s" or "2h45m". Valid time units are "s" (seconds), "m" (minutes), "h" (hours). Setting a timeout for a Delete operation is only applicable if changes are saved into state before the destroy operation occurs.
     */
    delete?: string;
    /**
     * A string that can be [parsed as a duration](https://pkg.go.dev/time#ParseDuration) consisting of numbers and unit suffixes, such as "30s" or "2h45m". Valid time units are "s" (seconds), "m" (minutes), "h" (hours). Read operations occur during any refresh or planning operation when refresh is enabled.
     */
    read?: string;
    /**
     * A string that can be [parsed as a duration](https://pkg.go.dev/time#ParseDuration) consisting of numbers and unit suffixes, such as "30s" or "2h45m". Valid time units are "s" (seconds), "m" (minutes), "h" (hours).
     */
    update?: string;
}

export interface PgComponent {
    /**
     * Service component name
     */
    component: string;
    /**
     * Connection info for connecting to the service component. This is a combination of host and port.
     */
    connectionUri: string;
    /**
     * Host name for connecting to the service component
     */
    host: string;
    /**
     * Kafka authentication method. This is a value specific to the 'kafka' service component
     */
    kafkaAuthenticationMethod: string;
    /**
     * Port number for connecting to the service component
     */
    port: number;
    /**
     * Network access route
     */
    route: string;
    /**
     * Whether the endpoint is encrypted or accepts plaintext. By default endpoints are always encrypted and this property is only included for service components they may disable encryption
     */
    ssl: boolean;
    /**
     * DNS usage name
     */
    usage: string;
}

export interface PgPg {
    /**
     * PgBouncer connection details for [connection pooling](https://aiven.io/docs/products/postgresql/concepts/pg-connection-pooling).
     *
     * @deprecated This field was added by mistake and has never worked. It will be removed in future versions.
     */
    bouncer: string;
    /**
     * Primary PostgreSQL database name.
     */
    dbname: string;
    /**
     * PostgreSQL primary node host IP or name.
     */
    host: string;
    /**
     * The [number of allowed connections](https://aiven.io/docs/products/postgresql/reference/pg-connection-limits). Varies based on the service plan.
     */
    maxConnections: number;
    /**
     * PostgreSQL connection parameters.
     */
    params: outputs.PgPgParam[];
    /**
     * PostgreSQL admin user password.
     */
    password: string;
    /**
     * PostgreSQL port.
     */
    port: number;
    /**
     * PostgreSQL replica URI for services with a replica.
     */
    replicaUri: string;
    /**
     * PostgreSQL SSL mode setting.
     */
    sslmode: string;
    /**
     * PostgreSQL standby connection URIs.
     */
    standbyUris: string[];
    /**
     * PostgreSQL syncing connection URIs.
     */
    syncingUris: string[];
    /**
     * PostgreSQL primary connection URI.
     */
    uri: string;
    /**
     * PostgreSQL primary connection URIs.
     */
    uris: string[];
    /**
     * PostgreSQL admin user name.
     */
    user: string;
}

export interface PgPgParam {
    /**
     * Primary PostgreSQL database name.
     */
    databaseName: string;
    /**
     * PostgreSQL host IP or name.
     */
    host: string;
    /**
     * PostgreSQL admin user password.
     */
    password: string;
    /**
     * PostgreSQL port.
     */
    port: number;
    /**
     * PostgreSQL SSL mode setting.
     */
    sslmode: string;
    /**
     * PostgreSQL admin user name.
     */
    user: string;
}

export interface PgPgUserConfig {
    /**
     * Additional Cloud Regions for Backup Replication.
     *
     * @deprecated This property is deprecated.
     */
    additionalBackupRegions?: string;
    /**
     * Custom password for admin user. Defaults to random string. This must be set only when a new service is being created.
     */
    adminPassword?: string;
    /**
     * Custom username for admin user. This must be set only when a new service is being created. Example: `avnadmin`.
     */
    adminUsername?: string;
    /**
     * The hour of day (in UTC) when backup for the service is started. New backup is only started if previous backup has already completed. Example: `3`.
     */
    backupHour?: number;
    /**
     * The minute of an hour when backup for the service is started. New backup is only started if previous backup has already completed. Example: `30`.
     */
    backupMinute?: number;
    /**
     * Register AAAA DNS records for the service, and allow IPv6 packets to service ports.
     */
    enableIpv6?: boolean;
    /**
     * Allow incoming connections from CIDR address block, e.g. `10.20.0.0/16`
     */
    ipFilterObjects?: outputs.PgPgUserConfigIpFilterObject[];
    /**
     * Allow incoming connections from CIDR address block, e.g. `10.20.0.0/16`.
     */
    ipFilterStrings?: string[];
    /**
     * Allow incoming connections from CIDR address block, e.g. `10.20.0.0/16`.
     *
     * @deprecated Deprecated. Use `ipFilterString` instead.
     */
    ipFilters?: string[];
    /**
     * Migrate data from existing server
     */
    migration?: outputs.PgPgUserConfigMigration;
    /**
     * postgresql.conf configuration values
     */
    pg?: outputs.PgPgUserConfigPg;
    /**
     * System-wide settings for the pg*qualstats extension
     *
     * @deprecated This property is deprecated.
     */
    pgQualstats?: outputs.PgPgUserConfigPgQualstats;
    /**
     * Should the service which is being forked be a read replica (deprecated, use readReplica service integration instead).
     */
    pgReadReplica?: boolean;
    /**
     * Name of the PG Service from which to fork (deprecated, use service*to*fork_from). This has effect only when a new service is being created. Example: `anotherservicename`.
     */
    pgServiceToForkFrom?: string;
    /**
     * Enable the pg*stat*monitor extension. Enabling this extension will cause the cluster to be restarted.When this extension is enabled, pg*stat*statements results for utility commands are unreliable. Default: `false`.
     */
    pgStatMonitorEnable?: boolean;
    /**
     * Enum: `10`, `11`, `12`, `13`, `14`, `15`, `16`, and newer. PostgreSQL major version.
     */
    pgVersion?: string;
    /**
     * System-wide settings for the pgaudit extension
     *
     * @deprecated This property is deprecated.
     */
    pgaudit?: outputs.PgPgUserConfigPgaudit;
    /**
     * PGBouncer connection pooling settings
     */
    pgbouncer?: outputs.PgPgUserConfigPgbouncer;
    /**
     * System-wide settings for pglookout
     */
    pglookout?: outputs.PgPgUserConfigPglookout;
    /**
     * Allow access to selected service ports from private networks
     */
    privateAccess?: outputs.PgPgUserConfigPrivateAccess;
    /**
     * Allow access to selected service components through Privatelink
     */
    privatelinkAccess?: outputs.PgPgUserConfigPrivatelinkAccess;
    /**
     * Name of another project to fork a service from. This has effect only when a new service is being created. Example: `anotherprojectname`.
     */
    projectToForkFrom?: string;
    /**
     * Allow access to selected service ports from the public Internet
     */
    publicAccess?: outputs.PgPgUserConfigPublicAccess;
    /**
     * Recovery target time when forking a service. This has effect only when a new service is being created. Example: `2019-01-01 23:34:45`.
     */
    recoveryTargetTime?: string;
    /**
     * Store logs for the service so that they are available in the HTTP API and console.
     */
    serviceLog?: boolean;
    /**
     * Name of another service to fork from. This has effect only when a new service is being created. Example: `anotherservicename`.
     */
    serviceToForkFrom?: string;
    /**
     * Percentage of total RAM that the database server uses for shared memory buffers. Valid range is 20-60 (float), which corresponds to 20% - 60%. This setting adjusts the sharedBuffers configuration value. Example: `41.5`.
     */
    sharedBuffersPercentage?: number;
    /**
     * Use static public IP addresses.
     */
    staticIps?: boolean;
    /**
     * Enum: `quorum`, `off`. Synchronous replication type. Note that the service plan also needs to support synchronous replication.
     */
    synchronousReplication?: string;
    /**
     * System-wide settings for the timescaledb extension
     */
    timescaledb?: outputs.PgPgUserConfigTimescaledb;
    /**
     * Enum: `aiven`, `timescale`. Variant of the PostgreSQL service, may affect the features that are exposed by default.
     */
    variant?: string;
    /**
     * Sets the maximum amount of memory to be used by a query operation (such as a sort or hash table) before writing to temporary disk files, in MB. Default is 1MB + 0.075% of total RAM (up to 32MB). Example: `4`.
     */
    workMem?: number;
}

export interface PgPgUserConfigIpFilterObject {
    /**
     * Description for IP filter list entry. Example: `Production service IP range`.
     */
    description?: string;
    /**
     * CIDR address block. Example: `10.20.0.0/16`.
     */
    network: string;
}

export interface PgPgUserConfigMigration {
    /**
     * Database name for bootstrapping the initial connection. Example: `defaultdb`.
     */
    dbname?: string;
    /**
     * Hostname or IP address of the server where to migrate data from. Example: `my.server.com`.
     */
    host: string;
    /**
     * Comma-separated list of databases, which should be ignored during migration (supported by MySQL and PostgreSQL only at the moment). Example: `db1,db2`.
     */
    ignoreDbs?: string;
    /**
     * Comma-separated list of database roles, which should be ignored during migration (supported by PostgreSQL only at the moment). Example: `role1,role2`.
     */
    ignoreRoles?: string;
    /**
     * Enum: `dump`, `replication`. The migration method to be used (currently supported only by Redis, Dragonfly, MySQL and PostgreSQL service types).
     */
    method?: string;
    /**
     * Password for authentication with the server where to migrate data from. Example: `jjKk45Nnd`.
     */
    password?: string;
    /**
     * Port number of the server where to migrate data from. Example: `1234`.
     */
    port: number;
    /**
     * The server where to migrate data from is secured with SSL. Default: `true`.
     */
    ssl?: boolean;
    /**
     * User name for authentication with the server where to migrate data from. Example: `myname`.
     */
    username?: string;
}

export interface PgPgUserConfigPg {
    /**
     * Specifies a fraction of the table size to add to autovacuum*analyze*threshold when deciding whether to trigger an ANALYZE. The default is 0.2 (20% of table size).
     */
    autovacuumAnalyzeScaleFactor?: number;
    /**
     * Specifies the minimum number of inserted, updated or deleted tuples needed to trigger an ANALYZE in any one table. The default is 50 tuples.
     */
    autovacuumAnalyzeThreshold?: number;
    /**
     * Specifies the maximum age (in transactions) that a table's pg_class.relfrozenxid field can attain before a VACUUM operation is forced to prevent transaction ID wraparound within the table. Note that the system will launch autovacuum processes to prevent wraparound even when autovacuum is otherwise disabled. This parameter will cause the server to be restarted. Example: `200000000`.
     */
    autovacuumFreezeMaxAge?: number;
    /**
     * Specifies the maximum number of autovacuum processes (other than the autovacuum launcher) that may be running at any one time. The default is three. This parameter can only be set at server start.
     */
    autovacuumMaxWorkers?: number;
    /**
     * Specifies the minimum delay between autovacuum runs on any given database. The delay is measured in seconds, and the default is one minute.
     */
    autovacuumNaptime?: number;
    /**
     * Specifies the cost delay value that will be used in automatic VACUUM operations. If -1 is specified, the regular vacuum*cost*delay value will be used. The default value is 20 milliseconds.
     */
    autovacuumVacuumCostDelay?: number;
    /**
     * Specifies the cost limit value that will be used in automatic VACUUM operations. If -1 is specified (which is the default), the regular vacuum*cost*limit value will be used.
     */
    autovacuumVacuumCostLimit?: number;
    /**
     * Specifies a fraction of the table size to add to autovacuum*vacuum*threshold when deciding whether to trigger a VACUUM. The default is 0.2 (20% of table size).
     */
    autovacuumVacuumScaleFactor?: number;
    /**
     * Specifies the minimum number of updated or deleted tuples needed to trigger a VACUUM in any one table. The default is 50 tuples.
     */
    autovacuumVacuumThreshold?: number;
    /**
     * Specifies the delay between activity rounds for the background writer in milliseconds. Default is 200. Example: `200`.
     */
    bgwriterDelay?: number;
    /**
     * Whenever more than bgwriter*flush*after bytes have been written by the background writer, attempt to force the OS to issue these writes to the underlying storage. Specified in kilobytes, default is 512. Setting of 0 disables forced writeback. Example: `512`.
     */
    bgwriterFlushAfter?: number;
    /**
     * In each round, no more than this many buffers will be written by the background writer. Setting this to zero disables background writing. Default is 100. Example: `100`.
     */
    bgwriterLruMaxpages?: number;
    /**
     * The average recent need for new buffers is multiplied by bgwriter*lru*multiplier to arrive at an estimate of the number that will be needed during the next round, (up to bgwriter*lru*maxpages). 1.0 represents a “just in time” policy of writing exactly the number of buffers predicted to be needed. Larger values provide some cushion against spikes in demand, while smaller values intentionally leave writes to be done by server processes. The default is 2.0. Example: `2.0`.
     */
    bgwriterLruMultiplier?: number;
    /**
     * This is the amount of time, in milliseconds, to wait on a lock before checking to see if there is a deadlock condition. Example: `1000`.
     */
    deadlockTimeout?: number;
    /**
     * Enum: `lz4`, `pglz`. Specifies the default TOAST compression method for values of compressible columns (the default is lz4).
     */
    defaultToastCompression?: string;
    /**
     * Time out sessions with open transactions after this number of milliseconds.
     */
    idleInTransactionSessionTimeout?: number;
    /**
     * Controls system-wide use of Just-in-Time Compilation (JIT).
     */
    jit?: boolean;
    /**
     * Causes each action executed by autovacuum to be logged if it ran for at least the specified number of milliseconds. Setting this to zero logs all autovacuum actions. Minus-one (the default) disables logging autovacuum actions.
     */
    logAutovacuumMinDuration?: number;
    /**
     * Enum: `TERSE`, `DEFAULT`, `VERBOSE`. Controls the amount of detail written in the server log for each message that is logged.
     */
    logErrorVerbosity?: string;
    /**
     * Enum: `'pid=%p,user=%u,db=%d,app=%a,client=%h '`, `'%t [%p]: [%l-1] user=%u,db=%d,app=%a,client=%h '`, `'%m [%p] %q[user=%u,db=%d,app=%a] '`, `'pid=%p,user=%u,db=%d,app=%a,client=%h,txid=%x,qid=%Q '`. Choose from one of the available log formats.
     */
    logLinePrefix?: string;
    /**
     * Log statements that take more than this number of milliseconds to run, -1 disables.
     */
    logMinDurationStatement?: number;
    /**
     * Log statements for each temporary file created larger than this number of kilobytes, -1 disables.
     */
    logTempFiles?: number;
    /**
     * PostgreSQL maximum number of files that can be open per process.
     */
    maxFilesPerProcess?: number;
    /**
     * PostgreSQL maximum locks per transaction.
     */
    maxLocksPerTransaction?: number;
    /**
     * PostgreSQL maximum logical replication workers (taken from the pool of max*parallel*workers).
     */
    maxLogicalReplicationWorkers?: number;
    /**
     * Sets the maximum number of workers that the system can support for parallel queries.
     */
    maxParallelWorkers?: number;
    /**
     * Sets the maximum number of workers that can be started by a single Gather or Gather Merge node.
     */
    maxParallelWorkersPerGather?: number;
    /**
     * PostgreSQL maximum predicate locks per transaction.
     */
    maxPredLocksPerTransaction?: number;
    /**
     * PostgreSQL maximum prepared transactions.
     */
    maxPreparedTransactions?: number;
    /**
     * PostgreSQL maximum replication slots.
     */
    maxReplicationSlots?: number;
    /**
     * PostgreSQL maximum WAL size (MB) reserved for replication slots. Default is -1 (unlimited). wal*keep*size minimum WAL size setting takes precedence over this.
     */
    maxSlotWalKeepSize?: number;
    /**
     * Maximum depth of the stack in bytes.
     */
    maxStackDepth?: number;
    /**
     * Max standby archive delay in milliseconds.
     */
    maxStandbyArchiveDelay?: number;
    /**
     * Max standby streaming delay in milliseconds.
     */
    maxStandbyStreamingDelay?: number;
    /**
     * PostgreSQL maximum WAL senders.
     */
    maxWalSenders?: number;
    /**
     * Sets the maximum number of background processes that the system can support.
     */
    maxWorkerProcesses?: number;
    /**
     * Sets the time interval to run pg_partman's scheduled tasks. Example: `3600`.
     */
    pgPartmanBgwDotInterval?: number;
    /**
     * Controls which role to use for pg_partman's scheduled background tasks. Example: `myrolename`.
     */
    pgPartmanBgwDotRole?: string;
    /**
     * Enables or disables query plan monitoring.
     */
    pgStatMonitorDotPgsmEnableQueryPlan?: boolean;
    /**
     * Sets the maximum number of buckets. Example: `10`.
     */
    pgStatMonitorDotPgsmMaxBuckets?: number;
    /**
     * Enum: `all`, `top`, `none`. Controls which statements are counted. Specify top to track top-level statements (those issued directly by clients), all to also track nested statements (such as statements invoked within functions), or none to disable statement statistics collection. The default value is top.
     */
    pgStatStatementsDotTrack?: string;
    /**
     * PostgreSQL temporary file limit in KiB, -1 for unlimited. Example: `5000000`.
     */
    tempFileLimit?: number;
    /**
     * PostgreSQL service timezone. Example: `Europe/Helsinki`.
     */
    timezone?: string;
    /**
     * Specifies the number of bytes reserved to track the currently executing command for each active session. Example: `1024`.
     */
    trackActivityQuerySize?: number;
    /**
     * Enum: `off`, `on`. Record commit time of transactions.
     */
    trackCommitTimestamp?: string;
    /**
     * Enum: `all`, `pl`, `none`. Enables tracking of function call counts and time used.
     */
    trackFunctions?: string;
    /**
     * Enum: `off`, `on`. Enables timing of database I/O calls. This parameter is off by default, because it will repeatedly query the operating system for the current time, which may cause significant overhead on some platforms.
     */
    trackIoTiming?: string;
    /**
     * Terminate replication connections that are inactive for longer than this amount of time, in milliseconds. Setting this value to zero disables the timeout. Example: `60000`.
     */
    walSenderTimeout?: number;
    /**
     * WAL flush interval in milliseconds. Note that setting this value to lower than the default 200ms may negatively impact performance. Example: `50`.
     */
    walWriterDelay?: number;
}

export interface PgPgUserConfigPgQualstats {
    /**
     * Enable / Disable pg_qualstats. Default: `false`.
     *
     * @deprecated This property is deprecated.
     */
    enabled?: boolean;
    /**
     * Error estimation num threshold to save quals. Default: `0`.
     *
     * @deprecated This property is deprecated.
     */
    minErrEstimateNum?: number;
    /**
     * Error estimation ratio threshold to save quals. Default: `0`.
     *
     * @deprecated This property is deprecated.
     */
    minErrEstimateRatio?: number;
    /**
     * Enable / Disable pgQualstats constants tracking. Default: `true`.
     *
     * @deprecated This property is deprecated.
     */
    trackConstants?: boolean;
    /**
     * Track quals on system catalogs too. Default: `false`.
     *
     * @deprecated This property is deprecated.
     */
    trackPgCatalog?: boolean;
}

export interface PgPgUserConfigPgaudit {
    /**
     * Enable pgaudit extension. When enabled, pgaudit extension will be automatically installed.Otherwise, extension will be uninstalled but auditing configurations will be preserved. Default: `false`.
     *
     * @deprecated This property is deprecated.
     */
    featureEnabled?: boolean;
    /**
     * Specifies that session logging should be enabled in the casewhere all relations in a statement are in pg_catalog. Default: `true`.
     *
     * @deprecated This property is deprecated.
     */
    logCatalog?: boolean;
    /**
     * Specifies whether log messages will be visible to a client process such as psql. Default: `false`.
     *
     * @deprecated This property is deprecated.
     */
    logClient?: boolean;
    /**
     * Enum: `debug1`, `debug2`, `debug3`, `debug4`, `debug5`, `info`, `notice`, `warning`, `log`. Specifies the log level that will be used for log entries. Default: `log`.
     *
     * @deprecated This property is deprecated.
     */
    logLevel?: string;
    /**
     * Crop parameters representation and whole statements if they exceed this threshold. A (default) value of -1 disable the truncation. Default: `-1`.
     *
     * @deprecated This property is deprecated.
     */
    logMaxStringLength?: number;
    /**
     * This GUC allows to turn off logging nested statements, that is, statements that are executed as part of another ExecutorRun. Default: `true`.
     *
     * @deprecated This property is deprecated.
     */
    logNestedStatements?: boolean;
    /**
     * Specifies that audit logging should include the parameters that were passed with the statement. Default: `false`.
     *
     * @deprecated This property is deprecated.
     */
    logParameter?: boolean;
    /**
     * Specifies that parameter values longer than this setting (in bytes) should not be logged, but replaced with \n\n. Default: `0`.
     *
     * @deprecated This property is deprecated.
     */
    logParameterMaxSize?: number;
    /**
     * Specifies whether session audit logging should create a separate log entry for each relation (TABLE, VIEW, etc.) referenced in a SELECT or DML statement. Default: `false`.
     *
     * @deprecated This property is deprecated.
     */
    logRelation?: boolean;
    /**
     * Specifies that audit logging should include the rows retrieved or affected by a statement. When enabled the rows field will be included after the parameter field. Default: `false`.
     *
     * @deprecated This property is deprecated.
     */
    logRows?: boolean;
    /**
     * Specifies whether logging will include the statement text and parameters (if enabled). Default: `true`.
     *
     * @deprecated This property is deprecated.
     */
    logStatement?: boolean;
    /**
     * Specifies whether logging will include the statement text and parameters with the first log entry for a statement/substatement combination or with every entry. Default: `false`.
     *
     * @deprecated This property is deprecated.
     */
    logStatementOnce?: boolean;
    /**
     * Specifies which classes of statements will be logged by session audit logging.
     *
     * @deprecated This property is deprecated.
     */
    logs?: string[];
    /**
     * Specifies the master role to use for object audit logging.
     *
     * @deprecated This property is deprecated.
     */
    role?: string;
}

export interface PgPgUserConfigPgbouncer {
    /**
     * If the automatically created database pools have been unused this many seconds, they are freed. If 0 then timeout is disabled. (seconds). Default: `3600`.
     */
    autodbIdleTimeout?: number;
    /**
     * Do not allow more than this many server connections per database (regardless of user). Setting it to 0 means unlimited. Example: `0`.
     */
    autodbMaxDbConnections?: number;
    /**
     * Enum: `session`, `transaction`, `statement`. PGBouncer pool mode. Default: `transaction`.
     */
    autodbPoolMode?: string;
    /**
     * If non-zero then create automatically a pool of that size per user when a pool doesn't exist. Default: `0`.
     */
    autodbPoolSize?: number;
    /**
     * List of parameters to ignore when given in startup packet.
     */
    ignoreStartupParameters?: string[];
    /**
     * PgBouncer tracks protocol-level named prepared statements related commands sent by the client in transaction and statement pooling modes when max*prepared*statements is set to a non-zero value. Setting it to 0 disables prepared statements. max*prepared*statements defaults to 100, and its maximum is 3000. Default: `100`.
     */
    maxPreparedStatements?: number;
    /**
     * Add more server connections to pool if below this number. Improves behavior when usual load comes suddenly back after period of total inactivity. The value is effectively capped at the pool size. Default: `0`.
     */
    minPoolSize?: number;
    /**
     * If a server connection has been idle more than this many seconds it will be dropped. If 0 then timeout is disabled. (seconds). Default: `600`.
     */
    serverIdleTimeout?: number;
    /**
     * The pooler will close an unused server connection that has been connected longer than this. (seconds). Default: `3600`.
     */
    serverLifetime?: number;
    /**
     * Run server*reset*query (DISCARD ALL) in all pooling modes. Default: `false`.
     */
    serverResetQueryAlways?: boolean;
}

export interface PgPgUserConfigPglookout {
    /**
     * Number of seconds of master unavailability before triggering database failover to standby. Default: `60`.
     */
    maxFailoverReplicationTimeLag?: number;
}

export interface PgPgUserConfigPrivateAccess {
    /**
     * Allow clients to connect to pg with a DNS name that always resolves to the service's private IP addresses. Only available in certain network locations.
     */
    pg?: boolean;
    /**
     * Allow clients to connect to pgbouncer with a DNS name that always resolves to the service's private IP addresses. Only available in certain network locations.
     */
    pgbouncer?: boolean;
    /**
     * Allow clients to connect to prometheus with a DNS name that always resolves to the service's private IP addresses. Only available in certain network locations.
     */
    prometheus?: boolean;
}

export interface PgPgUserConfigPrivatelinkAccess {
    /**
     * Enable pg.
     */
    pg?: boolean;
    /**
     * Enable pgbouncer.
     */
    pgbouncer?: boolean;
    /**
     * Enable prometheus.
     */
    prometheus?: boolean;
}

export interface PgPgUserConfigPublicAccess {
    /**
     * Allow clients to connect to pg from the public internet for service nodes that are in a project VPC or another type of private network.
     */
    pg?: boolean;
    /**
     * Allow clients to connect to pgbouncer from the public internet for service nodes that are in a project VPC or another type of private network.
     */
    pgbouncer?: boolean;
    /**
     * Allow clients to connect to prometheus from the public internet for service nodes that are in a project VPC or another type of private network.
     */
    prometheus?: boolean;
}

export interface PgPgUserConfigTimescaledb {
    /**
     * The number of background workers for timescaledb operations. You should configure this setting to the sum of your number of databases and the total number of concurrent background workers you want running at any given point in time. Default: `16`.
     */
    maxBackgroundWorkers?: number;
}

export interface PgServiceIntegration {
    /**
     * Type of the service integration. The only supported value at the moment is `readReplica`
     */
    integrationType: string;
    /**
     * Name of the source service
     */
    sourceServiceName: string;
}

export interface PgTag {
    /**
     * Service tag key
     */
    key: string;
    /**
     * Service tag value
     */
    value: string;
}

export interface PgTechEmail {
    /**
     * An email address to contact for technical issues
     */
    email: string;
}

export interface ProjectTag {
    /**
     * Project tag key.
     */
    key: string;
    /**
     * Project tag value.
     */
    value: string;
}

export interface RedisComponent {
    /**
     * Service component name
     */
    component: string;
    /**
     * Connection info for connecting to the service component. This is a combination of host and port.
     */
    connectionUri: string;
    /**
     * Host name for connecting to the service component
     */
    host: string;
    /**
     * Kafka authentication method. This is a value specific to the 'kafka' service component
     */
    kafkaAuthenticationMethod: string;
    /**
     * Port number for connecting to the service component
     */
    port: number;
    /**
     * Network access route
     */
    route: string;
    /**
     * Whether the endpoint is encrypted or accepts plaintext. By default endpoints are always encrypted and this property is only included for service components they may disable encryption
     */
    ssl: boolean;
    /**
     * DNS usage name
     */
    usage: string;
}

export interface RedisRedis {
    /**
     * Redis password.
     */
    password: string;
    /**
     * Redis replica server URI.
     */
    replicaUri: string;
    /**
     * Redis slave server URIs.
     */
    slaveUris: string[];
    /**
     * Redis server URIs.
     */
    uris: string[];
}

export interface RedisRedisUserConfig {
    /**
     * Additional Cloud Regions for Backup Replication.
     */
    additionalBackupRegions?: string;
    /**
     * The hour of day (in UTC) when backup for the service is started. New backup is only started if previous backup has already completed. Example: `3`.
     */
    backupHour?: number;
    /**
     * The minute of an hour when backup for the service is started. New backup is only started if previous backup has already completed. Example: `30`.
     */
    backupMinute?: number;
    /**
     * Allow incoming connections from CIDR address block, e.g. `10.20.0.0/16`
     */
    ipFilterObjects?: outputs.RedisRedisUserConfigIpFilterObject[];
    /**
     * Allow incoming connections from CIDR address block, e.g. `10.20.0.0/16`.
     */
    ipFilterStrings?: string[];
    /**
     * Allow incoming connections from CIDR address block, e.g. `10.20.0.0/16`.
     *
     * @deprecated Deprecated. Use `ipFilterString` instead.
     */
    ipFilters?: string[];
    /**
     * Migrate data from existing server
     */
    migration?: outputs.RedisRedisUserConfigMigration;
    /**
     * Allow access to selected service ports from private networks
     */
    privateAccess?: outputs.RedisRedisUserConfigPrivateAccess;
    /**
     * Allow access to selected service components through Privatelink
     */
    privatelinkAccess?: outputs.RedisRedisUserConfigPrivatelinkAccess;
    /**
     * Name of another project to fork a service from. This has effect only when a new service is being created. Example: `anotherprojectname`.
     */
    projectToForkFrom?: string;
    /**
     * Allow access to selected service ports from the public Internet
     */
    publicAccess?: outputs.RedisRedisUserConfigPublicAccess;
    /**
     * Name of the basebackup to restore in forked service. Example: `backup-20191112t091354293891z`.
     */
    recoveryBasebackupName?: string;
    /**
     * Enum: `allchannels`, `resetchannels`. Determines default pub/sub channels' ACL for new users if ACL is not supplied. When this option is not defined, allChannels is assumed to keep backward compatibility. This option doesn't affect Redis configuration acl-pubsub-default.
     */
    redisAclChannelsDefault?: string;
    /**
     * Set Redis IO thread count. Changing this will cause a restart of the Redis service. Example: `1`.
     */
    redisIoThreads?: number;
    /**
     * LFU maxmemory-policy counter decay time in minutes. Default: `1`.
     */
    redisLfuDecayTime?: number;
    /**
     * Counter logarithm factor for volatile-lfu and allkeys-lfu maxmemory-policies. Default: `10`.
     */
    redisLfuLogFactor?: number;
    /**
     * Enum: `noeviction`, `allkeys-lru`, `volatile-lru`, `allkeys-random`, `volatile-random`, `volatile-ttl`, `volatile-lfu`, `allkeys-lfu`. Redis maxmemory-policy. Default: `noeviction`.
     */
    redisMaxmemoryPolicy?: string;
    /**
     * Set notify-keyspace-events option.
     */
    redisNotifyKeyspaceEvents?: string;
    /**
     * Set number of Redis databases. Changing this will cause a restart of the Redis service. Example: `16`.
     */
    redisNumberOfDatabases?: number;
    /**
     * Enum: `off`, `rdb`. When persistence is `rdb`, Redis does RDB dumps each 10 minutes if any key is changed. Also RDB dumps are done according to the backup schedule for backup purposes. When persistence is `off`, no RDB dumps or backups are done, so data can be lost at any moment if the service is restarted for any reason, or if the service is powered off. Also, the service can't be forked.
     */
    redisPersistence?: string;
    /**
     * Set output buffer limit for pub / sub clients in MB. The value is the hard limit, the soft limit is 1/4 of the hard limit. When setting the limit, be mindful of the available memory in the selected service plan. Example: `64`.
     */
    redisPubsubClientOutputBufferLimit?: number;
    /**
     * Require SSL to access Redis. Default: `true`.
     */
    redisSsl?: boolean;
    /**
     * Redis idle connection timeout in seconds. Default: `300`.
     */
    redisTimeout?: number;
    /**
     * Enum: `7.0`, and newer. Redis major version.
     */
    redisVersion?: string;
    /**
     * Store logs for the service so that they are available in the HTTP API and console.
     */
    serviceLog?: boolean;
    /**
     * Name of another service to fork from. This has effect only when a new service is being created. Example: `anotherservicename`.
     */
    serviceToForkFrom?: string;
    /**
     * Use static public IP addresses.
     */
    staticIps?: boolean;
}

export interface RedisRedisUserConfigIpFilterObject {
    /**
     * Description for IP filter list entry. Example: `Production service IP range`.
     */
    description?: string;
    /**
     * CIDR address block. Example: `10.20.0.0/16`.
     */
    network: string;
}

export interface RedisRedisUserConfigMigration {
    /**
     * Database name for bootstrapping the initial connection. Example: `defaultdb`.
     */
    dbname?: string;
    /**
     * Hostname or IP address of the server where to migrate data from. Example: `my.server.com`.
     */
    host: string;
    /**
     * Comma-separated list of databases, which should be ignored during migration (supported by MySQL and PostgreSQL only at the moment). Example: `db1,db2`.
     */
    ignoreDbs?: string;
    /**
     * Comma-separated list of database roles, which should be ignored during migration (supported by PostgreSQL only at the moment). Example: `role1,role2`.
     */
    ignoreRoles?: string;
    /**
     * Enum: `dump`, `replication`. The migration method to be used (currently supported only by Redis, Dragonfly, MySQL and PostgreSQL service types).
     */
    method?: string;
    /**
     * Password for authentication with the server where to migrate data from. Example: `jjKk45Nnd`.
     */
    password?: string;
    /**
     * Port number of the server where to migrate data from. Example: `1234`.
     */
    port: number;
    /**
     * The server where to migrate data from is secured with SSL. Default: `true`.
     */
    ssl?: boolean;
    /**
     * User name for authentication with the server where to migrate data from. Example: `myname`.
     */
    username?: string;
}

export interface RedisRedisUserConfigPrivateAccess {
    /**
     * Allow clients to connect to prometheus with a DNS name that always resolves to the service's private IP addresses. Only available in certain network locations.
     */
    prometheus?: boolean;
    /**
     * Allow clients to connect to redis with a DNS name that always resolves to the service's private IP addresses. Only available in certain network locations.
     */
    redis?: boolean;
}

export interface RedisRedisUserConfigPrivatelinkAccess {
    /**
     * Enable prometheus.
     */
    prometheus?: boolean;
    /**
     * Enable redis.
     */
    redis?: boolean;
}

export interface RedisRedisUserConfigPublicAccess {
    /**
     * Allow clients to connect to prometheus from the public internet for service nodes that are in a project VPC or another type of private network.
     */
    prometheus?: boolean;
    /**
     * Allow clients to connect to redis from the public internet for service nodes that are in a project VPC or another type of private network.
     */
    redis?: boolean;
}

export interface RedisServiceIntegration {
    /**
     * Type of the service integration. The only supported value at the moment is `readReplica`
     */
    integrationType: string;
    /**
     * Name of the source service
     */
    sourceServiceName: string;
}

export interface RedisTag {
    /**
     * Service tag key
     */
    key: string;
    /**
     * Service tag value
     */
    value: string;
}

export interface RedisTechEmail {
    /**
     * An email address to contact for technical issues
     */
    email: string;
}

export interface ServiceIntegrationClickhouseKafkaUserConfig {
    /**
     * Tables to create
     */
    tables?: outputs.ServiceIntegrationClickhouseKafkaUserConfigTable[];
}

export interface ServiceIntegrationClickhouseKafkaUserConfigTable {
    /**
     * Enum: `smallest`, `earliest`, `beginning`, `largest`, `latest`, `end`. Action to take when there is no initial offset in offset store or the desired offset is out of range. Default: `earliest`.
     */
    autoOffsetReset?: string;
    /**
     * Table columns
     */
    columns: outputs.ServiceIntegrationClickhouseKafkaUserConfigTableColumn[];
    /**
     * Enum: `Avro`, `CSV`, `JSONAsString`, `JSONCompactEachRow`, `JSONCompactStringsEachRow`, `JSONEachRow`, `JSONStringsEachRow`, `MsgPack`, `TSKV`, `TSV`, `TabSeparated`, `RawBLOB`, `AvroConfluent`, `Parquet`. Message data format. Default: `JSONEachRow`.
     */
    dataFormat: string;
    /**
     * Enum: `basic`, `bestEffort`, `bestEffortUs`. Method to read DateTime from text input formats. Default: `basic`.
     */
    dateTimeInputFormat?: string;
    /**
     * Kafka consumers group. Default: `clickhouse`.
     */
    groupName: string;
    /**
     * Enum: `default`, `stream`. How to handle errors for Kafka engine. Default: `default`.
     */
    handleErrorMode?: string;
    /**
     * Number of row collected by poll(s) for flushing data from Kafka. Default: `0`.
     */
    maxBlockSize?: number;
    /**
     * The maximum number of rows produced in one kafka message for row-based formats. Default: `1`.
     */
    maxRowsPerMessage?: number;
    /**
     * Name of the table. Example: `events`.
     */
    name: string;
    /**
     * The number of consumers per table per replica. Default: `1`.
     */
    numConsumers?: number;
    /**
     * Maximum amount of messages to be polled in a single Kafka poll. Default: `0`.
     */
    pollMaxBatchSize?: number;
    /**
     * Timeout in milliseconds for a single poll from Kafka. Takes the value of the stream*flush*interval_ms server setting by default (500ms). Default: `0`.
     */
    pollMaxTimeoutMs?: number;
    /**
     * Skip at least this number of broken messages from Kafka topic per block. Default: `0`.
     */
    skipBrokenMessages?: number;
    /**
     * Provide an independent thread for each consumer. All consumers run in the same thread by default. Default: `false`.
     */
    threadPerConsumer?: boolean;
    /**
     * Kafka topics
     */
    topics: outputs.ServiceIntegrationClickhouseKafkaUserConfigTableTopic[];
}

export interface ServiceIntegrationClickhouseKafkaUserConfigTableColumn {
    /**
     * Column name. Example: `key`.
     */
    name: string;
    /**
     * Column type. Example: `UInt64`.
     */
    type: string;
}

export interface ServiceIntegrationClickhouseKafkaUserConfigTableTopic {
    /**
     * Name of the topic. Example: `topicName`.
     */
    name: string;
}

export interface ServiceIntegrationClickhousePostgresqlUserConfig {
    /**
     * Databases to expose
     */
    databases?: outputs.ServiceIntegrationClickhousePostgresqlUserConfigDatabase[];
}

export interface ServiceIntegrationClickhousePostgresqlUserConfigDatabase {
    /**
     * PostgreSQL database to expose. Default: `defaultdb`.
     */
    database?: string;
    /**
     * PostgreSQL schema to expose. Default: `public`.
     */
    schema?: string;
}

export interface ServiceIntegrationDatadogUserConfig {
    /**
     * Enable Datadog Database Monitoring.
     */
    datadogDbmEnabled?: boolean;
    /**
     * Enable Datadog PgBouncer Metric Tracking.
     */
    datadogPgbouncerEnabled?: boolean;
    /**
     * Custom tags provided by user
     */
    datadogTags?: outputs.ServiceIntegrationDatadogUserConfigDatadogTag[];
    /**
     * List of custom metrics.
     */
    excludeConsumerGroups?: string[];
    /**
     * List of topics to exclude.
     */
    excludeTopics?: string[];
    /**
     * List of custom metrics.
     */
    includeConsumerGroups?: string[];
    /**
     * List of topics to include.
     */
    includeTopics?: string[];
    /**
     * List of custom metrics.
     */
    kafkaCustomMetrics?: string[];
    /**
     * Maximum number of JMX metrics to send. Example: `2000`.
     */
    maxJmxMetrics?: number;
    /**
     * List of custom metrics.
     */
    mirrormakerCustomMetrics?: string[];
    /**
     * Datadog Opensearch Options
     */
    opensearch?: outputs.ServiceIntegrationDatadogUserConfigOpensearch;
    /**
     * Datadog Redis Options
     */
    redis?: outputs.ServiceIntegrationDatadogUserConfigRedis;
}

export interface ServiceIntegrationDatadogUserConfigDatadogTag {
    /**
     * Optional tag explanation. Example: `Used to tag primary replica metrics`.
     */
    comment?: string;
    /**
     * Tag format and usage are described here: https://docs.datadoghq.com/getting_started/tagging. Tags with prefix `aiven-` are reserved for Aiven. Example: `replica:primary`.
     */
    tag: string;
}

export interface ServiceIntegrationDatadogUserConfigOpensearch {
    /**
     * Enable Datadog Opensearch Cluster Monitoring.
     */
    clusterStatsEnabled?: boolean;
    /**
     * Enable Datadog Opensearch Index Monitoring.
     */
    indexStatsEnabled?: boolean;
    /**
     * Enable Datadog Opensearch Pending Task Monitoring.
     */
    pendingTaskStatsEnabled?: boolean;
    /**
     * Enable Datadog Opensearch Primary Shard Monitoring.
     */
    pshardStatsEnabled?: boolean;
}

export interface ServiceIntegrationDatadogUserConfigRedis {
    /**
     * Enable commandStats option in the agent's configuration. Default: `false`.
     */
    commandStatsEnabled?: boolean;
}

export interface ServiceIntegrationEndpointDatadogUserConfig {
    /**
     * Datadog API key. Example: `848f30907c15c55d601fe45487cce9b6`.
     */
    datadogApiKey: string;
    /**
     * Custom tags provided by user
     */
    datadogTags?: outputs.ServiceIntegrationEndpointDatadogUserConfigDatadogTag[];
    /**
     * Disable consumer group metrics.
     */
    disableConsumerStats?: boolean;
    /**
     * Number of separate instances to fetch kafka consumer statistics with. Example: `8`.
     */
    kafkaConsumerCheckInstances?: number;
    /**
     * Number of seconds that datadog will wait to get consumer statistics from brokers. Example: `60`.
     */
    kafkaConsumerStatsTimeout?: number;
    /**
     * Maximum number of partition contexts to send. Example: `32000`.
     */
    maxPartitionContexts?: number;
    /**
     * Enum: `datadoghq.com`, `datadoghq.eu`, `us3.datadoghq.com`, `us5.datadoghq.com`, `ddog-gov.com`, `ap1.datadoghq.com`. Datadog intake site. Defaults to datadoghq.com.
     */
    site?: string;
}

export interface ServiceIntegrationEndpointDatadogUserConfigDatadogTag {
    /**
     * Optional tag explanation. Example: `Used to tag primary replica metrics`.
     */
    comment?: string;
    /**
     * Tag format and usage are described here: https://docs.datadoghq.com/getting_started/tagging. Tags with prefix `aiven-` are reserved for Aiven. Example: `replica:primary`.
     */
    tag: string;
}

export interface ServiceIntegrationEndpointExternalAwsCloudwatchLogsUserConfig {
    /**
     * AWS access key. Required permissions are logs:CreateLogGroup, logs:CreateLogStream, logs:PutLogEvents and logs:DescribeLogStreams. Example: `AAAAAAAAAAAAAAAAAAAA`.
     */
    accessKey: string;
    /**
     * AWS CloudWatch log group name. Example: `my-log-group`.
     */
    logGroupName?: string;
    /**
     * AWS region. Example: `us-east-1`.
     */
    region: string;
    /**
     * AWS secret key. Example: `AAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAA`.
     */
    secretKey: string;
}

export interface ServiceIntegrationEndpointExternalAwsCloudwatchMetricsUserConfig {
    /**
     * AWS access key. Required permissions are cloudwatch:PutMetricData. Example: `AAAAAAAAAAAAAAAAAAAA`.
     */
    accessKey: string;
    /**
     * AWS CloudWatch Metrics Namespace. Example: `my-metrics-namespace`.
     */
    namespace: string;
    /**
     * AWS region. Example: `us-east-1`.
     */
    region: string;
    /**
     * AWS secret key. Example: `AAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAA`.
     */
    secretKey: string;
}

export interface ServiceIntegrationEndpointExternalAwsS3UserConfig {
    /**
     * Access Key Id. Example: `AAAAAAAAAAAAAAAAAAA`.
     */
    accessKeyId: string;
    /**
     * Secret Access Key. Example: `AAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAA`.
     */
    secretAccessKey: string;
    /**
     * S3-compatible bucket URL. Example: `https://mybucket.s3-myregion.amazonaws.com/mydataset/`.
     */
    url: string;
}

export interface ServiceIntegrationEndpointExternalClickhouseUserConfig {
    /**
     * Hostname or IP address of the server. Example: `my.server.com`.
     */
    host: string;
    /**
     * Password. Example: `jjKk45Nnd`.
     */
    password: string;
    /**
     * Secure TCP server port. Example: `9440`.
     */
    port: number;
    /**
     * User name. Example: `default`.
     */
    username: string;
}

export interface ServiceIntegrationEndpointExternalElasticsearchLogsUserConfig {
    /**
     * PEM encoded CA certificate. Example: `-----BEGIN CERTIFICATE-----
     * ...
     * -----END CERTIFICATE-----
     * `.
     */
    ca?: string;
    /**
     * Maximum number of days of logs to keep. Default: `3`.
     */
    indexDaysMax?: number;
    /**
     * Elasticsearch index prefix. Default: `logs`.
     */
    indexPrefix: string;
    /**
     * Elasticsearch request timeout limit. Default: `10.0`.
     */
    timeout?: number;
    /**
     * Elasticsearch connection URL. Example: `https://user:passwd@logs.example.com/`.
     */
    url: string;
}

export interface ServiceIntegrationEndpointExternalGoogleCloudBigquery {
    /**
     * GCP project id. Example: `snappy-photon-12345`.
     */
    projectId: string;
    /**
     * This is a JSON object with the fields documented in https://cloud.google.com/iam/docs/creating-managing-service-account-keys. Example: `{"type": "serviceAccount", ...`.
     */
    serviceAccountCredentials: string;
}

export interface ServiceIntegrationEndpointExternalGoogleCloudLoggingUserConfig {
    /**
     * Google Cloud Logging log id. Example: `syslog`.
     */
    logId: string;
    /**
     * GCP project id. Example: `snappy-photon-12345`.
     */
    projectId: string;
    /**
     * This is a JSON object with the fields documented in https://cloud.google.com/iam/docs/creating-managing-service-account-keys. Example: `{"type": "serviceAccount", ...`.
     */
    serviceAccountCredentials: string;
}

export interface ServiceIntegrationEndpointExternalKafkaUserConfig {
    /**
     * Bootstrap servers. Example: `10.0.0.1:9092,10.0.0.2:9092`.
     */
    bootstrapServers: string;
    /**
     * Enum: `PLAIN`, `SCRAM-SHA-256`, `SCRAM-SHA-512`. SASL mechanism used for connections to the Kafka server.
     */
    saslMechanism?: string;
    /**
     * Password for SASL PLAIN mechanism in the Kafka server. Example: `admin`.
     */
    saslPlainPassword?: string;
    /**
     * Username for SASL PLAIN mechanism in the Kafka server. Example: `admin`.
     */
    saslPlainUsername?: string;
    /**
     * Enum: `PLAINTEXT`, `SSL`, `SASL_PLAINTEXT`, `SASL_SSL`. Security protocol.
     */
    securityProtocol: string;
    /**
     * PEM-encoded CA certificate. Example: `-----BEGIN CERTIFICATE-----
     * ...
     * -----END CERTIFICATE-----
     * `.
     */
    sslCaCert?: string;
    /**
     * PEM-encoded client certificate. Example: `-----BEGIN CERTIFICATE-----
     * ...
     * -----END CERTIFICATE-----
     * `.
     */
    sslClientCert?: string;
    /**
     * PEM-encoded client key. Example: `-----BEGIN PRIVATE KEY-----
     * ...
     * -----END PRIVATE KEY-----
     * `.
     */
    sslClientKey?: string;
    /**
     * Enum: `https`. The endpoint identification algorithm to validate server hostname using server certificate.
     */
    sslEndpointIdentificationAlgorithm?: string;
}

export interface ServiceIntegrationEndpointExternalMysqlUserConfig {
    /**
     * Hostname or IP address of the server. Example: `my.server.com`.
     */
    host: string;
    /**
     * Password. Example: `jjKk45Nnd`.
     */
    password: string;
    /**
     * Port number of the server. Example: `5432`.
     */
    port: number;
    /**
     * Enum: `verify-full`. SSL Mode. Default: `verify-full`.
     */
    sslMode?: string;
    /**
     * SSL Root Cert. Example: `-----BEGIN CERTIFICATE-----
     * ...
     * -----END CERTIFICATE-----
     * `.
     */
    sslRootCert?: string;
    /**
     * User name. Example: `myname`.
     */
    username: string;
}

export interface ServiceIntegrationEndpointExternalOpensearchLogsUserConfig {
    /**
     * PEM encoded CA certificate. Example: `-----BEGIN CERTIFICATE-----
     * ...
     * -----END CERTIFICATE-----
     * `.
     */
    ca?: string;
    /**
     * Maximum number of days of logs to keep. Default: `3`.
     */
    indexDaysMax?: number;
    /**
     * OpenSearch index prefix. Default: `logs`.
     */
    indexPrefix: string;
    /**
     * OpenSearch request timeout limit. Default: `10.0`.
     */
    timeout?: number;
    /**
     * OpenSearch connection URL. Example: `https://user:passwd@logs.example.com/`.
     */
    url: string;
}

export interface ServiceIntegrationEndpointExternalPostgresql {
    /**
     * Default database. Example: `testdb`.
     */
    defaultDatabase?: string;
    /**
     * Hostname or IP address of the server. Example: `my.server.com`.
     */
    host: string;
    /**
     * Password. Example: `jjKk45Nnd`.
     */
    password?: string;
    /**
     * Port number of the server. Example: `5432`.
     */
    port: number;
    /**
     * Client certificate. Example: `-----BEGIN CERTIFICATE-----
     * ...
     * -----END CERTIFICATE-----
     * `.
     */
    sslClientCertificate?: string;
    /**
     * Client key. Example: `-----BEGIN PRIVATE KEY-----
     * ...
     * -----END PRIVATE KEY-----`.
     */
    sslClientKey?: string;
    /**
     * Enum: `disable`, `allow`, `prefer`, `require`, `verify-ca`, `verify-full`. SSL mode to use for the connection.  Please note that Aiven requires TLS for all connections to external PostgreSQL services. Default: `verify-full`.
     */
    sslMode?: string;
    /**
     * SSL Root Cert. Example: `-----BEGIN CERTIFICATE-----
     * ...
     * -----END CERTIFICATE-----
     * `.
     */
    sslRootCert?: string;
    /**
     * User name. Example: `myname`.
     */
    username: string;
}

export interface ServiceIntegrationEndpointExternalSchemaRegistryUserConfig {
    /**
     * Enum: `none`, `basic`. Authentication method.
     */
    authentication: string;
    /**
     * Basic authentication password. Example: `Zm9vYg==`.
     */
    basicAuthPassword?: string;
    /**
     * Basic authentication user name. Example: `avnadmin`.
     */
    basicAuthUsername?: string;
    /**
     * Schema Registry URL. Example: `https://schema-registry.kafka.company.com:28419`.
     */
    url: string;
}

export interface ServiceIntegrationEndpointJolokiaUserConfig {
    /**
     * Jolokia basic authentication password. Example: `yhfBNFii4C`.
     */
    basicAuthPassword?: string;
    /**
     * Jolokia basic authentication username. Example: `jol48k51`.
     */
    basicAuthUsername?: string;
}

export interface ServiceIntegrationEndpointPrometheusUserConfig {
    /**
     * Prometheus basic authentication password. Example: `fhyFNBjj3R`.
     */
    basicAuthPassword?: string;
    /**
     * Prometheus basic authentication username. Example: `prom4851`.
     */
    basicAuthUsername?: string;
}

export interface ServiceIntegrationEndpointRsyslogUserConfig {
    /**
     * PEM encoded CA certificate. Example: `-----BEGIN CERTIFICATE-----
     * ...
     * -----END CERTIFICATE-----
     * `.
     */
    ca?: string;
    /**
     * PEM encoded client certificate. Example: `-----BEGIN CERTIFICATE-----
     * ...
     * -----END CERTIFICATE-----
     * `.
     */
    cert?: string;
    /**
     * Enum: `rfc5424`, `rfc3164`, `custom`. Message format. Default: `rfc5424`.
     */
    format: string;
    /**
     * PEM encoded client key. Example: `-----BEGIN PRIVATE KEY-----
     * ...
     * -----END PRIVATE KEY-----
     * `.
     */
    key?: string;
    /**
     * Custom syslog message format. Example: `<%pri%>%timestamp:::date-rfc3339% %HOSTNAME% %app-name% %msg%`.
     */
    logline?: string;
    /**
     * Rsyslog max message size. Default: `8192`.
     */
    maxMessageSize?: number;
    /**
     * Rsyslog server port. Default: `514`.
     */
    port: number;
    /**
     * Structured data block for log message. Example: `TOKEN tag="LiteralValue"`.
     */
    sd?: string;
    /**
     * Rsyslog server IP address or hostname. Example: `logs.example.com`.
     */
    server: string;
    /**
     * Require TLS. Default: `true`.
     */
    tls: boolean;
}

export interface ServiceIntegrationExternalAwsCloudwatchLogsUserConfig {
    /**
     * The list of logging fields that will be sent to the integration logging service. The MESSAGE and timestamp fields are always sent.
     */
    selectedLogFields?: string[];
}

export interface ServiceIntegrationExternalAwsCloudwatchMetricsUserConfig {
    /**
     * Metrics to not send to AWS CloudWatch (takes precedence over extra*metrics)
     */
    droppedMetrics?: outputs.ServiceIntegrationExternalAwsCloudwatchMetricsUserConfigDroppedMetric[];
    /**
     * Metrics to allow through to AWS CloudWatch (in addition to default metrics)
     */
    extraMetrics?: outputs.ServiceIntegrationExternalAwsCloudwatchMetricsUserConfigExtraMetric[];
}

export interface ServiceIntegrationExternalAwsCloudwatchMetricsUserConfigDroppedMetric {
    /**
     * Identifier of a value in the metric. Example: `used`.
     */
    field: string;
    /**
     * Identifier of the metric. Example: `java.lang:Memory`.
     */
    metric: string;
}

export interface ServiceIntegrationExternalAwsCloudwatchMetricsUserConfigExtraMetric {
    /**
     * Identifier of a value in the metric. Example: `used`.
     */
    field: string;
    /**
     * Identifier of the metric. Example: `java.lang:Memory`.
     */
    metric: string;
}

export interface ServiceIntegrationExternalElasticsearchLogsUserConfig {
    /**
     * The list of logging fields that will be sent to the integration logging service. The MESSAGE and timestamp fields are always sent.
     */
    selectedLogFields?: string[];
}

export interface ServiceIntegrationExternalOpensearchLogsUserConfig {
    /**
     * The list of logging fields that will be sent to the integration logging service. The MESSAGE and timestamp fields are always sent.
     */
    selectedLogFields?: string[];
}

export interface ServiceIntegrationKafkaConnectUserConfig {
    /**
     * Kafka Connect service configuration values
     */
    kafkaConnect?: outputs.ServiceIntegrationKafkaConnectUserConfigKafkaConnect;
}

export interface ServiceIntegrationKafkaConnectUserConfigKafkaConnect {
    /**
     * The name of the topic where connector and task configuration data are stored.This must be the same for all workers with the same group_id. Example: `__connect_configs`.
     */
    configStorageTopic?: string;
    /**
     * A unique string that identifies the Connect cluster group this worker belongs to. Example: `connect`.
     */
    groupId?: string;
    /**
     * The name of the topic where connector and task configuration offsets are stored.This must be the same for all workers with the same group_id. Example: `__connect_offsets`.
     */
    offsetStorageTopic?: string;
    /**
     * The name of the topic where connector and task configuration status updates are stored.This must be the same for all workers with the same group_id. Example: `__connect_status`.
     */
    statusStorageTopic?: string;
}

export interface ServiceIntegrationKafkaLogsUserConfig {
    /**
     * Topic name. Example: `mytopic`.
     */
    kafkaTopic: string;
    /**
     * The list of logging fields that will be sent to the integration logging service. The MESSAGE and timestamp fields are always sent.
     */
    selectedLogFields?: string[];
}

export interface ServiceIntegrationKafkaMirrormakerUserConfig {
    /**
     * The alias under which the Kafka cluster is known to MirrorMaker. Can contain the following symbols: ASCII alphanumerics, `.`, `_`, and `-`. Example: `kafka-abc`.
     */
    clusterAlias?: string;
    /**
     * Kafka MirrorMaker configuration values
     */
    kafkaMirrormaker?: outputs.ServiceIntegrationKafkaMirrormakerUserConfigKafkaMirrormaker;
}

export interface ServiceIntegrationKafkaMirrormakerUserConfigKafkaMirrormaker {
    /**
     * Enum: `earliest`, `latest`. Set where consumer starts to consume data. Value `earliest`: Start replication from the earliest offset. Value `latest`: Start replication from the latest offset. Default is `earliest`.
     */
    consumerAutoOffsetReset?: string;
    /**
     * The minimum amount of data the server should return for a fetch request. Example: `1024`.
     */
    consumerFetchMinBytes?: number;
    /**
     * Set consumer max.poll.records. The default is 500. Example: `500`.
     */
    consumerMaxPollRecords?: number;
    /**
     * The batch size in bytes producer will attempt to collect before publishing to broker. Example: `1024`.
     */
    producerBatchSize?: number;
    /**
     * The amount of bytes producer can use for buffering data before publishing to broker. Example: `8388608`.
     */
    producerBufferMemory?: number;
    /**
     * Enum: `gzip`, `snappy`, `lz4`, `zstd`, `none`. Specify the default compression type for producers. This configuration accepts the standard compression codecs (`gzip`, `snappy`, `lz4`, `zstd`). It additionally accepts `none` which is the default and equivalent to no compression.
     */
    producerCompressionType?: string;
    /**
     * The linger time (ms) for waiting new data to arrive for publishing. Example: `100`.
     */
    producerLingerMs?: number;
    /**
     * The maximum request size in bytes. Example: `1048576`.
     */
    producerMaxRequestSize?: number;
}

export interface ServiceIntegrationLogsUserConfig {
    /**
     * Elasticsearch index retention limit. Default: `3`.
     */
    elasticsearchIndexDaysMax?: number;
    /**
     * Elasticsearch index prefix. Default: `logs`.
     */
    elasticsearchIndexPrefix?: string;
    /**
     * The list of logging fields that will be sent to the integration logging service. The MESSAGE and timestamp fields are always sent.
     */
    selectedLogFields?: string[];
}

export interface ServiceIntegrationMetricsUserConfig {
    /**
     * Name of the database where to store metric datapoints. Only affects PostgreSQL destinations. Defaults to `metrics`. Note that this must be the same for all metrics integrations that write data to the same PostgreSQL service.
     */
    database?: string;
    /**
     * Number of days to keep old metrics. Only affects PostgreSQL destinations. Set to 0 for no automatic cleanup. Defaults to 30 days.
     */
    retentionDays?: number;
    /**
     * Name of a user that can be used to read metrics. This will be used for Grafana integration (if enabled) to prevent Grafana users from making undesired changes. Only affects PostgreSQL destinations. Defaults to `metricsReader`. Note that this must be the same for all metrics integrations that write data to the same PostgreSQL service.
     */
    roUsername?: string;
    /**
     * Configuration options for metrics where source service is MySQL
     */
    sourceMysql?: outputs.ServiceIntegrationMetricsUserConfigSourceMysql;
    /**
     * Name of the user used to write metrics. Only affects PostgreSQL destinations. Defaults to `metricsWriter`. Note that this must be the same for all metrics integrations that write data to the same PostgreSQL service.
     */
    username?: string;
}

export interface ServiceIntegrationMetricsUserConfigSourceMysql {
    /**
     * Configuration options for Telegraf MySQL input plugin
     */
    telegraf?: outputs.ServiceIntegrationMetricsUserConfigSourceMysqlTelegraf;
}

export interface ServiceIntegrationMetricsUserConfigSourceMysqlTelegraf {
    /**
     * Gather metrics from PERFORMANCE*SCHEMA.EVENT*WAITS.
     */
    gatherEventWaits?: boolean;
    /**
     * Gather metrics from PERFORMANCE*SCHEMA.FILE*SUMMARY*BY*EVENT_NAME.
     */
    gatherFileEventsStats?: boolean;
    /**
     * Gather metrics from PERFORMANCE*SCHEMA.TABLE*IO*WAITS*SUMMARY*BY*INDEX_USAGE.
     */
    gatherIndexIoWaits?: boolean;
    /**
     * Gather autoIncrement columns and max values from information schema.
     */
    gatherInfoSchemaAutoInc?: boolean;
    /**
     * Gather metrics from INFORMATION*SCHEMA.INNODB*METRICS.
     */
    gatherInnodbMetrics?: boolean;
    /**
     * Gather metrics from PERFORMANCE*SCHEMA.EVENTS*STATEMENTS*SUMMARY*BY_DIGEST.
     */
    gatherPerfEventsStatements?: boolean;
    /**
     * Gather thread state counts from INFORMATION_SCHEMA.PROCESSLIST.
     */
    gatherProcessList?: boolean;
    /**
     * Gather metrics from SHOW SLAVE STATUS command output.
     */
    gatherSlaveStatus?: boolean;
    /**
     * Gather metrics from PERFORMANCE*SCHEMA.TABLE*IO*WAITS*SUMMARY*BY*TABLE.
     */
    gatherTableIoWaits?: boolean;
    /**
     * Gather metrics from PERFORMANCE*SCHEMA.TABLE*LOCK_WAITS.
     */
    gatherTableLockWaits?: boolean;
    /**
     * Gather metrics from INFORMATION_SCHEMA.TABLES.
     */
    gatherTableSchema?: boolean;
    /**
     * Truncates digest text from perf*events*statements into this many characters. Example: `120`.
     */
    perfEventsStatementsDigestTextLimit?: number;
    /**
     * Limits metrics from perf*events*statements. Example: `250`.
     */
    perfEventsStatementsLimit?: number;
    /**
     * Only include perf*events*statements whose last seen is less than this many seconds. Example: `86400`.
     */
    perfEventsStatementsTimeLimit?: number;
}

export interface ServiceIntegrationPrometheusUserConfig {
    /**
     * Configuration options for metrics where source service is MySQL
     */
    sourceMysql?: outputs.ServiceIntegrationPrometheusUserConfigSourceMysql;
}

export interface ServiceIntegrationPrometheusUserConfigSourceMysql {
    /**
     * Configuration options for Telegraf MySQL input plugin
     */
    telegraf?: outputs.ServiceIntegrationPrometheusUserConfigSourceMysqlTelegraf;
}

export interface ServiceIntegrationPrometheusUserConfigSourceMysqlTelegraf {
    /**
     * Gather metrics from PERFORMANCE*SCHEMA.EVENT*WAITS.
     */
    gatherEventWaits?: boolean;
    /**
     * Gather metrics from PERFORMANCE*SCHEMA.FILE*SUMMARY*BY*EVENT_NAME.
     */
    gatherFileEventsStats?: boolean;
    /**
     * Gather metrics from PERFORMANCE*SCHEMA.TABLE*IO*WAITS*SUMMARY*BY*INDEX_USAGE.
     */
    gatherIndexIoWaits?: boolean;
    /**
     * Gather autoIncrement columns and max values from information schema.
     */
    gatherInfoSchemaAutoInc?: boolean;
    /**
     * Gather metrics from INFORMATION*SCHEMA.INNODB*METRICS.
     */
    gatherInnodbMetrics?: boolean;
    /**
     * Gather metrics from PERFORMANCE*SCHEMA.EVENTS*STATEMENTS*SUMMARY*BY_DIGEST.
     */
    gatherPerfEventsStatements?: boolean;
    /**
     * Gather thread state counts from INFORMATION_SCHEMA.PROCESSLIST.
     */
    gatherProcessList?: boolean;
    /**
     * Gather metrics from SHOW SLAVE STATUS command output.
     */
    gatherSlaveStatus?: boolean;
    /**
     * Gather metrics from PERFORMANCE*SCHEMA.TABLE*IO*WAITS*SUMMARY*BY*TABLE.
     */
    gatherTableIoWaits?: boolean;
    /**
     * Gather metrics from PERFORMANCE*SCHEMA.TABLE*LOCK_WAITS.
     */
    gatherTableLockWaits?: boolean;
    /**
     * Gather metrics from INFORMATION_SCHEMA.TABLES.
     */
    gatherTableSchema?: boolean;
    /**
     * Truncates digest text from perf*events*statements into this many characters. Example: `120`.
     */
    perfEventsStatementsDigestTextLimit?: number;
    /**
     * Limits metrics from perf*events*statements. Example: `250`.
     */
    perfEventsStatementsLimit?: number;
    /**
     * Only include perf*events*statements whose last seen is less than this many seconds. Example: `86400`.
     */
    perfEventsStatementsTimeLimit?: number;
}

export interface ThanosComponent {
    /**
     * Service component name
     */
    component: string;
    /**
     * Connection info for connecting to the service component. This is a combination of host and port.
     */
    connectionUri: string;
    /**
     * Host name for connecting to the service component
     */
    host: string;
    /**
     * Kafka authentication method. This is a value specific to the 'kafka' service component
     */
    kafkaAuthenticationMethod: string;
    /**
     * Port number for connecting to the service component
     */
    port: number;
    /**
     * Network access route
     */
    route: string;
    /**
     * Whether the endpoint is encrypted or accepts plaintext. By default endpoints are always encrypted and this property is only included for service components they may disable encryption
     */
    ssl: boolean;
    /**
     * DNS usage name
     */
    usage: string;
}

export interface ThanosServiceIntegration {
    /**
     * Type of the service integration. The only supported value at the moment is `readReplica`
     */
    integrationType: string;
    /**
     * Name of the source service
     */
    sourceServiceName: string;
}

export interface ThanosTag {
    /**
     * Service tag key
     */
    key: string;
    /**
     * Service tag value
     */
    value: string;
}

export interface ThanosTechEmail {
    /**
     * An email address to contact for technical issues
     */
    email: string;
}

export interface ThanosThanos {
    /**
     * Query frontend URI.
     */
    queryFrontendUri: string;
    /**
     * Query URI.
     */
    queryUri: string;
    /**
     * Receiver ingesting remote write URI.
     */
    receiverIngestingRemoteWriteUri: string;
    /**
     * Receiver remote write URI.
     */
    receiverRemoteWriteUri: string;
    /**
     * Store URI.
     *
     * @deprecated This field was added by mistake and has never worked. It will be removed in future versions.
     */
    storeUri: string;
    /**
     * Thanos server URIs.
     */
    uris: string[];
}

export interface ThanosThanosUserConfig {
    /**
     * ThanosCompactor
     */
    compactor?: outputs.ThanosThanosUserConfigCompactor;
    /**
     * Environmental variables.
     */
    env?: {[key: string]: string};
    /**
     * Allow incoming connections from CIDR address block, e.g. `10.20.0.0/16`
     */
    ipFilterObjects?: outputs.ThanosThanosUserConfigIpFilterObject[];
    /**
     * Allow incoming connections from CIDR address block, e.g. `10.20.0.0/16`.
     */
    ipFilterStrings?: string[];
    /**
     * Allow incoming connections from CIDR address block, e.g. `10.20.0.0/16`.
     *
     * @deprecated Deprecated. Use `ipFilterString` instead.
     */
    ipFilters?: string[];
    /**
     * After exceeding the limit a service alert is going to be raised (0 means not set).
     */
    objectStorageUsageAlertThresholdGb?: number;
    /**
     * Allow access to selected service ports from the public Internet
     */
    publicAccess?: outputs.ThanosThanosUserConfigPublicAccess;
    /**
     * ThanosQuery
     */
    query?: outputs.ThanosThanosUserConfigQuery;
    /**
     * ThanosQueryFrontend
     */
    queryFrontend?: outputs.ThanosThanosUserConfigQueryFrontend;
    /**
     * Store logs for the service so that they are available in the HTTP API and console.
     */
    serviceLog?: boolean;
    /**
     * Use static public IP addresses.
     */
    staticIps?: boolean;
}

export interface ThanosThanosUserConfigCompactor {
    /**
     * Retention time for data in days for each resolution (5m, 1h, raw).
     */
    retentionDays?: number;
}

export interface ThanosThanosUserConfigIpFilterObject {
    /**
     * Description for IP filter list entry. Example: `Production service IP range`.
     */
    description?: string;
    /**
     * CIDR address block. Example: `10.20.0.0/16`.
     */
    network: string;
}

export interface ThanosThanosUserConfigPublicAccess {
    /**
     * Allow clients to connect to compactor from the public internet for service nodes that are in a project VPC or another type of private network.
     */
    compactor?: boolean;
    /**
     * Allow clients to connect to query from the public internet for service nodes that are in a project VPC or another type of private network.
     */
    query?: boolean;
    /**
     * Allow clients to connect to queryFrontend from the public internet for service nodes that are in a project VPC or another type of private network.
     */
    queryFrontend?: boolean;
    /**
     * Allow clients to connect to receiverIngesting from the public internet for service nodes that are in a project VPC or another type of private network.
     */
    receiverIngesting?: boolean;
    /**
     * Allow clients to connect to receiverRouting from the public internet for service nodes that are in a project VPC or another type of private network.
     */
    receiverRouting?: boolean;
    /**
     * Allow clients to connect to store from the public internet for service nodes that are in a project VPC or another type of private network.
     */
    store?: boolean;
}

export interface ThanosThanosUserConfigQuery {
    /**
     * Set the default evaluation interval for subqueries. Default: `1m`.
     */
    queryDefaultEvaluationInterval?: string;
    /**
     * The maximum lookback duration for retrieving metrics during expression evaluations in PromQL. PromQL always evaluates the query for a certain timestamp, and it looks back for the given amount of time to get the latest sample. If it exceeds the maximum lookback delta, it assumes the series is stale and returns none (a gap). The lookback delta should be set to at least 2 times the slowest scrape interval. If unset, it will use the promql default of 5m. Default: `5m`.
     */
    queryLookbackDelta?: string;
    /**
     * The default metadata time range duration for retrieving labels through Labels and Series API when the range parameters are not specified. The zero value means the range covers the time since the beginning. Default: `0s`.
     */
    queryMetadataDefaultTimeRange?: string;
    /**
     * Maximum time to process a query by the query node. Default: `2m`.
     */
    queryTimeout?: string;
    /**
     * The maximum samples allowed for a single Series request. The Series call fails if this limit is exceeded. Set to 0 for no limit. NOTE: For efficiency, the limit is internally implemented as 'chunks limit' considering each chunk contains a maximum of 120 samples. The default value is 100 * store.limits.request-series. Default: `0`.
     */
    storeLimitsRequestSamples?: number;
    /**
     * The maximum series allowed for a single Series request. The Series call fails if this limit is exceeded. Set to 0 for no limit. The default value is 1000 * cpu_count. Default: `0`.
     */
    storeLimitsRequestSeries?: number;
}

export interface ThanosThanosUserConfigQueryFrontend {
    /**
     * Whether to align the query range boundaries with the step. If enabled, the query range boundaries will be aligned to the step, providing more accurate results for queries with high-resolution data. Default: `true`.
     */
    queryRangeAlignRangeWithStep?: boolean;
}

export interface ValkeyComponent {
    /**
     * Service component name
     */
    component: string;
    /**
     * Connection info for connecting to the service component. This is a combination of host and port.
     */
    connectionUri: string;
    /**
     * Host name for connecting to the service component
     */
    host: string;
    /**
     * Kafka authentication method. This is a value specific to the 'kafka' service component
     */
    kafkaAuthenticationMethod: string;
    /**
     * Port number for connecting to the service component
     */
    port: number;
    /**
     * Network access route
     */
    route: string;
    /**
     * Whether the endpoint is encrypted or accepts plaintext. By default endpoints are always encrypted and this property is only included for service components they may disable encryption
     */
    ssl: boolean;
    /**
     * DNS usage name
     */
    usage: string;
}

export interface ValkeyServiceIntegration {
    /**
     * Type of the service integration. The only supported value at the moment is `readReplica`
     */
    integrationType: string;
    /**
     * Name of the source service
     */
    sourceServiceName: string;
}

export interface ValkeyTag {
    /**
     * Service tag key
     */
    key: string;
    /**
     * Service tag value
     */
    value: string;
}

export interface ValkeyTechEmail {
    /**
     * An email address to contact for technical issues
     */
    email: string;
}

export interface ValkeyValkey {
    /**
     * Valkey password.
     */
    password: string;
    /**
     * Valkey replica server URI.
     */
    replicaUri: string;
    /**
     * Valkey slave server URIs.
     */
    slaveUris: string[];
    /**
     * Valkey server URIs.
     */
    uris: string[];
}

export interface ValkeyValkeyUserConfig {
    /**
     * Additional Cloud Regions for Backup Replication.
     */
    additionalBackupRegions?: string;
    /**
     * The hour of day (in UTC) when backup for the service is started. New backup is only started if previous backup has already completed. Example: `3`.
     */
    backupHour?: number;
    /**
     * The minute of an hour when backup for the service is started. New backup is only started if previous backup has already completed. Example: `30`.
     */
    backupMinute?: number;
    /**
     * Allow incoming connections from CIDR address block, e.g. `10.20.0.0/16`
     */
    ipFilterObjects?: outputs.ValkeyValkeyUserConfigIpFilterObject[];
    /**
     * Allow incoming connections from CIDR address block, e.g. `10.20.0.0/16`.
     */
    ipFilterStrings?: string[];
    /**
     * Allow incoming connections from CIDR address block, e.g. `10.20.0.0/16`.
     *
     * @deprecated Deprecated. Use `ipFilterString` instead.
     */
    ipFilters?: string[];
    /**
     * Migrate data from existing server
     */
    migration?: outputs.ValkeyValkeyUserConfigMigration;
    /**
     * Allow access to selected service ports from private networks
     */
    privateAccess?: outputs.ValkeyValkeyUserConfigPrivateAccess;
    /**
     * Allow access to selected service components through Privatelink
     */
    privatelinkAccess?: outputs.ValkeyValkeyUserConfigPrivatelinkAccess;
    /**
     * Name of another project to fork a service from. This has effect only when a new service is being created. Example: `anotherprojectname`.
     */
    projectToForkFrom?: string;
    /**
     * Allow access to selected service ports from the public Internet
     */
    publicAccess?: outputs.ValkeyValkeyUserConfigPublicAccess;
    /**
     * Name of the basebackup to restore in forked service. Example: `backup-20191112t091354293891z`.
     */
    recoveryBasebackupName?: string;
    /**
     * Store logs for the service so that they are available in the HTTP API and console.
     */
    serviceLog?: boolean;
    /**
     * Name of another service to fork from. This has effect only when a new service is being created. Example: `anotherservicename`.
     */
    serviceToForkFrom?: string;
    /**
     * Use static public IP addresses.
     */
    staticIps?: boolean;
    /**
     * Enum: `allchannels`, `resetchannels`. Determines default pub/sub channels' ACL for new users if ACL is not supplied. When this option is not defined, allChannels is assumed to keep backward compatibility. This option doesn't affect Valkey configuration acl-pubsub-default.
     */
    valkeyAclChannelsDefault?: string;
    /**
     * Set Valkey IO thread count. Changing this will cause a restart of the Valkey service. Example: `1`.
     */
    valkeyIoThreads?: number;
    /**
     * LFU maxmemory-policy counter decay time in minutes. Default: `1`.
     */
    valkeyLfuDecayTime?: number;
    /**
     * Counter logarithm factor for volatile-lfu and allkeys-lfu maxmemory-policies. Default: `10`.
     */
    valkeyLfuLogFactor?: number;
    /**
     * Enum: `noeviction`, `allkeys-lru`, `volatile-lru`, `allkeys-random`, `volatile-random`, `volatile-ttl`, `volatile-lfu`, `allkeys-lfu`. Valkey maxmemory-policy. Default: `noeviction`.
     */
    valkeyMaxmemoryPolicy?: string;
    /**
     * Set notify-keyspace-events option.
     */
    valkeyNotifyKeyspaceEvents?: string;
    /**
     * Set number of Valkey databases. Changing this will cause a restart of the Valkey service. Example: `16`.
     */
    valkeyNumberOfDatabases?: number;
    /**
     * Enum: `off`, `rdb`. When persistence is `rdb`, Valkey does RDB dumps each 10 minutes if any key is changed. Also RDB dumps are done according to backup schedule for backup purposes. When persistence is `off`, no RDB dumps and backups are done, so data can be lost at any moment if service is restarted for any reason, or if service is powered off. Also service can't be forked.
     */
    valkeyPersistence?: string;
    /**
     * Set output buffer limit for pub / sub clients in MB. The value is the hard limit, the soft limit is 1/4 of the hard limit. When setting the limit, be mindful of the available memory in the selected service plan. Example: `64`.
     */
    valkeyPubsubClientOutputBufferLimit?: number;
    /**
     * Require SSL to access Valkey. Default: `true`.
     */
    valkeySsl?: boolean;
    /**
     * Valkey idle connection timeout in seconds. Default: `300`.
     */
    valkeyTimeout?: number;
}

export interface ValkeyValkeyUserConfigIpFilterObject {
    /**
     * Description for IP filter list entry. Example: `Production service IP range`.
     */
    description?: string;
    /**
     * CIDR address block. Example: `10.20.0.0/16`.
     */
    network: string;
}

export interface ValkeyValkeyUserConfigMigration {
    /**
     * Database name for bootstrapping the initial connection. Example: `defaultdb`.
     */
    dbname?: string;
    /**
     * Hostname or IP address of the server where to migrate data from. Example: `my.server.com`.
     */
    host: string;
    /**
     * Comma-separated list of databases, which should be ignored during migration (supported by MySQL and PostgreSQL only at the moment). Example: `db1,db2`.
     */
    ignoreDbs?: string;
    /**
     * Comma-separated list of database roles, which should be ignored during migration (supported by PostgreSQL only at the moment). Example: `role1,role2`.
     */
    ignoreRoles?: string;
    /**
     * Enum: `dump`, `replication`. The migration method to be used (currently supported only by Redis, Dragonfly, MySQL and PostgreSQL service types).
     */
    method?: string;
    /**
     * Password for authentication with the server where to migrate data from. Example: `jjKk45Nnd`.
     */
    password?: string;
    /**
     * Port number of the server where to migrate data from. Example: `1234`.
     */
    port: number;
    /**
     * The server where to migrate data from is secured with SSL. Default: `true`.
     */
    ssl?: boolean;
    /**
     * User name for authentication with the server where to migrate data from. Example: `myname`.
     */
    username?: string;
}

export interface ValkeyValkeyUserConfigPrivateAccess {
    /**
     * Allow clients to connect to prometheus with a DNS name that always resolves to the service's private IP addresses. Only available in certain network locations.
     */
    prometheus?: boolean;
    /**
     * Allow clients to connect to valkey with a DNS name that always resolves to the service's private IP addresses. Only available in certain network locations.
     */
    valkey?: boolean;
}

export interface ValkeyValkeyUserConfigPrivatelinkAccess {
    /**
     * Enable prometheus.
     */
    prometheus?: boolean;
    /**
     * Enable valkey.
     */
    valkey?: boolean;
}

export interface ValkeyValkeyUserConfigPublicAccess {
    /**
     * Allow clients to connect to prometheus from the public internet for service nodes that are in a project VPC or another type of private network.
     */
    prometheus?: boolean;
    /**
     * Allow clients to connect to valkey from the public internet for service nodes that are in a project VPC or another type of private network.
     */
    valkey?: boolean;
}

