// *** WARNING: this file was generated by the Pulumi Terraform Bridge (tfgen) Tool. ***
// *** Do not edit by hand unless you're certain you know what you are doing! ***

import * as pulumi from "@pulumi/pulumi";
import * as inputs from "../types/input";
import * as outputs from "../types/output";

export interface AccountAuthenticationSamlFieldMapping {
    /**
     * Field name for user email
     */
    email?: string;
    /**
     * Field name for user's first name
     */
    firstName?: string;
    /**
     * Field name for user's identity. This field must always exist in responses, and must be immutable and unique. Contents of this field are used to identify the user. Using user ID (such as unix user id) is highly recommended, as email address may change, requiring relinking user to Aiven user.
     */
    identity?: string;
    /**
     * Field name for user's last name
     */
    lastName?: string;
    /**
     * Field name for user's full name. If specified, first*name and last*name mappings are ignored
     */
    realName?: string;
}

export interface CassandraCassandra {
}

export interface CassandraCassandraUserConfig {
    /**
     * Additional Cloud Regions for Backup Replication.
     */
    additionalBackupRegions?: string;
    /**
     * The hour of day (in UTC) when backup for the service is started. New backup is only started if previous backup has already completed.
     */
    backupHour?: number;
    /**
     * The minute of an hour when backup for the service is started. New backup is only started if previous backup has already completed.
     */
    backupMinute?: number;
    /**
     * cassandra configuration values.
     */
    cassandra?: outputs.CassandraCassandraUserConfigCassandra;
    /**
     * Cassandra major version.
     */
    cassandraVersion?: string;
    /**
     * Allow incoming connections from CIDR address block, e.g. '10.20.0.0/16'.
     */
    ipFilterObjects?: outputs.CassandraCassandraUserConfigIpFilterObject[];
    /**
     * Allow incoming connections from CIDR address block, e.g. '10.20.0.0/16'.
     */
    ipFilterStrings?: string[];
    /**
     * Allow incoming connections from CIDR address block, e.g. '10.20.0.0/16'.
     *
     * @deprecated This will be removed in v5.0.0 and replaced with ip_filter_string instead.
     */
    ipFilters?: string[];
    /**
     * Sets the service into migration mode enabling the sstableloader utility to be used to upload Cassandra data files. Available only on service create.
     */
    migrateSstableloader?: boolean;
    /**
     * Allow access to selected service ports from private networks.
     */
    privateAccess?: outputs.CassandraCassandraUserConfigPrivateAccess;
    /**
     * Name of another project to fork a service from. This has effect only when a new service is being created.
     */
    projectToForkFrom?: string;
    /**
     * Allow access to selected service ports from the public Internet.
     */
    publicAccess?: outputs.CassandraCassandraUserConfigPublicAccess;
    /**
     * Store logs for the service so that they are available in the HTTP API and console.
     */
    serviceLog?: boolean;
    /**
     * Name of another service to fork from. This has effect only when a new service is being created.
     */
    serviceToForkFrom?: string;
    /**
     * When bootstrapping, instead of creating a new Cassandra cluster try to join an existing one from another service. Can only be set on service creation.
     */
    serviceToJoinWith?: string;
    /**
     * Use static public IP addresses.
     */
    staticIps?: boolean;
}

export interface CassandraCassandraUserConfigCassandra {
    /**
     * Fail any multiple-partition batch exceeding this value. 50kb (10x warn threshold) by default.
     */
    batchSizeFailThresholdInKb?: number;
    /**
     * Log a warning message on any multiple-partition batch size exceeding this value.5kb per batch by default.Caution should be taken on increasing the size of this thresholdas it can lead to node instability.
     */
    batchSizeWarnThresholdInKb?: number;
    /**
     * Name of the datacenter to which nodes of this service belong. Can be set only when creating the service.
     */
    datacenter?: string;
}

export interface CassandraCassandraUserConfigIpFilterObject {
    /**
     * Description for IP filter list entry.
     */
    description?: string;
    /**
     * CIDR address block.
     */
    network: string;
}

export interface CassandraCassandraUserConfigPrivateAccess {
    /**
     * Allow clients to connect to prometheus with a DNS name that always resolves to the service's private IP addresses. Only available in certain network locations.
     */
    prometheus?: boolean;
}

export interface CassandraCassandraUserConfigPublicAccess {
    /**
     * Allow clients to connect to prometheus with a DNS name that always resolves to the service's private IP addresses. Only available in certain network locations.
     */
    prometheus?: boolean;
}

export interface CassandraComponent {
    component: string;
    connectionUri: string;
    host: string;
    kafkaAuthenticationMethod: string;
    port: number;
    route: string;
    ssl: boolean;
    usage: string;
}

export interface CassandraServiceIntegration {
    /**
     * Type of the service integration. The only supported value at the moment is `readReplica`
     */
    integrationType: string;
    /**
     * Name of the source service
     */
    sourceServiceName: string;
}

export interface CassandraTag {
    /**
     * Service tag key
     */
    key: string;
    /**
     * Service tag value
     */
    value: string;
}

export interface CassandraTechEmail {
    /**
     * An email address to contact for technical issues
     */
    email: string;
}

export interface ClickhouseClickhouse {
}

export interface ClickhouseClickhouseUserConfig {
    /**
     * Additional Cloud Regions for Backup Replication.
     */
    additionalBackupRegions?: string;
    /**
     * Allow incoming connections from CIDR address block, e.g. '10.20.0.0/16'.
     */
    ipFilterObjects?: outputs.ClickhouseClickhouseUserConfigIpFilterObject[];
    /**
     * Allow incoming connections from CIDR address block, e.g. '10.20.0.0/16'.
     */
    ipFilterStrings?: string[];
    /**
     * Allow incoming connections from CIDR address block, e.g. '10.20.0.0/16'.
     *
     * @deprecated This will be removed in v5.0.0 and replaced with ip_filter_string instead.
     */
    ipFilters?: string[];
    /**
     * Allow access to selected service ports from private networks.
     */
    privateAccess?: outputs.ClickhouseClickhouseUserConfigPrivateAccess;
    /**
     * Allow access to selected service components through Privatelink.
     */
    privatelinkAccess?: outputs.ClickhouseClickhouseUserConfigPrivatelinkAccess;
    /**
     * Name of another project to fork a service from. This has effect only when a new service is being created.
     */
    projectToForkFrom?: string;
    /**
     * Allow access to selected service ports from the public Internet.
     */
    publicAccess?: outputs.ClickhouseClickhouseUserConfigPublicAccess;
    /**
     * Store logs for the service so that they are available in the HTTP API and console.
     */
    serviceLog?: boolean;
    /**
     * Name of another service to fork from. This has effect only when a new service is being created.
     */
    serviceToForkFrom?: string;
    /**
     * Use static public IP addresses.
     */
    staticIps?: boolean;
}

export interface ClickhouseClickhouseUserConfigIpFilterObject {
    /**
     * Description for IP filter list entry.
     */
    description?: string;
    /**
     * CIDR address block.
     */
    network: string;
}

export interface ClickhouseClickhouseUserConfigPrivateAccess {
    /**
     * Allow clients to connect to clickhouse with a DNS name that always resolves to the service's private IP addresses. Only available in certain network locations.
     */
    clickhouse?: boolean;
    /**
     * Allow clients to connect to clickhouseHttps with a DNS name that always resolves to the service's private IP addresses. Only available in certain network locations.
     */
    clickhouseHttps?: boolean;
    /**
     * Allow clients to connect to clickhouseMysql with a DNS name that always resolves to the service's private IP addresses. Only available in certain network locations.
     */
    clickhouseMysql?: boolean;
    /**
     * Allow clients to connect to prometheus with a DNS name that always resolves to the service's private IP addresses. Only available in certain network locations.
     */
    prometheus?: boolean;
}

export interface ClickhouseClickhouseUserConfigPrivatelinkAccess {
    /**
     * Allow clients to connect to clickhouse with a DNS name that always resolves to the service's private IP addresses. Only available in certain network locations.
     */
    clickhouse?: boolean;
    /**
     * Allow clients to connect to clickhouseHttps with a DNS name that always resolves to the service's private IP addresses. Only available in certain network locations.
     */
    clickhouseHttps?: boolean;
    /**
     * Allow clients to connect to clickhouseMysql with a DNS name that always resolves to the service's private IP addresses. Only available in certain network locations.
     */
    clickhouseMysql?: boolean;
    /**
     * Allow clients to connect to prometheus with a DNS name that always resolves to the service's private IP addresses. Only available in certain network locations.
     */
    prometheus?: boolean;
}

export interface ClickhouseClickhouseUserConfigPublicAccess {
    /**
     * Allow clients to connect to clickhouse with a DNS name that always resolves to the service's private IP addresses. Only available in certain network locations.
     */
    clickhouse?: boolean;
    /**
     * Allow clients to connect to clickhouseHttps with a DNS name that always resolves to the service's private IP addresses. Only available in certain network locations.
     */
    clickhouseHttps?: boolean;
    /**
     * Allow clients to connect to clickhouseMysql with a DNS name that always resolves to the service's private IP addresses. Only available in certain network locations.
     */
    clickhouseMysql?: boolean;
    /**
     * Allow clients to connect to prometheus with a DNS name that always resolves to the service's private IP addresses. Only available in certain network locations.
     */
    prometheus?: boolean;
}

export interface ClickhouseComponent {
    component: string;
    connectionUri: string;
    host: string;
    kafkaAuthenticationMethod: string;
    port: number;
    route: string;
    ssl: boolean;
    usage: string;
}

export interface ClickhouseGrantPrivilegeGrant {
    /**
     * The column that the grant refers to. This property cannot be changed, doing so forces recreation of the resource.
     */
    column?: string;
    /**
     * The database that the grant refers to. To set up proper dependencies please refer to this variable as a reference. This property cannot be changed, doing so forces recreation of the resource.
     */
    database: string;
    /**
     * The privilege to grant, i.e. 'INSERT', 'SELECT', etc. This property cannot be changed, doing so forces recreation of the resource.
     */
    privilege?: string;
    /**
     * The table that the grant refers to. This property cannot be changed, doing so forces recreation of the resource.
     */
    table?: string;
    /**
     * If true then the grantee gets the ability to grant the privileges he received too. This property cannot be changed, doing so forces recreation of the resource.
     */
    withGrant?: boolean;
}

export interface ClickhouseGrantRoleGrant {
    /**
     * The role that is to be granted. To set up proper dependencies please refer to this variable as a reference. This property cannot be changed, doing so forces recreation of the resource.
     */
    role?: string;
}

export interface ClickhouseServiceIntegration {
    /**
     * Type of the service integration. The only supported values at the moment are `clickhouseKafka` and `clickhousePostgresql`.
     */
    integrationType: string;
    /**
     * Name of the source service
     */
    sourceServiceName: string;
}

export interface ClickhouseTag {
    /**
     * Service tag key
     */
    key: string;
    /**
     * Service tag value
     */
    value: string;
}

export interface ClickhouseTechEmail {
    /**
     * An email address to contact for technical issues
     */
    email: string;
}

export interface FlinkApplicationVersionSink {
    /**
     * The CREATE TABLE statement
     */
    createTable: string;
    /**
     * The integration ID
     */
    integrationId?: string;
}

export interface FlinkApplicationVersionSource {
    /**
     * The CREATE TABLE statement
     */
    createTable: string;
    /**
     * The integration ID
     */
    integrationId?: string;
}

export interface FlinkComponent {
    component: string;
    connectionUri: string;
    host: string;
    kafkaAuthenticationMethod: string;
    port: number;
    route: string;
    ssl: boolean;
    usage: string;
}

export interface FlinkFlink {
    /**
     * Host and Port of a Flink server
     */
    hostPorts: string[];
}

export interface FlinkFlinkUserConfig {
    /**
     * Additional Cloud Regions for Backup Replication.
     */
    additionalBackupRegions?: string;
    /**
     * Flink major version.
     */
    flinkVersion?: string;
    /**
     * Allow incoming connections from CIDR address block, e.g. '10.20.0.0/16'.
     */
    ipFilterObjects?: outputs.FlinkFlinkUserConfigIpFilterObject[];
    /**
     * Allow incoming connections from CIDR address block, e.g. '10.20.0.0/16'.
     */
    ipFilterStrings?: string[];
    /**
     * Allow incoming connections from CIDR address block, e.g. '10.20.0.0/16'.
     *
     * @deprecated This will be removed in v5.0.0 and replaced with ip_filter_string instead.
     */
    ipFilters?: string[];
    /**
     * Task slots per node. For a 3 node plan, total number of task slots is 3x this value.
     */
    numberOfTaskSlots?: number;
    /**
     * Allow access to selected service components through Privatelink.
     */
    privatelinkAccess?: outputs.FlinkFlinkUserConfigPrivatelinkAccess;
    /**
     * Store logs for the service so that they are available in the HTTP API and console.
     */
    serviceLog?: boolean;
    /**
     * Use static public IP addresses.
     */
    staticIps?: boolean;
}

export interface FlinkFlinkUserConfigIpFilterObject {
    /**
     * Description for IP filter list entry.
     */
    description?: string;
    /**
     * CIDR address block.
     */
    network: string;
}

export interface FlinkFlinkUserConfigPrivatelinkAccess {
    /**
     * Enable flink.
     */
    flink?: boolean;
    /**
     * Enable prometheus.
     */
    prometheus?: boolean;
}

export interface FlinkServiceIntegration {
    /**
     * Type of the service integration. The only supported value at the moment is `readReplica`
     */
    integrationType: string;
    /**
     * Name of the source service
     */
    sourceServiceName: string;
}

export interface FlinkTag {
    /**
     * Service tag key
     */
    key: string;
    /**
     * Service tag value
     */
    value: string;
}

export interface FlinkTechEmail {
    /**
     * An email address to contact for technical issues
     */
    email: string;
}

export interface GetAccountAuthenticationSamlFieldMapping {
    email?: string;
    firstName?: string;
    identity?: string;
    lastName?: string;
    realName?: string;
}

export interface GetCassandaCassandra {
}

export interface GetCassandaCassandraUserConfig {
    additionalBackupRegions?: string;
    backupHour?: number;
    backupMinute?: number;
    cassandra?: outputs.GetCassandaCassandraUserConfigCassandra;
    cassandraVersion?: string;
    ipFilterObjects?: outputs.GetCassandaCassandraUserConfigIpFilterObject[];
    ipFilterStrings?: string[];
    /**
     * @deprecated This will be removed in v5.0.0 and replaced with ip_filter_string instead.
     */
    ipFilters?: string[];
    migrateSstableloader?: boolean;
    privateAccess?: outputs.GetCassandaCassandraUserConfigPrivateAccess;
    projectToForkFrom?: string;
    publicAccess?: outputs.GetCassandaCassandraUserConfigPublicAccess;
    serviceLog?: boolean;
    serviceToForkFrom?: string;
    serviceToJoinWith?: string;
    staticIps?: boolean;
}

export interface GetCassandaCassandraUserConfigCassandra {
    batchSizeFailThresholdInKb?: number;
    batchSizeWarnThresholdInKb?: number;
    datacenter?: string;
}

export interface GetCassandaCassandraUserConfigIpFilterObject {
    description?: string;
    network: string;
}

export interface GetCassandaCassandraUserConfigPrivateAccess {
    prometheus?: boolean;
}

export interface GetCassandaCassandraUserConfigPublicAccess {
    prometheus?: boolean;
}

export interface GetCassandaComponent {
    component: string;
    connectionUri: string;
    host: string;
    kafkaAuthenticationMethod: string;
    port: number;
    route: string;
    ssl: boolean;
    usage: string;
}

export interface GetCassandaServiceIntegration {
    integrationType: string;
    sourceServiceName: string;
}

export interface GetCassandaTag {
    key: string;
    value: string;
}

export interface GetCassandaTechEmail {
    email: string;
}

export interface GetCassandraCassandra {
}

export interface GetCassandraCassandraUserConfig {
    additionalBackupRegions?: string;
    backupHour?: number;
    backupMinute?: number;
    cassandra?: outputs.GetCassandraCassandraUserConfigCassandra;
    cassandraVersion?: string;
    ipFilterObjects?: outputs.GetCassandraCassandraUserConfigIpFilterObject[];
    ipFilterStrings?: string[];
    /**
     * @deprecated This will be removed in v5.0.0 and replaced with ip_filter_string instead.
     */
    ipFilters?: string[];
    migrateSstableloader?: boolean;
    privateAccess?: outputs.GetCassandraCassandraUserConfigPrivateAccess;
    projectToForkFrom?: string;
    publicAccess?: outputs.GetCassandraCassandraUserConfigPublicAccess;
    serviceLog?: boolean;
    serviceToForkFrom?: string;
    serviceToJoinWith?: string;
    staticIps?: boolean;
}

export interface GetCassandraCassandraUserConfigCassandra {
    batchSizeFailThresholdInKb?: number;
    batchSizeWarnThresholdInKb?: number;
    datacenter?: string;
}

export interface GetCassandraCassandraUserConfigIpFilterObject {
    description?: string;
    network: string;
}

export interface GetCassandraCassandraUserConfigPrivateAccess {
    prometheus?: boolean;
}

export interface GetCassandraCassandraUserConfigPublicAccess {
    prometheus?: boolean;
}

export interface GetCassandraComponent {
    component: string;
    connectionUri: string;
    host: string;
    kafkaAuthenticationMethod: string;
    port: number;
    route: string;
    ssl: boolean;
    usage: string;
}

export interface GetCassandraServiceIntegration {
    integrationType: string;
    sourceServiceName: string;
}

export interface GetCassandraTag {
    key: string;
    value: string;
}

export interface GetCassandraTechEmail {
    email: string;
}

export interface GetClickhouseClickhouse {
}

export interface GetClickhouseClickhouseUserConfig {
    additionalBackupRegions?: string;
    ipFilterObjects?: outputs.GetClickhouseClickhouseUserConfigIpFilterObject[];
    ipFilterStrings?: string[];
    /**
     * @deprecated This will be removed in v5.0.0 and replaced with ip_filter_string instead.
     */
    ipFilters?: string[];
    privateAccess?: outputs.GetClickhouseClickhouseUserConfigPrivateAccess;
    privatelinkAccess?: outputs.GetClickhouseClickhouseUserConfigPrivatelinkAccess;
    projectToForkFrom?: string;
    publicAccess?: outputs.GetClickhouseClickhouseUserConfigPublicAccess;
    serviceLog?: boolean;
    serviceToForkFrom?: string;
    staticIps?: boolean;
}

export interface GetClickhouseClickhouseUserConfigIpFilterObject {
    description?: string;
    network: string;
}

export interface GetClickhouseClickhouseUserConfigPrivateAccess {
    clickhouse?: boolean;
    clickhouseHttps?: boolean;
    clickhouseMysql?: boolean;
    prometheus?: boolean;
}

export interface GetClickhouseClickhouseUserConfigPrivatelinkAccess {
    clickhouse?: boolean;
    clickhouseHttps?: boolean;
    clickhouseMysql?: boolean;
    prometheus?: boolean;
}

export interface GetClickhouseClickhouseUserConfigPublicAccess {
    clickhouse?: boolean;
    clickhouseHttps?: boolean;
    clickhouseMysql?: boolean;
    prometheus?: boolean;
}

export interface GetClickhouseComponent {
    component: string;
    connectionUri: string;
    host: string;
    kafkaAuthenticationMethod: string;
    port: number;
    route: string;
    ssl: boolean;
    usage: string;
}

export interface GetClickhouseServiceIntegration {
    integrationType: string;
    sourceServiceName: string;
}

export interface GetClickhouseTag {
    key: string;
    value: string;
}

export interface GetClickhouseTechEmail {
    email: string;
}

export interface GetFlinkApplicationVersionSink {
    createTable: string;
    integrationId?: string;
}

export interface GetFlinkApplicationVersionSource {
    createTable: string;
    integrationId?: string;
}

export interface GetFlinkComponent {
    component: string;
    connectionUri: string;
    host: string;
    kafkaAuthenticationMethod: string;
    port: number;
    route: string;
    ssl: boolean;
    usage: string;
}

export interface GetFlinkFlink {
    hostPorts: string[];
}

export interface GetFlinkFlinkUserConfig {
    additionalBackupRegions?: string;
    flinkVersion?: string;
    ipFilterObjects?: outputs.GetFlinkFlinkUserConfigIpFilterObject[];
    ipFilterStrings?: string[];
    /**
     * @deprecated This will be removed in v5.0.0 and replaced with ip_filter_string instead.
     */
    ipFilters?: string[];
    numberOfTaskSlots?: number;
    privatelinkAccess?: outputs.GetFlinkFlinkUserConfigPrivatelinkAccess;
    serviceLog?: boolean;
    staticIps?: boolean;
}

export interface GetFlinkFlinkUserConfigIpFilterObject {
    description?: string;
    network: string;
}

export interface GetFlinkFlinkUserConfigPrivatelinkAccess {
    flink?: boolean;
    prometheus?: boolean;
}

export interface GetFlinkServiceIntegration {
    integrationType: string;
    sourceServiceName: string;
}

export interface GetFlinkTag {
    key: string;
    value: string;
}

export interface GetFlinkTechEmail {
    email: string;
}

export interface GetGrafanaComponent {
    component: string;
    connectionUri: string;
    host: string;
    kafkaAuthenticationMethod: string;
    port: number;
    route: string;
    ssl: boolean;
    usage: string;
}

export interface GetGrafanaGrafana {
}

export interface GetGrafanaGrafanaUserConfig {
    additionalBackupRegions?: string;
    alertingEnabled?: boolean;
    alertingErrorOrTimeout?: string;
    alertingMaxAnnotationsToKeep?: number;
    alertingNodataOrNullvalues?: string;
    allowEmbedding?: boolean;
    authAzuread?: outputs.GetGrafanaGrafanaUserConfigAuthAzuread;
    authBasicEnabled?: boolean;
    authGenericOauth?: outputs.GetGrafanaGrafanaUserConfigAuthGenericOauth;
    authGithub?: outputs.GetGrafanaGrafanaUserConfigAuthGithub;
    authGitlab?: outputs.GetGrafanaGrafanaUserConfigAuthGitlab;
    authGoogle?: outputs.GetGrafanaGrafanaUserConfigAuthGoogle;
    cookieSamesite?: string;
    customDomain?: string;
    dashboardPreviewsEnabled?: boolean;
    dashboardsMinRefreshInterval?: string;
    dashboardsVersionsToKeep?: number;
    dataproxySendUserHeader?: boolean;
    dataproxyTimeout?: number;
    dateFormats?: outputs.GetGrafanaGrafanaUserConfigDateFormats;
    disableGravatar?: boolean;
    editorsCanAdmin?: boolean;
    externalImageStorage?: outputs.GetGrafanaGrafanaUserConfigExternalImageStorage;
    googleAnalyticsUaId?: string;
    ipFilterObjects?: outputs.GetGrafanaGrafanaUserConfigIpFilterObject[];
    ipFilterStrings?: string[];
    /**
     * @deprecated This will be removed in v5.0.0 and replaced with ip_filter_string instead.
     */
    ipFilters?: string[];
    metricsEnabled?: boolean;
    oauthAllowInsecureEmailLookup?: boolean;
    privateAccess?: outputs.GetGrafanaGrafanaUserConfigPrivateAccess;
    privatelinkAccess?: outputs.GetGrafanaGrafanaUserConfigPrivatelinkAccess;
    projectToForkFrom?: string;
    publicAccess?: outputs.GetGrafanaGrafanaUserConfigPublicAccess;
    recoveryBasebackupName?: string;
    serviceLog?: boolean;
    serviceToForkFrom?: string;
    smtpServer?: outputs.GetGrafanaGrafanaUserConfigSmtpServer;
    staticIps?: boolean;
    unifiedAlertingEnabled?: boolean;
    userAutoAssignOrg?: boolean;
    userAutoAssignOrgRole?: string;
    viewersCanEdit?: boolean;
}

export interface GetGrafanaGrafanaUserConfigAuthAzuread {
    allowSignUp?: boolean;
    allowedDomains?: string[];
    allowedGroups?: string[];
    authUrl: string;
    clientId: string;
    clientSecret: string;
    tokenUrl: string;
}

export interface GetGrafanaGrafanaUserConfigAuthGenericOauth {
    allowSignUp?: boolean;
    allowedDomains?: string[];
    allowedOrganizations?: string[];
    apiUrl: string;
    authUrl: string;
    autoLogin?: boolean;
    clientId: string;
    clientSecret: string;
    name?: string;
    scopes?: string[];
    tokenUrl: string;
}

export interface GetGrafanaGrafanaUserConfigAuthGithub {
    allowSignUp?: boolean;
    allowedOrganizations?: string[];
    clientId: string;
    clientSecret: string;
    skipOrgRoleSync?: boolean;
    teamIds?: number[];
}

export interface GetGrafanaGrafanaUserConfigAuthGitlab {
    allowSignUp?: boolean;
    allowedGroups?: string[];
    apiUrl?: string;
    authUrl?: string;
    clientId: string;
    clientSecret: string;
    tokenUrl?: string;
}

export interface GetGrafanaGrafanaUserConfigAuthGoogle {
    allowSignUp?: boolean;
    allowedDomains?: string[];
    clientId: string;
    clientSecret: string;
}

export interface GetGrafanaGrafanaUserConfigDateFormats {
    defaultTimezone?: string;
    fullDate?: string;
    intervalDay?: string;
    intervalHour?: string;
    intervalMinute?: string;
    intervalMonth?: string;
    intervalSecond?: string;
    intervalYear?: string;
}

export interface GetGrafanaGrafanaUserConfigExternalImageStorage {
    accessKey: string;
    bucketUrl: string;
    provider: string;
    secretKey: string;
}

export interface GetGrafanaGrafanaUserConfigIpFilterObject {
    description?: string;
    network: string;
}

export interface GetGrafanaGrafanaUserConfigPrivateAccess {
    grafana?: boolean;
}

export interface GetGrafanaGrafanaUserConfigPrivatelinkAccess {
    grafana?: boolean;
}

export interface GetGrafanaGrafanaUserConfigPublicAccess {
    grafana?: boolean;
}

export interface GetGrafanaGrafanaUserConfigSmtpServer {
    fromAddress: string;
    fromName?: string;
    host: string;
    password?: string;
    port: number;
    skipVerify?: boolean;
    starttlsPolicy?: string;
    username?: string;
}

export interface GetGrafanaServiceIntegration {
    integrationType: string;
    sourceServiceName: string;
}

export interface GetGrafanaTag {
    key: string;
    value: string;
}

export interface GetGrafanaTechEmail {
    email: string;
}

export interface GetInfluxDbComponent {
    component: string;
    connectionUri: string;
    host: string;
    kafkaAuthenticationMethod: string;
    port: number;
    route: string;
    ssl: boolean;
    usage: string;
}

export interface GetInfluxDbInfluxdb {
    databaseName: string;
}

export interface GetInfluxDbInfluxdbUserConfig {
    additionalBackupRegions?: string;
    customDomain?: string;
    influxdb?: outputs.GetInfluxDbInfluxdbUserConfigInfluxdb;
    ipFilterObjects?: outputs.GetInfluxDbInfluxdbUserConfigIpFilterObject[];
    ipFilterStrings?: string[];
    /**
     * @deprecated This will be removed in v5.0.0 and replaced with ip_filter_string instead.
     */
    ipFilters?: string[];
    privateAccess?: outputs.GetInfluxDbInfluxdbUserConfigPrivateAccess;
    privatelinkAccess?: outputs.GetInfluxDbInfluxdbUserConfigPrivatelinkAccess;
    projectToForkFrom?: string;
    publicAccess?: outputs.GetInfluxDbInfluxdbUserConfigPublicAccess;
    recoveryBasebackupName?: string;
    serviceLog?: boolean;
    serviceToForkFrom?: string;
    staticIps?: boolean;
}

export interface GetInfluxDbInfluxdbUserConfigInfluxdb {
    logQueriesAfter?: number;
    maxConnectionLimit?: number;
    maxRowLimit?: number;
    maxSelectBuckets?: number;
    maxSelectPoint?: number;
    queryLogEnabled?: boolean;
    queryTimeout?: number;
}

export interface GetInfluxDbInfluxdbUserConfigIpFilterObject {
    description?: string;
    network: string;
}

export interface GetInfluxDbInfluxdbUserConfigPrivateAccess {
    influxdb?: boolean;
}

export interface GetInfluxDbInfluxdbUserConfigPrivatelinkAccess {
    influxdb?: boolean;
}

export interface GetInfluxDbInfluxdbUserConfigPublicAccess {
    influxdb?: boolean;
}

export interface GetInfluxDbServiceIntegration {
    integrationType: string;
    sourceServiceName: string;
}

export interface GetInfluxDbTag {
    key: string;
    value: string;
}

export interface GetInfluxDbTechEmail {
    email: string;
}

export interface GetKafkaComponent {
    component: string;
    connectionUri: string;
    host: string;
    kafkaAuthenticationMethod: string;
    port: number;
    route: string;
    ssl: boolean;
    usage: string;
}

export interface GetKafkaConnectComponent {
    component: string;
    connectionUri: string;
    host: string;
    kafkaAuthenticationMethod: string;
    port: number;
    route: string;
    ssl: boolean;
    usage: string;
}

export interface GetKafkaConnectKafkaConnect {
}

export interface GetKafkaConnectKafkaConnectUserConfig {
    additionalBackupRegions?: string;
    ipFilterObjects?: outputs.GetKafkaConnectKafkaConnectUserConfigIpFilterObject[];
    ipFilterStrings?: string[];
    /**
     * @deprecated This will be removed in v5.0.0 and replaced with ip_filter_string instead.
     */
    ipFilters?: string[];
    kafkaConnect?: outputs.GetKafkaConnectKafkaConnectUserConfigKafkaConnect;
    privateAccess?: outputs.GetKafkaConnectKafkaConnectUserConfigPrivateAccess;
    privatelinkAccess?: outputs.GetKafkaConnectKafkaConnectUserConfigPrivatelinkAccess;
    publicAccess?: outputs.GetKafkaConnectKafkaConnectUserConfigPublicAccess;
    serviceLog?: boolean;
    staticIps?: boolean;
}

export interface GetKafkaConnectKafkaConnectUserConfigIpFilterObject {
    description?: string;
    network: string;
}

export interface GetKafkaConnectKafkaConnectUserConfigKafkaConnect {
    connectorClientConfigOverridePolicy?: string;
    consumerAutoOffsetReset?: string;
    consumerFetchMaxBytes?: number;
    consumerIsolationLevel?: string;
    consumerMaxPartitionFetchBytes?: number;
    consumerMaxPollIntervalMs?: number;
    consumerMaxPollRecords?: number;
    offsetFlushIntervalMs?: number;
    offsetFlushTimeoutMs?: number;
    producerBatchSize?: number;
    producerBufferMemory?: number;
    producerCompressionType?: string;
    producerLingerMs?: number;
    producerMaxRequestSize?: number;
    scheduledRebalanceMaxDelayMs?: number;
    sessionTimeoutMs?: number;
}

export interface GetKafkaConnectKafkaConnectUserConfigPrivateAccess {
    kafkaConnect?: boolean;
    prometheus?: boolean;
}

export interface GetKafkaConnectKafkaConnectUserConfigPrivatelinkAccess {
    jolokia?: boolean;
    kafkaConnect?: boolean;
    prometheus?: boolean;
}

export interface GetKafkaConnectKafkaConnectUserConfigPublicAccess {
    kafkaConnect?: boolean;
    prometheus?: boolean;
}

export interface GetKafkaConnectServiceIntegration {
    integrationType: string;
    sourceServiceName: string;
}

export interface GetKafkaConnectTag {
    key: string;
    value: string;
}

export interface GetKafkaConnectTechEmail {
    email: string;
}

export interface GetKafkaConnectorTask {
    connector: string;
    task: number;
}

export interface GetKafkaKafka {
    accessCert: string;
    accessKey: string;
    connectUri: string;
    restUri: string;
    schemaRegistryUri: string;
}

export interface GetKafkaKafkaUserConfig {
    additionalBackupRegions?: string;
    aivenKafkaTopicMessages?: boolean;
    customDomain?: string;
    ipFilterObjects?: outputs.GetKafkaKafkaUserConfigIpFilterObject[];
    ipFilterStrings?: string[];
    /**
     * @deprecated This will be removed in v5.0.0 and replaced with ip_filter_string instead.
     */
    ipFilters?: string[];
    kafka?: outputs.GetKafkaKafkaUserConfigKafka;
    kafkaAuthenticationMethods?: outputs.GetKafkaKafkaUserConfigKafkaAuthenticationMethods;
    kafkaConnect?: boolean;
    kafkaConnectConfig?: outputs.GetKafkaKafkaUserConfigKafkaConnectConfig;
    kafkaRest?: boolean;
    kafkaRestAuthorization?: boolean;
    kafkaRestConfig?: outputs.GetKafkaKafkaUserConfigKafkaRestConfig;
    kafkaVersion?: string;
    privateAccess?: outputs.GetKafkaKafkaUserConfigPrivateAccess;
    privatelinkAccess?: outputs.GetKafkaKafkaUserConfigPrivatelinkAccess;
    publicAccess?: outputs.GetKafkaKafkaUserConfigPublicAccess;
    schemaRegistry?: boolean;
    schemaRegistryConfig?: outputs.GetKafkaKafkaUserConfigSchemaRegistryConfig;
    serviceLog?: boolean;
    staticIps?: boolean;
    tieredStorage?: outputs.GetKafkaKafkaUserConfigTieredStorage;
}

export interface GetKafkaKafkaUserConfigIpFilterObject {
    description?: string;
    network: string;
}

export interface GetKafkaKafkaUserConfigKafka {
    autoCreateTopicsEnable?: boolean;
    compressionType?: string;
    connectionsMaxIdleMs?: number;
    defaultReplicationFactor?: number;
    groupInitialRebalanceDelayMs?: number;
    groupMaxSessionTimeoutMs?: number;
    groupMinSessionTimeoutMs?: number;
    logCleanerDeleteRetentionMs?: number;
    logCleanerMaxCompactionLagMs?: number;
    logCleanerMinCleanableRatio?: number;
    logCleanerMinCompactionLagMs?: number;
    logCleanupPolicy?: string;
    logFlushIntervalMessages?: number;
    logFlushIntervalMs?: number;
    logIndexIntervalBytes?: number;
    logIndexSizeMaxBytes?: number;
    logLocalRetentionBytes?: number;
    logLocalRetentionMs?: number;
    logMessageDownconversionEnable?: boolean;
    logMessageTimestampDifferenceMaxMs?: number;
    logMessageTimestampType?: string;
    logPreallocate?: boolean;
    logRetentionBytes?: number;
    logRetentionHours?: number;
    logRetentionMs?: number;
    logRollJitterMs?: number;
    logRollMs?: number;
    logSegmentBytes?: number;
    logSegmentDeleteDelayMs?: number;
    maxConnectionsPerIp?: number;
    maxIncrementalFetchSessionCacheSlots?: number;
    messageMaxBytes?: number;
    minInsyncReplicas?: number;
    numPartitions?: number;
    offsetsRetentionMinutes?: number;
    producerPurgatoryPurgeIntervalRequests?: number;
    replicaFetchMaxBytes?: number;
    replicaFetchResponseMaxBytes?: number;
    saslOauthbearerExpectedAudience?: string;
    saslOauthbearerExpectedIssuer?: string;
    saslOauthbearerJwksEndpointUrl?: string;
    saslOauthbearerSubClaimName?: string;
    socketRequestMaxBytes?: number;
    transactionPartitionVerificationEnable?: boolean;
    transactionRemoveExpiredTransactionCleanupIntervalMs?: number;
    transactionStateLogSegmentBytes?: number;
}

export interface GetKafkaKafkaUserConfigKafkaAuthenticationMethods {
    certificate?: boolean;
    sasl?: boolean;
}

export interface GetKafkaKafkaUserConfigKafkaConnectConfig {
    connectorClientConfigOverridePolicy?: string;
    consumerAutoOffsetReset?: string;
    consumerFetchMaxBytes?: number;
    consumerIsolationLevel?: string;
    consumerMaxPartitionFetchBytes?: number;
    consumerMaxPollIntervalMs?: number;
    consumerMaxPollRecords?: number;
    offsetFlushIntervalMs?: number;
    offsetFlushTimeoutMs?: number;
    producerBatchSize?: number;
    producerBufferMemory?: number;
    producerCompressionType?: string;
    producerLingerMs?: number;
    producerMaxRequestSize?: number;
    scheduledRebalanceMaxDelayMs?: number;
    sessionTimeoutMs?: number;
}

export interface GetKafkaKafkaUserConfigKafkaRestConfig {
    consumerEnableAutoCommit?: boolean;
    consumerRequestMaxBytes?: number;
    consumerRequestTimeoutMs?: number;
    nameStrategyValidation?: boolean;
    producerAcks?: string;
    producerCompressionType?: string;
    producerLingerMs?: number;
    producerMaxRequestSize?: number;
    simpleconsumerPoolSizeMax?: number;
}

export interface GetKafkaKafkaUserConfigPrivateAccess {
    kafka?: boolean;
    kafkaConnect?: boolean;
    kafkaRest?: boolean;
    prometheus?: boolean;
    schemaRegistry?: boolean;
}

export interface GetKafkaKafkaUserConfigPrivatelinkAccess {
    jolokia?: boolean;
    kafka?: boolean;
    kafkaConnect?: boolean;
    kafkaRest?: boolean;
    prometheus?: boolean;
    schemaRegistry?: boolean;
}

export interface GetKafkaKafkaUserConfigPublicAccess {
    kafka?: boolean;
    kafkaConnect?: boolean;
    kafkaRest?: boolean;
    prometheus?: boolean;
    schemaRegistry?: boolean;
}

export interface GetKafkaKafkaUserConfigSchemaRegistryConfig {
    leaderEligibility?: boolean;
    topicName?: string;
}

export interface GetKafkaKafkaUserConfigTieredStorage {
    enabled?: boolean;
    localCache?: outputs.GetKafkaKafkaUserConfigTieredStorageLocalCache;
}

export interface GetKafkaKafkaUserConfigTieredStorageLocalCache {
    size?: number;
}

export interface GetKafkaMirrorMakerComponent {
    component: string;
    connectionUri: string;
    host: string;
    kafkaAuthenticationMethod: string;
    port: number;
    route: string;
    ssl: boolean;
    usage: string;
}

export interface GetKafkaMirrorMakerKafkaMirrormaker {
}

export interface GetKafkaMirrorMakerKafkaMirrormakerUserConfig {
    additionalBackupRegions?: string;
    ipFilterObjects?: outputs.GetKafkaMirrorMakerKafkaMirrormakerUserConfigIpFilterObject[];
    ipFilterStrings?: string[];
    /**
     * @deprecated This will be removed in v5.0.0 and replaced with ip_filter_string instead.
     */
    ipFilters?: string[];
    kafkaMirrormaker?: outputs.GetKafkaMirrorMakerKafkaMirrormakerUserConfigKafkaMirrormaker;
    serviceLog?: boolean;
    staticIps?: boolean;
}

export interface GetKafkaMirrorMakerKafkaMirrormakerUserConfigIpFilterObject {
    description?: string;
    network: string;
}

export interface GetKafkaMirrorMakerKafkaMirrormakerUserConfigKafkaMirrormaker {
    emitCheckpointsEnabled?: boolean;
    emitCheckpointsIntervalSeconds?: number;
    refreshGroupsEnabled?: boolean;
    refreshGroupsIntervalSeconds?: number;
    refreshTopicsEnabled?: boolean;
    refreshTopicsIntervalSeconds?: number;
    syncGroupOffsetsEnabled?: boolean;
    syncGroupOffsetsIntervalSeconds?: number;
    syncTopicConfigsEnabled?: boolean;
    tasksMaxPerCpu?: number;
}

export interface GetKafkaMirrorMakerServiceIntegration {
    integrationType: string;
    sourceServiceName: string;
}

export interface GetKafkaMirrorMakerTag {
    key: string;
    value: string;
}

export interface GetKafkaMirrorMakerTechEmail {
    email: string;
}

export interface GetKafkaServiceIntegration {
    integrationType: string;
    sourceServiceName: string;
}

export interface GetKafkaTag {
    key: string;
    value: string;
}

export interface GetKafkaTechEmail {
    email: string;
}

export interface GetKafkaTopicConfig {
    cleanupPolicy?: string;
    compressionType?: string;
    deleteRetentionMs?: string;
    fileDeleteDelayMs?: string;
    flushMessages?: string;
    flushMs?: string;
    indexIntervalBytes?: string;
    localRetentionBytes?: string;
    localRetentionMs?: string;
    maxCompactionLagMs?: string;
    maxMessageBytes?: string;
    messageDownconversionEnable?: boolean;
    messageFormatVersion?: string;
    messageTimestampDifferenceMaxMs?: string;
    messageTimestampType?: string;
    minCleanableDirtyRatio?: number;
    minCompactionLagMs?: string;
    minInsyncReplicas?: string;
    preallocate?: boolean;
    remoteStorageEnable?: boolean;
    retentionBytes?: string;
    retentionMs?: string;
    segmentBytes?: string;
    segmentIndexBytes?: string;
    segmentJitterMs?: string;
    segmentMs?: string;
    /**
     * @deprecated This field is deprecated and no longer functional.
     */
    uncleanLeaderElectionEnable?: boolean;
}

export interface GetKafkaTopicTag {
    key: string;
    value?: string;
}

export interface GetM3AggregatorComponent {
    component: string;
    connectionUri: string;
    host: string;
    kafkaAuthenticationMethod: string;
    port: number;
    route: string;
    ssl: boolean;
    usage: string;
}

export interface GetM3AggregatorM3aggregator {
}

export interface GetM3AggregatorM3aggregatorUserConfig {
    customDomain?: string;
    ipFilterObjects?: outputs.GetM3AggregatorM3aggregatorUserConfigIpFilterObject[];
    ipFilterStrings?: string[];
    /**
     * @deprecated This will be removed in v5.0.0 and replaced with ip_filter_string instead.
     */
    ipFilters?: string[];
    /**
     * @deprecated Usage of this field is discouraged.
     */
    m3Version?: string;
    m3aggregatorVersion?: string;
    serviceLog?: boolean;
    staticIps?: boolean;
}

export interface GetM3AggregatorM3aggregatorUserConfigIpFilterObject {
    description?: string;
    network: string;
}

export interface GetM3AggregatorServiceIntegration {
    integrationType: string;
    sourceServiceName: string;
}

export interface GetM3AggregatorTag {
    key: string;
    value: string;
}

export interface GetM3AggregatorTechEmail {
    email: string;
}

export interface GetM3DbComponent {
    component: string;
    connectionUri: string;
    host: string;
    kafkaAuthenticationMethod: string;
    port: number;
    route: string;
    ssl: boolean;
    usage: string;
}

export interface GetM3DbM3db {
}

export interface GetM3DbM3dbUserConfig {
    additionalBackupRegions?: string;
    customDomain?: string;
    ipFilterObjects?: outputs.GetM3DbM3dbUserConfigIpFilterObject[];
    ipFilterStrings?: string[];
    /**
     * @deprecated This will be removed in v5.0.0 and replaced with ip_filter_string instead.
     */
    ipFilters?: string[];
    limits?: outputs.GetM3DbM3dbUserConfigLimits;
    m3?: outputs.GetM3DbM3dbUserConfigM3;
    /**
     * @deprecated Usage of this field is discouraged.
     */
    m3Version?: string;
    m3coordinatorEnableGraphiteCarbonIngest?: boolean;
    m3dbVersion?: string;
    namespaces?: outputs.GetM3DbM3dbUserConfigNamespace[];
    privateAccess?: outputs.GetM3DbM3dbUserConfigPrivateAccess;
    projectToForkFrom?: string;
    publicAccess?: outputs.GetM3DbM3dbUserConfigPublicAccess;
    rules?: outputs.GetM3DbM3dbUserConfigRules;
    serviceLog?: boolean;
    serviceToForkFrom?: string;
    staticIps?: boolean;
}

export interface GetM3DbM3dbUserConfigIpFilterObject {
    description?: string;
    network: string;
}

export interface GetM3DbM3dbUserConfigLimits {
    maxRecentlyQueriedSeriesBlocks?: number;
    maxRecentlyQueriedSeriesDiskBytesRead?: number;
    maxRecentlyQueriedSeriesLookback?: string;
    queryDocs?: number;
    queryRequireExhaustive?: boolean;
    querySeries?: number;
}

export interface GetM3DbM3dbUserConfigM3 {
    tagOptions?: outputs.GetM3DbM3dbUserConfigM3TagOptions;
}

export interface GetM3DbM3dbUserConfigM3TagOptions {
    allowTagNameDuplicates?: boolean;
    allowTagValueEmpty?: boolean;
}

export interface GetM3DbM3dbUserConfigNamespace {
    name: string;
    options?: outputs.GetM3DbM3dbUserConfigNamespaceOptions;
    resolution?: string;
    type: string;
}

export interface GetM3DbM3dbUserConfigNamespaceOptions {
    retentionOptions?: outputs.GetM3DbM3dbUserConfigNamespaceOptionsRetentionOptions;
    snapshotEnabled?: boolean;
    writesToCommitlog?: boolean;
}

export interface GetM3DbM3dbUserConfigNamespaceOptionsRetentionOptions {
    blockDataExpiryDuration?: string;
    blocksizeDuration?: string;
    bufferFutureDuration?: string;
    bufferPastDuration?: string;
    retentionPeriodDuration?: string;
}

export interface GetM3DbM3dbUserConfigPrivateAccess {
    m3coordinator?: boolean;
}

export interface GetM3DbM3dbUserConfigPublicAccess {
    m3coordinator?: boolean;
}

export interface GetM3DbM3dbUserConfigRules {
    mappings?: outputs.GetM3DbM3dbUserConfigRulesMapping[];
}

export interface GetM3DbM3dbUserConfigRulesMapping {
    aggregations?: string[];
    drop?: boolean;
    filter: string;
    name?: string;
    /**
     * @deprecated This will be removed in v5.0.0 and replaced with namespaces_string instead.
     */
    namespaces?: string[];
    namespacesObjects?: outputs.GetM3DbM3dbUserConfigRulesMappingNamespacesObject[];
    namespacesStrings?: string[];
    tags?: outputs.GetM3DbM3dbUserConfigRulesMappingTag[];
}

export interface GetM3DbM3dbUserConfigRulesMappingNamespacesObject {
    resolution?: string;
    retention?: string;
}

export interface GetM3DbM3dbUserConfigRulesMappingTag {
    name: string;
    value: string;
}

export interface GetM3DbServiceIntegration {
    integrationType: string;
    sourceServiceName: string;
}

export interface GetM3DbTag {
    key: string;
    value: string;
}

export interface GetM3DbTechEmail {
    email: string;
}

export interface GetMySqlComponent {
    component: string;
    connectionUri: string;
    host: string;
    kafkaAuthenticationMethod: string;
    port: number;
    route: string;
    ssl: boolean;
    usage: string;
}

export interface GetMySqlMysql {
}

export interface GetMySqlMysqlUserConfig {
    additionalBackupRegions?: string;
    adminPassword?: string;
    adminUsername?: string;
    backupHour?: number;
    backupMinute?: number;
    binlogRetentionPeriod?: number;
    ipFilterObjects?: outputs.GetMySqlMysqlUserConfigIpFilterObject[];
    ipFilterStrings?: string[];
    /**
     * @deprecated This will be removed in v5.0.0 and replaced with ip_filter_string instead.
     */
    ipFilters?: string[];
    migration?: outputs.GetMySqlMysqlUserConfigMigration;
    mysql?: outputs.GetMySqlMysqlUserConfigMysql;
    mysqlVersion?: string;
    privateAccess?: outputs.GetMySqlMysqlUserConfigPrivateAccess;
    privatelinkAccess?: outputs.GetMySqlMysqlUserConfigPrivatelinkAccess;
    projectToForkFrom?: string;
    publicAccess?: outputs.GetMySqlMysqlUserConfigPublicAccess;
    recoveryTargetTime?: string;
    serviceLog?: boolean;
    serviceToForkFrom?: string;
    staticIps?: boolean;
}

export interface GetMySqlMysqlUserConfigIpFilterObject {
    description?: string;
    network: string;
}

export interface GetMySqlMysqlUserConfigMigration {
    dbname?: string;
    host: string;
    ignoreDbs?: string;
    method?: string;
    password?: string;
    port: number;
    ssl?: boolean;
    username?: string;
}

export interface GetMySqlMysqlUserConfigMysql {
    connectTimeout?: number;
    defaultTimeZone?: string;
    groupConcatMaxLen?: number;
    informationSchemaStatsExpiry?: number;
    innodbChangeBufferMaxSize?: number;
    innodbFlushNeighbors?: number;
    innodbFtMinTokenSize?: number;
    innodbFtServerStopwordTable?: string;
    innodbLockWaitTimeout?: number;
    innodbLogBufferSize?: number;
    innodbOnlineAlterLogMaxSize?: number;
    innodbPrintAllDeadlocks?: boolean;
    innodbReadIoThreads?: number;
    innodbRollbackOnTimeout?: boolean;
    innodbThreadConcurrency?: number;
    innodbWriteIoThreads?: number;
    interactiveTimeout?: number;
    internalTmpMemStorageEngine?: string;
    longQueryTime?: number;
    maxAllowedPacket?: number;
    maxHeapTableSize?: number;
    netBufferLength?: number;
    netReadTimeout?: number;
    netWriteTimeout?: number;
    slowQueryLog?: boolean;
    sortBufferSize?: number;
    sqlMode?: string;
    sqlRequirePrimaryKey?: boolean;
    tmpTableSize?: number;
    waitTimeout?: number;
}

export interface GetMySqlMysqlUserConfigPrivateAccess {
    mysql?: boolean;
    mysqlx?: boolean;
    prometheus?: boolean;
}

export interface GetMySqlMysqlUserConfigPrivatelinkAccess {
    mysql?: boolean;
    mysqlx?: boolean;
    prometheus?: boolean;
}

export interface GetMySqlMysqlUserConfigPublicAccess {
    mysql?: boolean;
    mysqlx?: boolean;
    prometheus?: boolean;
}

export interface GetMySqlServiceIntegration {
    integrationType: string;
    sourceServiceName: string;
}

export interface GetMySqlTag {
    key: string;
    value: string;
}

export interface GetMySqlTechEmail {
    email: string;
}

export interface GetOpenSearchComponent {
    component: string;
    connectionUri: string;
    host: string;
    kafkaAuthenticationMethod: string;
    port: number;
    route: string;
    ssl: boolean;
    usage: string;
}

export interface GetOpenSearchOpensearch {
    opensearchDashboardsUri: string;
}

export interface GetOpenSearchOpensearchUserConfig {
    additionalBackupRegions?: string;
    customDomain?: string;
    /**
     * @deprecated Usage of this field is discouraged.
     */
    disableReplicationFactorAdjustment?: boolean;
    indexPatterns?: outputs.GetOpenSearchOpensearchUserConfigIndexPattern[];
    indexTemplate?: outputs.GetOpenSearchOpensearchUserConfigIndexTemplate;
    ipFilterObjects?: outputs.GetOpenSearchOpensearchUserConfigIpFilterObject[];
    ipFilterStrings?: string[];
    /**
     * @deprecated This will be removed in v5.0.0 and replaced with ip_filter_string instead.
     */
    ipFilters?: string[];
    keepIndexRefreshInterval?: boolean;
    /**
     * @deprecated Usage of this field is discouraged.
     */
    maxIndexCount?: number;
    openid?: outputs.GetOpenSearchOpensearchUserConfigOpenid;
    opensearch?: outputs.GetOpenSearchOpensearchUserConfigOpensearch;
    opensearchDashboards?: outputs.GetOpenSearchOpensearchUserConfigOpensearchDashboards;
    opensearchVersion?: string;
    privateAccess?: outputs.GetOpenSearchOpensearchUserConfigPrivateAccess;
    privatelinkAccess?: outputs.GetOpenSearchOpensearchUserConfigPrivatelinkAccess;
    projectToForkFrom?: string;
    publicAccess?: outputs.GetOpenSearchOpensearchUserConfigPublicAccess;
    recoveryBasebackupName?: string;
    saml?: outputs.GetOpenSearchOpensearchUserConfigSaml;
    serviceLog?: boolean;
    serviceToForkFrom?: string;
    staticIps?: boolean;
}

export interface GetOpenSearchOpensearchUserConfigIndexPattern {
    maxIndexCount: number;
    pattern: string;
    sortingAlgorithm?: string;
}

export interface GetOpenSearchOpensearchUserConfigIndexTemplate {
    mappingNestedObjectsLimit?: number;
    numberOfReplicas?: number;
    numberOfShards?: number;
}

export interface GetOpenSearchOpensearchUserConfigIpFilterObject {
    description?: string;
    network: string;
}

export interface GetOpenSearchOpensearchUserConfigOpenid {
    clientId: string;
    clientSecret: string;
    connectUrl: string;
    enabled: boolean;
    header?: string;
    jwtHeader?: string;
    jwtUrlParameter?: string;
    refreshRateLimitCount?: number;
    refreshRateLimitTimeWindowMs?: number;
    rolesKey?: string;
    scope?: string;
    subjectKey?: string;
}

export interface GetOpenSearchOpensearchUserConfigOpensearch {
    actionAutoCreateIndexEnabled?: boolean;
    actionDestructiveRequiresName?: boolean;
    authFailureListeners?: outputs.GetOpenSearchOpensearchUserConfigOpensearchAuthFailureListeners;
    clusterMaxShardsPerNode?: number;
    clusterRoutingAllocationNodeConcurrentRecoveries?: number;
    emailSenderName?: string;
    emailSenderPassword?: string;
    emailSenderUsername?: string;
    enableSecurityAudit?: boolean;
    httpMaxContentLength?: number;
    httpMaxHeaderSize?: number;
    httpMaxInitialLineLength?: number;
    indicesFielddataCacheSize?: number;
    indicesMemoryIndexBufferSize?: number;
    indicesMemoryMaxIndexBufferSize?: number;
    indicesMemoryMinIndexBufferSize?: number;
    indicesQueriesCacheSize?: number;
    indicesQueryBoolMaxClauseCount?: number;
    indicesRecoveryMaxBytesPerSec?: number;
    indicesRecoveryMaxConcurrentFileChunks?: number;
    ismEnabled?: boolean;
    ismHistoryEnabled?: boolean;
    ismHistoryMaxAge?: number;
    ismHistoryMaxDocs?: number;
    ismHistoryRolloverCheckPeriod?: number;
    ismHistoryRolloverRetentionPeriod?: number;
    overrideMainResponseVersion?: boolean;
    reindexRemoteWhitelists?: string[];
    scriptMaxCompilationsRate?: string;
    searchMaxBuckets?: number;
    threadPoolAnalyzeQueueSize?: number;
    threadPoolAnalyzeSize?: number;
    threadPoolForceMergeSize?: number;
    threadPoolGetQueueSize?: number;
    threadPoolGetSize?: number;
    threadPoolSearchQueueSize?: number;
    threadPoolSearchSize?: number;
    threadPoolSearchThrottledQueueSize?: number;
    threadPoolSearchThrottledSize?: number;
    threadPoolWriteQueueSize?: number;
    threadPoolWriteSize?: number;
}

export interface GetOpenSearchOpensearchUserConfigOpensearchAuthFailureListeners {
    internalAuthenticationBackendLimiting?: outputs.GetOpenSearchOpensearchUserConfigOpensearchAuthFailureListenersInternalAuthenticationBackendLimiting;
    ipRateLimiting?: outputs.GetOpenSearchOpensearchUserConfigOpensearchAuthFailureListenersIpRateLimiting;
}

export interface GetOpenSearchOpensearchUserConfigOpensearchAuthFailureListenersInternalAuthenticationBackendLimiting {
    allowedTries?: number;
    authenticationBackend?: string;
    blockExpirySeconds?: number;
    maxBlockedClients?: number;
    maxTrackedClients?: number;
    timeWindowSeconds?: number;
    type?: string;
}

export interface GetOpenSearchOpensearchUserConfigOpensearchAuthFailureListenersIpRateLimiting {
    allowedTries?: number;
    blockExpirySeconds?: number;
    maxBlockedClients?: number;
    maxTrackedClients?: number;
    timeWindowSeconds?: number;
    type?: string;
}

export interface GetOpenSearchOpensearchUserConfigOpensearchDashboards {
    enabled?: boolean;
    maxOldSpaceSize?: number;
    opensearchRequestTimeout?: number;
}

export interface GetOpenSearchOpensearchUserConfigPrivateAccess {
    opensearch?: boolean;
    opensearchDashboards?: boolean;
    prometheus?: boolean;
}

export interface GetOpenSearchOpensearchUserConfigPrivatelinkAccess {
    opensearch?: boolean;
    opensearchDashboards?: boolean;
    prometheus?: boolean;
}

export interface GetOpenSearchOpensearchUserConfigPublicAccess {
    opensearch?: boolean;
    opensearchDashboards?: boolean;
    prometheus?: boolean;
}

export interface GetOpenSearchOpensearchUserConfigSaml {
    enabled: boolean;
    idpEntityId: string;
    idpMetadataUrl: string;
    idpPemtrustedcasContent?: string;
    rolesKey?: string;
    spEntityId: string;
    subjectKey?: string;
}

export interface GetOpenSearchServiceIntegration {
    integrationType: string;
    sourceServiceName: string;
}

export interface GetOpenSearchTag {
    key: string;
    value: string;
}

export interface GetOpenSearchTechEmail {
    email: string;
}

export interface GetPgComponent {
    component: string;
    connectionUri: string;
    host: string;
    kafkaAuthenticationMethod: string;
    port: number;
    route: string;
    ssl: boolean;
    usage: string;
}

export interface GetPgPg {
    dbname: string;
    host: string;
    maxConnections: number;
    password: string;
    port: number;
    replicaUri: string;
    sslmode: string;
    uri: string;
    user: string;
}

export interface GetPgPgUserConfig {
    additionalBackupRegions?: string;
    adminPassword?: string;
    adminUsername?: string;
    backupHour?: number;
    backupMinute?: number;
    enableIpv6?: boolean;
    ipFilterObjects?: outputs.GetPgPgUserConfigIpFilterObject[];
    ipFilterStrings?: string[];
    /**
     * @deprecated This will be removed in v5.0.0 and replaced with ip_filter_string instead.
     */
    ipFilters?: string[];
    migration?: outputs.GetPgPgUserConfigMigration;
    pg?: outputs.GetPgPgUserConfigPg;
    pgQualstats?: outputs.GetPgPgUserConfigPgQualstats;
    /**
     * @deprecated Usage of this field is discouraged.
     */
    pgReadReplica?: boolean;
    /**
     * @deprecated Usage of this field is discouraged.
     */
    pgServiceToForkFrom?: string;
    pgStatMonitorEnable?: boolean;
    pgVersion?: string;
    pgbouncer?: outputs.GetPgPgUserConfigPgbouncer;
    pglookout?: outputs.GetPgPgUserConfigPglookout;
    privateAccess?: outputs.GetPgPgUserConfigPrivateAccess;
    privatelinkAccess?: outputs.GetPgPgUserConfigPrivatelinkAccess;
    projectToForkFrom?: string;
    publicAccess?: outputs.GetPgPgUserConfigPublicAccess;
    recoveryTargetTime?: string;
    serviceLog?: boolean;
    serviceToForkFrom?: string;
    sharedBuffersPercentage?: number;
    staticIps?: boolean;
    synchronousReplication?: string;
    timescaledb?: outputs.GetPgPgUserConfigTimescaledb;
    variant?: string;
    workMem?: number;
}

export interface GetPgPgUserConfigIpFilterObject {
    description?: string;
    network: string;
}

export interface GetPgPgUserConfigMigration {
    dbname?: string;
    host: string;
    ignoreDbs?: string;
    method?: string;
    password?: string;
    port: number;
    ssl?: boolean;
    username?: string;
}

export interface GetPgPgUserConfigPg {
    autovacuumAnalyzeScaleFactor?: number;
    autovacuumAnalyzeThreshold?: number;
    autovacuumFreezeMaxAge?: number;
    autovacuumMaxWorkers?: number;
    autovacuumNaptime?: number;
    autovacuumVacuumCostDelay?: number;
    autovacuumVacuumCostLimit?: number;
    autovacuumVacuumScaleFactor?: number;
    autovacuumVacuumThreshold?: number;
    bgwriterDelay?: number;
    bgwriterFlushAfter?: number;
    bgwriterLruMaxpages?: number;
    bgwriterLruMultiplier?: number;
    deadlockTimeout?: number;
    defaultToastCompression?: string;
    idleInTransactionSessionTimeout?: number;
    jit?: boolean;
    logAutovacuumMinDuration?: number;
    logErrorVerbosity?: string;
    logLinePrefix?: string;
    logMinDurationStatement?: number;
    logTempFiles?: number;
    maxFilesPerProcess?: number;
    maxLocksPerTransaction?: number;
    maxLogicalReplicationWorkers?: number;
    maxParallelWorkers?: number;
    maxParallelWorkersPerGather?: number;
    maxPredLocksPerTransaction?: number;
    maxPreparedTransactions?: number;
    maxReplicationSlots?: number;
    maxSlotWalKeepSize?: number;
    maxStackDepth?: number;
    maxStandbyArchiveDelay?: number;
    maxStandbyStreamingDelay?: number;
    maxWalSenders?: number;
    maxWorkerProcesses?: number;
    pgPartmanBgwDotInterval?: number;
    pgPartmanBgwDotRole?: string;
    pgStatMonitorDotPgsmEnableQueryPlan?: boolean;
    pgStatMonitorDotPgsmMaxBuckets?: number;
    pgStatStatementsDotTrack?: string;
    tempFileLimit?: number;
    timezone?: string;
    trackActivityQuerySize?: number;
    trackCommitTimestamp?: string;
    trackFunctions?: string;
    trackIoTiming?: string;
    walSenderTimeout?: number;
    walWriterDelay?: number;
}

export interface GetPgPgUserConfigPgQualstats {
    enabled?: boolean;
    minErrEstimateNum?: number;
    minErrEstimateRatio?: number;
    trackConstants?: boolean;
    trackPgCatalog?: boolean;
}

export interface GetPgPgUserConfigPgbouncer {
    autodbIdleTimeout?: number;
    autodbMaxDbConnections?: number;
    autodbPoolMode?: string;
    autodbPoolSize?: number;
    ignoreStartupParameters?: string[];
    minPoolSize?: number;
    serverIdleTimeout?: number;
    serverLifetime?: number;
    serverResetQueryAlways?: boolean;
}

export interface GetPgPgUserConfigPglookout {
    maxFailoverReplicationTimeLag?: number;
}

export interface GetPgPgUserConfigPrivateAccess {
    pg?: boolean;
    pgbouncer?: boolean;
    prometheus?: boolean;
}

export interface GetPgPgUserConfigPrivatelinkAccess {
    pg?: boolean;
    pgbouncer?: boolean;
    prometheus?: boolean;
}

export interface GetPgPgUserConfigPublicAccess {
    pg?: boolean;
    pgbouncer?: boolean;
    prometheus?: boolean;
}

export interface GetPgPgUserConfigTimescaledb {
    maxBackgroundWorkers?: number;
}

export interface GetPgServiceIntegration {
    integrationType: string;
    sourceServiceName: string;
}

export interface GetPgTag {
    key: string;
    value: string;
}

export interface GetPgTechEmail {
    email: string;
}

export interface GetProjectTag {
    key: string;
    value: string;
}

export interface GetRedisComponent {
    component: string;
    connectionUri: string;
    host: string;
    kafkaAuthenticationMethod: string;
    port: number;
    route: string;
    ssl: boolean;
    usage: string;
}

export interface GetRedisRedi {
}

export interface GetRedisRedisUserConfig {
    additionalBackupRegions?: string;
    ipFilterObjects?: outputs.GetRedisRedisUserConfigIpFilterObject[];
    ipFilterStrings?: string[];
    /**
     * @deprecated This will be removed in v5.0.0 and replaced with ip_filter_string instead.
     */
    ipFilters?: string[];
    migration?: outputs.GetRedisRedisUserConfigMigration;
    privateAccess?: outputs.GetRedisRedisUserConfigPrivateAccess;
    privatelinkAccess?: outputs.GetRedisRedisUserConfigPrivatelinkAccess;
    projectToForkFrom?: string;
    publicAccess?: outputs.GetRedisRedisUserConfigPublicAccess;
    recoveryBasebackupName?: string;
    redisAclChannelsDefault?: string;
    redisIoThreads?: number;
    redisLfuDecayTime?: number;
    redisLfuLogFactor?: number;
    redisMaxmemoryPolicy?: string;
    redisNotifyKeyspaceEvents?: string;
    redisNumberOfDatabases?: number;
    redisPersistence?: string;
    redisPubsubClientOutputBufferLimit?: number;
    redisSsl?: boolean;
    redisTimeout?: number;
    serviceLog?: boolean;
    serviceToForkFrom?: string;
    staticIps?: boolean;
}

export interface GetRedisRedisUserConfigIpFilterObject {
    description?: string;
    network: string;
}

export interface GetRedisRedisUserConfigMigration {
    dbname?: string;
    host: string;
    ignoreDbs?: string;
    method?: string;
    password?: string;
    port: number;
    ssl?: boolean;
    username?: string;
}

export interface GetRedisRedisUserConfigPrivateAccess {
    prometheus?: boolean;
    redis?: boolean;
}

export interface GetRedisRedisUserConfigPrivatelinkAccess {
    prometheus?: boolean;
    redis?: boolean;
}

export interface GetRedisRedisUserConfigPublicAccess {
    prometheus?: boolean;
    redis?: boolean;
}

export interface GetRedisServiceIntegration {
    integrationType: string;
    sourceServiceName: string;
}

export interface GetRedisTag {
    key: string;
    value: string;
}

export interface GetRedisTechEmail {
    email: string;
}

export interface GetServiceIntegrationClickhouseKafkaUserConfig {
    tables?: outputs.GetServiceIntegrationClickhouseKafkaUserConfigTable[];
}

export interface GetServiceIntegrationClickhouseKafkaUserConfigTable {
    autoOffsetReset?: string;
    columns?: outputs.GetServiceIntegrationClickhouseKafkaUserConfigTableColumn[];
    dataFormat: string;
    dateTimeInputFormat?: string;
    groupName: string;
    handleErrorMode?: string;
    maxBlockSize?: number;
    maxRowsPerMessage?: number;
    name: string;
    numConsumers?: number;
    pollMaxBatchSize?: number;
    skipBrokenMessages?: number;
    topics?: outputs.GetServiceIntegrationClickhouseKafkaUserConfigTableTopic[];
}

export interface GetServiceIntegrationClickhouseKafkaUserConfigTableColumn {
    name: string;
    type: string;
}

export interface GetServiceIntegrationClickhouseKafkaUserConfigTableTopic {
    name: string;
}

export interface GetServiceIntegrationClickhousePostgresqlUserConfig {
    databases?: outputs.GetServiceIntegrationClickhousePostgresqlUserConfigDatabase[];
}

export interface GetServiceIntegrationClickhousePostgresqlUserConfigDatabase {
    database?: string;
    schema?: string;
}

export interface GetServiceIntegrationDatadogUserConfig {
    datadogDbmEnabled?: boolean;
    datadogTags?: outputs.GetServiceIntegrationDatadogUserConfigDatadogTag[];
    excludeConsumerGroups?: string[];
    excludeTopics?: string[];
    includeConsumerGroups?: string[];
    includeTopics?: string[];
    kafkaCustomMetrics?: string[];
    maxJmxMetrics?: number;
    opensearch?: outputs.GetServiceIntegrationDatadogUserConfigOpensearch;
    redis?: outputs.GetServiceIntegrationDatadogUserConfigRedis;
}

export interface GetServiceIntegrationDatadogUserConfigDatadogTag {
    comment?: string;
    tag: string;
}

export interface GetServiceIntegrationDatadogUserConfigOpensearch {
    indexStatsEnabled?: boolean;
    pendingTaskStatsEnabled?: boolean;
    pshardStatsEnabled?: boolean;
}

export interface GetServiceIntegrationDatadogUserConfigRedis {
    commandStatsEnabled?: boolean;
}

export interface GetServiceIntegrationEndpointDatadogUserConfig {
    datadogApiKey: string;
    datadogTags?: outputs.GetServiceIntegrationEndpointDatadogUserConfigDatadogTag[];
    disableConsumerStats?: boolean;
    kafkaConsumerCheckInstances?: number;
    kafkaConsumerStatsTimeout?: number;
    maxPartitionContexts?: number;
    site?: string;
}

export interface GetServiceIntegrationEndpointDatadogUserConfigDatadogTag {
    comment?: string;
    tag: string;
}

export interface GetServiceIntegrationEndpointExternalAwsCloudwatchLogsUserConfig {
    accessKey: string;
    logGroupName?: string;
    region: string;
    secretKey: string;
}

export interface GetServiceIntegrationEndpointExternalAwsCloudwatchMetricsUserConfig {
    accessKey: string;
    namespace: string;
    region: string;
    secretKey: string;
}

export interface GetServiceIntegrationEndpointExternalElasticsearchLogsUserConfig {
    ca?: string;
    indexDaysMax?: number;
    indexPrefix: string;
    timeout?: number;
    url: string;
}

export interface GetServiceIntegrationEndpointExternalGoogleCloudBigquery {
    projectId: string;
    serviceAccountCredentials: string;
}

export interface GetServiceIntegrationEndpointExternalGoogleCloudLoggingUserConfig {
    logId: string;
    projectId: string;
    serviceAccountCredentials: string;
}

export interface GetServiceIntegrationEndpointExternalKafkaUserConfig {
    bootstrapServers: string;
    saslMechanism?: string;
    saslPlainPassword?: string;
    saslPlainUsername?: string;
    securityProtocol: string;
    sslCaCert?: string;
    sslClientCert?: string;
    sslClientKey?: string;
    sslEndpointIdentificationAlgorithm?: string;
}

export interface GetServiceIntegrationEndpointExternalOpensearchLogsUserConfig {
    ca?: string;
    indexDaysMax?: number;
    indexPrefix: string;
    timeout?: number;
    url: string;
}

export interface GetServiceIntegrationEndpointExternalPostgresql {
    host: string;
    password: string;
    port: number;
    sslMode?: string;
    sslRootCert?: string;
    username: string;
}

export interface GetServiceIntegrationEndpointExternalSchemaRegistryUserConfig {
    authentication: string;
    basicAuthPassword?: string;
    basicAuthUsername?: string;
    url: string;
}

export interface GetServiceIntegrationEndpointJolokiaUserConfig {
    basicAuthPassword?: string;
    basicAuthUsername?: string;
}

export interface GetServiceIntegrationEndpointPrometheusUserConfig {
    basicAuthPassword?: string;
    basicAuthUsername?: string;
}

export interface GetServiceIntegrationEndpointRsyslogUserConfig {
    ca?: string;
    cert?: string;
    format: string;
    key?: string;
    logline?: string;
    port: number;
    sd?: string;
    server: string;
    tls: boolean;
}

export interface GetServiceIntegrationExternalAwsCloudwatchMetricsUserConfig {
    droppedMetrics?: outputs.GetServiceIntegrationExternalAwsCloudwatchMetricsUserConfigDroppedMetric[];
    extraMetrics?: outputs.GetServiceIntegrationExternalAwsCloudwatchMetricsUserConfigExtraMetric[];
}

export interface GetServiceIntegrationExternalAwsCloudwatchMetricsUserConfigDroppedMetric {
    field: string;
    metric: string;
}

export interface GetServiceIntegrationExternalAwsCloudwatchMetricsUserConfigExtraMetric {
    field: string;
    metric: string;
}

export interface GetServiceIntegrationKafkaConnectUserConfig {
    kafkaConnect?: outputs.GetServiceIntegrationKafkaConnectUserConfigKafkaConnect;
}

export interface GetServiceIntegrationKafkaConnectUserConfigKafkaConnect {
    configStorageTopic?: string;
    groupId?: string;
    offsetStorageTopic?: string;
    statusStorageTopic?: string;
}

export interface GetServiceIntegrationKafkaLogsUserConfig {
    kafkaTopic: string;
    selectedLogFields?: string[];
}

export interface GetServiceIntegrationKafkaMirrormakerUserConfig {
    clusterAlias?: string;
    kafkaMirrormaker?: outputs.GetServiceIntegrationKafkaMirrormakerUserConfigKafkaMirrormaker;
}

export interface GetServiceIntegrationKafkaMirrormakerUserConfigKafkaMirrormaker {
    consumerFetchMinBytes?: number;
    producerBatchSize?: number;
    producerBufferMemory?: number;
    producerCompressionType?: string;
    producerLingerMs?: number;
    producerMaxRequestSize?: number;
}

export interface GetServiceIntegrationLogsUserConfig {
    elasticsearchIndexDaysMax?: number;
    elasticsearchIndexPrefix?: string;
    selectedLogFields?: string[];
}

export interface GetServiceIntegrationMetricsUserConfig {
    database?: string;
    retentionDays?: number;
    roUsername?: string;
    sourceMysql?: outputs.GetServiceIntegrationMetricsUserConfigSourceMysql;
    username?: string;
}

export interface GetServiceIntegrationMetricsUserConfigSourceMysql {
    telegraf?: outputs.GetServiceIntegrationMetricsUserConfigSourceMysqlTelegraf;
}

export interface GetServiceIntegrationMetricsUserConfigSourceMysqlTelegraf {
    gatherEventWaits?: boolean;
    gatherFileEventsStats?: boolean;
    gatherIndexIoWaits?: boolean;
    gatherInfoSchemaAutoInc?: boolean;
    gatherInnodbMetrics?: boolean;
    gatherPerfEventsStatements?: boolean;
    gatherProcessList?: boolean;
    gatherSlaveStatus?: boolean;
    gatherTableIoWaits?: boolean;
    gatherTableLockWaits?: boolean;
    gatherTableSchema?: boolean;
    perfEventsStatementsDigestTextLimit?: number;
    perfEventsStatementsLimit?: number;
    perfEventsStatementsTimeLimit?: number;
}

export interface GrafanaComponent {
    component: string;
    connectionUri: string;
    host: string;
    kafkaAuthenticationMethod: string;
    port: number;
    route: string;
    ssl: boolean;
    usage: string;
}

export interface GrafanaGrafana {
}

export interface GrafanaGrafanaUserConfig {
    additionalBackupRegions?: string;
    alertingEnabled?: boolean;
    alertingErrorOrTimeout?: string;
    alertingMaxAnnotationsToKeep?: number;
    alertingNodataOrNullvalues?: string;
    allowEmbedding?: boolean;
    authAzuread?: outputs.GrafanaGrafanaUserConfigAuthAzuread;
    authBasicEnabled?: boolean;
    authGenericOauth?: outputs.GrafanaGrafanaUserConfigAuthGenericOauth;
    authGithub?: outputs.GrafanaGrafanaUserConfigAuthGithub;
    authGitlab?: outputs.GrafanaGrafanaUserConfigAuthGitlab;
    authGoogle?: outputs.GrafanaGrafanaUserConfigAuthGoogle;
    cookieSamesite?: string;
    customDomain?: string;
    dashboardPreviewsEnabled?: boolean;
    dashboardsMinRefreshInterval?: string;
    dashboardsVersionsToKeep?: number;
    dataproxySendUserHeader?: boolean;
    dataproxyTimeout?: number;
    dateFormats?: outputs.GrafanaGrafanaUserConfigDateFormats;
    disableGravatar?: boolean;
    editorsCanAdmin?: boolean;
    externalImageStorage?: outputs.GrafanaGrafanaUserConfigExternalImageStorage;
    googleAnalyticsUaId?: string;
    ipFilterObjects?: outputs.GrafanaGrafanaUserConfigIpFilterObject[];
    ipFilterStrings?: string[];
    /**
     * @deprecated This will be removed in v5.0.0 and replaced with ip_filter_string instead.
     */
    ipFilters?: string[];
    metricsEnabled?: boolean;
    oauthAllowInsecureEmailLookup?: boolean;
    privateAccess?: outputs.GrafanaGrafanaUserConfigPrivateAccess;
    privatelinkAccess?: outputs.GrafanaGrafanaUserConfigPrivatelinkAccess;
    projectToForkFrom?: string;
    publicAccess?: outputs.GrafanaGrafanaUserConfigPublicAccess;
    recoveryBasebackupName?: string;
    serviceLog?: boolean;
    serviceToForkFrom?: string;
    smtpServer?: outputs.GrafanaGrafanaUserConfigSmtpServer;
    staticIps?: boolean;
    unifiedAlertingEnabled?: boolean;
    userAutoAssignOrg?: boolean;
    userAutoAssignOrgRole?: string;
    viewersCanEdit?: boolean;
}

export interface GrafanaGrafanaUserConfigAuthAzuread {
    allowSignUp?: boolean;
    allowedDomains?: string[];
    allowedGroups?: string[];
    authUrl: string;
    clientId: string;
    clientSecret: string;
    tokenUrl: string;
}

export interface GrafanaGrafanaUserConfigAuthGenericOauth {
    allowSignUp?: boolean;
    allowedDomains?: string[];
    allowedOrganizations?: string[];
    apiUrl: string;
    authUrl: string;
    autoLogin?: boolean;
    clientId: string;
    clientSecret: string;
    name?: string;
    scopes?: string[];
    tokenUrl: string;
}

export interface GrafanaGrafanaUserConfigAuthGithub {
    allowSignUp?: boolean;
    allowedOrganizations?: string[];
    clientId: string;
    clientSecret: string;
    skipOrgRoleSync?: boolean;
    teamIds?: number[];
}

export interface GrafanaGrafanaUserConfigAuthGitlab {
    allowSignUp?: boolean;
    allowedGroups?: string[];
    apiUrl?: string;
    authUrl?: string;
    clientId: string;
    clientSecret: string;
    tokenUrl?: string;
}

export interface GrafanaGrafanaUserConfigAuthGoogle {
    allowSignUp?: boolean;
    allowedDomains?: string[];
    clientId: string;
    clientSecret: string;
}

export interface GrafanaGrafanaUserConfigDateFormats {
    defaultTimezone?: string;
    fullDate?: string;
    intervalDay?: string;
    intervalHour?: string;
    intervalMinute?: string;
    intervalMonth?: string;
    intervalSecond?: string;
    intervalYear?: string;
}

export interface GrafanaGrafanaUserConfigExternalImageStorage {
    accessKey: string;
    bucketUrl: string;
    provider: string;
    secretKey: string;
}

export interface GrafanaGrafanaUserConfigIpFilterObject {
    description?: string;
    network: string;
}

export interface GrafanaGrafanaUserConfigPrivateAccess {
    grafana?: boolean;
}

export interface GrafanaGrafanaUserConfigPrivatelinkAccess {
    grafana?: boolean;
}

export interface GrafanaGrafanaUserConfigPublicAccess {
    grafana?: boolean;
}

export interface GrafanaGrafanaUserConfigSmtpServer {
    fromAddress: string;
    fromName?: string;
    host: string;
    password?: string;
    port: number;
    skipVerify?: boolean;
    starttlsPolicy?: string;
    username?: string;
}

export interface GrafanaServiceIntegration {
    integrationType: string;
    sourceServiceName: string;
}

export interface GrafanaTag {
    key: string;
    value: string;
}

export interface GrafanaTechEmail {
    email: string;
}

export interface InfluxDbComponent {
    component: string;
    connectionUri: string;
    host: string;
    kafkaAuthenticationMethod: string;
    port: number;
    route: string;
    ssl: boolean;
    usage: string;
}

export interface InfluxDbInfluxdb {
    databaseName: string;
}

export interface InfluxDbInfluxdbUserConfig {
    /**
     * Additional Cloud Regions for Backup Replication.
     */
    additionalBackupRegions?: string;
    /**
     * Serve the web frontend using a custom CNAME pointing to the Aiven DNS name.
     */
    customDomain?: string;
    /**
     * influxdb.conf configuration values.
     */
    influxdb?: outputs.InfluxDbInfluxdbUserConfigInfluxdb;
    /**
     * Allow incoming connections from CIDR address block, e.g. '10.20.0.0/16'.
     */
    ipFilterObjects?: outputs.InfluxDbInfluxdbUserConfigIpFilterObject[];
    /**
     * Allow incoming connections from CIDR address block, e.g. '10.20.0.0/16'.
     */
    ipFilterStrings?: string[];
    /**
     * Allow incoming connections from CIDR address block, e.g. '10.20.0.0/16'.
     *
     * @deprecated This will be removed in v5.0.0 and replaced with ip_filter_string instead.
     */
    ipFilters?: string[];
    /**
     * Allow access to selected service ports from private networks.
     */
    privateAccess?: outputs.InfluxDbInfluxdbUserConfigPrivateAccess;
    /**
     * Allow access to selected service components through Privatelink.
     */
    privatelinkAccess?: outputs.InfluxDbInfluxdbUserConfigPrivatelinkAccess;
    /**
     * Name of another project to fork a service from. This has effect only when a new service is being created.
     */
    projectToForkFrom?: string;
    /**
     * Allow access to selected service ports from the public Internet.
     */
    publicAccess?: outputs.InfluxDbInfluxdbUserConfigPublicAccess;
    /**
     * Name of the basebackup to restore in forked service.
     */
    recoveryBasebackupName?: string;
    /**
     * Store logs for the service so that they are available in the HTTP API and console.
     */
    serviceLog?: boolean;
    /**
     * Name of another service to fork from. This has effect only when a new service is being created.
     */
    serviceToForkFrom?: string;
    /**
     * Use static public IP addresses.
     */
    staticIps?: boolean;
}

export interface InfluxDbInfluxdbUserConfigInfluxdb {
    /**
     * The maximum duration in seconds before a query is logged as a slow query. Setting this to 0 (the default) will never log slow queries.
     */
    logQueriesAfter?: number;
    /**
     * Maximum number of connections to InfluxDB. Setting this to 0 (default) means no limit. If using max*connection*limit, it is recommended to set the value to be large enough in order to not block clients unnecessarily.
     */
    maxConnectionLimit?: number;
    /**
     * The maximum number of rows returned in a non-chunked query. Setting this to 0 (the default) allows an unlimited number to be returned.
     */
    maxRowLimit?: number;
    /**
     * The maximum number of `GROUP BY time()` buckets that can be processed in a query. Setting this to 0 (the default) allows an unlimited number to be processed.
     */
    maxSelectBuckets?: number;
    /**
     * The maximum number of points that can be processed in a SELECT statement. Setting this to 0 (the default) allows an unlimited number to be processed.
     */
    maxSelectPoint?: number;
    /**
     * Whether queries should be logged before execution. May log sensitive data contained within a query.
     */
    queryLogEnabled?: boolean;
    /**
     * The maximum duration in seconds before a query is killed. Setting this to 0 (the default) will never kill slow queries.
     */
    queryTimeout?: number;
}

export interface InfluxDbInfluxdbUserConfigIpFilterObject {
    /**
     * Description for IP filter list entry.
     */
    description?: string;
    /**
     * CIDR address block.
     */
    network: string;
}

export interface InfluxDbInfluxdbUserConfigPrivateAccess {
    /**
     * influxdb.conf configuration values.
     */
    influxdb?: boolean;
}

export interface InfluxDbInfluxdbUserConfigPrivatelinkAccess {
    /**
     * influxdb.conf configuration values.
     */
    influxdb?: boolean;
}

export interface InfluxDbInfluxdbUserConfigPublicAccess {
    /**
     * influxdb.conf configuration values.
     */
    influxdb?: boolean;
}

export interface InfluxDbServiceIntegration {
    /**
     * Type of the service integration. The only supported value at the moment is `readReplica`
     */
    integrationType: string;
    /**
     * Name of the source service
     */
    sourceServiceName: string;
}

export interface InfluxDbTag {
    /**
     * Service tag key
     */
    key: string;
    /**
     * Service tag value
     */
    value: string;
}

export interface InfluxDbTechEmail {
    /**
     * An email address to contact for technical issues
     */
    email: string;
}

export interface KafkaComponent {
    component: string;
    connectionUri: string;
    host: string;
    kafkaAuthenticationMethod: string;
    port: number;
    route: string;
    ssl: boolean;
    usage: string;
}

export interface KafkaConnectComponent {
    component: string;
    connectionUri: string;
    host: string;
    kafkaAuthenticationMethod: string;
    port: number;
    route: string;
    ssl: boolean;
    usage: string;
}

export interface KafkaConnectKafkaConnect {
}

export interface KafkaConnectKafkaConnectUserConfig {
    additionalBackupRegions?: string;
    ipFilterObjects?: outputs.KafkaConnectKafkaConnectUserConfigIpFilterObject[];
    ipFilterStrings?: string[];
    /**
     * @deprecated This will be removed in v5.0.0 and replaced with ip_filter_string instead.
     */
    ipFilters?: string[];
    kafkaConnect?: outputs.KafkaConnectKafkaConnectUserConfigKafkaConnect;
    privateAccess?: outputs.KafkaConnectKafkaConnectUserConfigPrivateAccess;
    privatelinkAccess?: outputs.KafkaConnectKafkaConnectUserConfigPrivatelinkAccess;
    publicAccess?: outputs.KafkaConnectKafkaConnectUserConfigPublicAccess;
    serviceLog?: boolean;
    staticIps?: boolean;
}

export interface KafkaConnectKafkaConnectUserConfigIpFilterObject {
    description?: string;
    network: string;
}

export interface KafkaConnectKafkaConnectUserConfigKafkaConnect {
    connectorClientConfigOverridePolicy?: string;
    consumerAutoOffsetReset?: string;
    consumerFetchMaxBytes?: number;
    consumerIsolationLevel?: string;
    consumerMaxPartitionFetchBytes?: number;
    consumerMaxPollIntervalMs?: number;
    consumerMaxPollRecords?: number;
    offsetFlushIntervalMs?: number;
    offsetFlushTimeoutMs?: number;
    producerBatchSize?: number;
    producerBufferMemory?: number;
    producerCompressionType?: string;
    producerLingerMs?: number;
    producerMaxRequestSize?: number;
    scheduledRebalanceMaxDelayMs?: number;
    sessionTimeoutMs?: number;
}

export interface KafkaConnectKafkaConnectUserConfigPrivateAccess {
    kafkaConnect?: boolean;
    prometheus?: boolean;
}

export interface KafkaConnectKafkaConnectUserConfigPrivatelinkAccess {
    jolokia?: boolean;
    kafkaConnect?: boolean;
    prometheus?: boolean;
}

export interface KafkaConnectKafkaConnectUserConfigPublicAccess {
    kafkaConnect?: boolean;
    prometheus?: boolean;
}

export interface KafkaConnectServiceIntegration {
    integrationType: string;
    sourceServiceName: string;
}

export interface KafkaConnectTag {
    key: string;
    value: string;
}

export interface KafkaConnectTechEmail {
    email: string;
}

export interface KafkaConnectorTask {
    connector: string;
    task: number;
}

export interface KafkaKafka {
    accessCert: string;
    accessKey: string;
    connectUri: string;
    restUri: string;
    schemaRegistryUri: string;
}

export interface KafkaKafkaUserConfig {
    /**
     * Additional Cloud Regions for Backup Replication.
     */
    additionalBackupRegions?: string;
    /**
     * Allow access to read Kafka topic messages in the Aiven Console and REST API.
     */
    aivenKafkaTopicMessages?: boolean;
    /**
     * Serve the web frontend using a custom CNAME pointing to the Aiven DNS name.
     */
    customDomain?: string;
    /**
     * Allow incoming connections from CIDR address block, e.g. '10.20.0.0/16'.
     */
    ipFilterObjects?: outputs.KafkaKafkaUserConfigIpFilterObject[];
    /**
     * Allow incoming connections from CIDR address block, e.g. '10.20.0.0/16'.
     */
    ipFilterStrings?: string[];
    /**
     * Allow incoming connections from CIDR address block, e.g. '10.20.0.0/16'.
     *
     * @deprecated This will be removed in v5.0.0 and replaced with ip_filter_string instead.
     */
    ipFilters?: string[];
    /**
     * Kafka broker configuration values.
     */
    kafka?: outputs.KafkaKafkaUserConfigKafka;
    /**
     * Kafka authentication methods.
     */
    kafkaAuthenticationMethods?: outputs.KafkaKafkaUserConfigKafkaAuthenticationMethods;
    /**
     * Enable Kafka Connect service. The default value is `false`.
     */
    kafkaConnect?: boolean;
    /**
     * Kafka Connect configuration values.
     */
    kafkaConnectConfig?: outputs.KafkaKafkaUserConfigKafkaConnectConfig;
    /**
     * Enable Kafka-REST service. The default value is `false`.
     */
    kafkaRest?: boolean;
    /**
     * Enable authorization in Kafka-REST service.
     */
    kafkaRestAuthorization?: boolean;
    /**
     * Kafka REST configuration.
     */
    kafkaRestConfig?: outputs.KafkaKafkaUserConfigKafkaRestConfig;
    /**
     * Kafka major version.
     */
    kafkaVersion?: string;
    /**
     * Allow access to selected service ports from private networks.
     */
    privateAccess?: outputs.KafkaKafkaUserConfigPrivateAccess;
    /**
     * Allow access to selected service components through Privatelink.
     */
    privatelinkAccess?: outputs.KafkaKafkaUserConfigPrivatelinkAccess;
    /**
     * Allow access to selected service ports from the public Internet.
     */
    publicAccess?: outputs.KafkaKafkaUserConfigPublicAccess;
    /**
     * Enable Schema-Registry service. The default value is `false`.
     */
    schemaRegistry?: boolean;
    /**
     * Schema Registry configuration.
     */
    schemaRegistryConfig?: outputs.KafkaKafkaUserConfigSchemaRegistryConfig;
    /**
     * Store logs for the service so that they are available in the HTTP API and console.
     */
    serviceLog?: boolean;
    /**
     * Use static public IP addresses.
     */
    staticIps?: boolean;
    /**
     * Tiered storage configuration.
     */
    tieredStorage?: outputs.KafkaKafkaUserConfigTieredStorage;
}

export interface KafkaKafkaUserConfigIpFilterObject {
    /**
     * Description for IP filter list entry.
     */
    description?: string;
    /**
     * CIDR address block.
     */
    network: string;
}

export interface KafkaKafkaUserConfigKafka {
    /**
     * Enable auto creation of topics.
     */
    autoCreateTopicsEnable?: boolean;
    /**
     * Specify the final compression type for a given topic. This configuration accepts the standard compression codecs ('gzip', 'snappy', 'lz4', 'zstd'). It additionally accepts 'uncompressed' which is equivalent to no compression; and 'producer' which means retain the original compression codec set by the producer.
     */
    compressionType?: string;
    /**
     * Idle connections timeout: the server socket processor threads close the connections that idle for longer than this.
     */
    connectionsMaxIdleMs?: number;
    /**
     * Replication factor for autocreated topics.
     */
    defaultReplicationFactor?: number;
    /**
     * The amount of time, in milliseconds, the group coordinator will wait for more consumers to join a new group before performing the first rebalance. A longer delay means potentially fewer rebalances, but increases the time until processing begins. The default value for this is 3 seconds. During development and testing it might be desirable to set this to 0 in order to not delay test execution time.
     */
    groupInitialRebalanceDelayMs?: number;
    /**
     * The maximum allowed session timeout for registered consumers. Longer timeouts give consumers more time to process messages in between heartbeats at the cost of a longer time to detect failures.
     */
    groupMaxSessionTimeoutMs?: number;
    /**
     * The minimum allowed session timeout for registered consumers. Longer timeouts give consumers more time to process messages in between heartbeats at the cost of a longer time to detect failures.
     */
    groupMinSessionTimeoutMs?: number;
    /**
     * How long are delete records retained?.
     */
    logCleanerDeleteRetentionMs?: number;
    /**
     * The maximum amount of time message will remain uncompacted. Only applicable for logs that are being compacted.
     */
    logCleanerMaxCompactionLagMs?: number;
    /**
     * Controls log compactor frequency. Larger value means more frequent compactions but also more space wasted for logs. Consider setting log.cleaner.max.compaction.lag.ms to enforce compactions sooner, instead of setting a very high value for this option.
     */
    logCleanerMinCleanableRatio?: number;
    /**
     * The minimum time a message will remain uncompacted in the log. Only applicable for logs that are being compacted.
     */
    logCleanerMinCompactionLagMs?: number;
    /**
     * The default cleanup policy for segments beyond the retention window.
     */
    logCleanupPolicy?: string;
    /**
     * The number of messages accumulated on a log partition before messages are flushed to disk.
     */
    logFlushIntervalMessages?: number;
    /**
     * The maximum time in ms that a message in any topic is kept in memory before flushed to disk. If not set, the value in log.flush.scheduler.interval.ms is used.
     */
    logFlushIntervalMs?: number;
    /**
     * The interval with which Kafka adds an entry to the offset index.
     */
    logIndexIntervalBytes?: number;
    /**
     * The maximum size in bytes of the offset index.
     */
    logIndexSizeMaxBytes?: number;
    /**
     * The maximum size of local log segments that can grow for a partition before it gets eligible for deletion. If set to -2, the value of log.retention.bytes is used. The effective value should always be less than or equal to log.retention.bytes value.
     */
    logLocalRetentionBytes?: number;
    /**
     * The number of milliseconds to keep the local log segments before it gets eligible for deletion. If set to -2, the value of log.retention.ms is used. The effective value should always be less than or equal to log.retention.ms value.
     */
    logLocalRetentionMs?: number;
    /**
     * This configuration controls whether down-conversion of message formats is enabled to satisfy consume requests. .
     */
    logMessageDownconversionEnable?: boolean;
    /**
     * The maximum difference allowed between the timestamp when a broker receives a message and the timestamp specified in the message.
     */
    logMessageTimestampDifferenceMaxMs?: number;
    /**
     * Define whether the timestamp in the message is message create time or log append time.
     */
    logMessageTimestampType?: string;
    /**
     * Should pre allocate file when create new segment?.
     */
    logPreallocate?: boolean;
    /**
     * The maximum size of the log before deleting messages.
     */
    logRetentionBytes?: number;
    /**
     * The number of hours to keep a log file before deleting it.
     */
    logRetentionHours?: number;
    /**
     * The number of milliseconds to keep a log file before deleting it (in milliseconds), If not set, the value in log.retention.minutes is used. If set to -1, no time limit is applied.
     */
    logRetentionMs?: number;
    /**
     * The maximum jitter to subtract from logRollTimeMillis (in milliseconds). If not set, the value in log.roll.jitter.hours is used.
     */
    logRollJitterMs?: number;
    /**
     * The maximum time before a new log segment is rolled out (in milliseconds).
     */
    logRollMs?: number;
    /**
     * The maximum size of a single log file.
     */
    logSegmentBytes?: number;
    /**
     * The amount of time to wait before deleting a file from the filesystem.
     */
    logSegmentDeleteDelayMs?: number;
    /**
     * The maximum number of connections allowed from each ip address (defaults to 2147483647).
     */
    maxConnectionsPerIp?: number;
    /**
     * The maximum number of incremental fetch sessions that the broker will maintain.
     */
    maxIncrementalFetchSessionCacheSlots?: number;
    /**
     * The maximum size of message that the server can receive.
     */
    messageMaxBytes?: number;
    /**
     * When a producer sets acks to 'all' (or '-1'), min.insync.replicas specifies the minimum number of replicas that must acknowledge a write for the write to be considered successful.
     */
    minInsyncReplicas?: number;
    /**
     * Number of partitions for autocreated topics.
     */
    numPartitions?: number;
    /**
     * Log retention window in minutes for offsets topic.
     */
    offsetsRetentionMinutes?: number;
    /**
     * The purge interval (in number of requests) of the producer request purgatory(defaults to 1000).
     */
    producerPurgatoryPurgeIntervalRequests?: number;
    /**
     * The number of bytes of messages to attempt to fetch for each partition (defaults to 1048576). This is not an absolute maximum, if the first record batch in the first non-empty partition of the fetch is larger than this value, the record batch will still be returned to ensure that progress can be made.
     */
    replicaFetchMaxBytes?: number;
    /**
     * Maximum bytes expected for the entire fetch response (defaults to 10485760). Records are fetched in batches, and if the first record batch in the first non-empty partition of the fetch is larger than this value, the record batch will still be returned to ensure that progress can be made. As such, this is not an absolute maximum.
     */
    replicaFetchResponseMaxBytes?: number;
    /**
     * The (optional) comma-delimited setting for the broker to use to verify that the JWT was issued for one of the expected audiences.
     */
    saslOauthbearerExpectedAudience?: string;
    /**
     * Optional setting for the broker to use to verify that the JWT was created by the expected issuer.
     */
    saslOauthbearerExpectedIssuer?: string;
    /**
     * OIDC JWKS endpoint URL. By setting this the SASL SSL OAuth2/OIDC authentication is enabled. See also other options for SASL OAuth2/OIDC. .
     */
    saslOauthbearerJwksEndpointUrl?: string;
    /**
     * Name of the scope from which to extract the subject claim from the JWT. Defaults to sub.
     */
    saslOauthbearerSubClaimName?: string;
    /**
     * The maximum number of bytes in a socket request (defaults to 104857600).
     */
    socketRequestMaxBytes?: number;
    /**
     * Enable verification that checks that the partition has been added to the transaction before writing transactional records to the partition.
     */
    transactionPartitionVerificationEnable?: boolean;
    /**
     * The interval at which to remove transactions that have expired due to transactional.id.expiration.ms passing (defaults to 3600000 (1 hour)).
     */
    transactionRemoveExpiredTransactionCleanupIntervalMs?: number;
    /**
     * The transaction topic segment bytes should be kept relatively small in order to facilitate faster log compaction and cache loads (defaults to 104857600 (100 mebibytes)).
     */
    transactionStateLogSegmentBytes?: number;
}

export interface KafkaKafkaUserConfigKafkaAuthenticationMethods {
    /**
     * Enable certificate/SSL authentication. The default value is `true`.
     */
    certificate?: boolean;
    /**
     * Enable SASL authentication. The default value is `false`.
     */
    sasl?: boolean;
}

export interface KafkaKafkaUserConfigKafkaConnectConfig {
    /**
     * Defines what client configurations can be overridden by the connector. Default is None.
     */
    connectorClientConfigOverridePolicy?: string;
    /**
     * What to do when there is no initial offset in Kafka or if the current offset does not exist any more on the server. Default is earliest.
     */
    consumerAutoOffsetReset?: string;
    /**
     * Records are fetched in batches by the consumer, and if the first record batch in the first non-empty partition of the fetch is larger than this value, the record batch will still be returned to ensure that the consumer can make progress. As such, this is not a absolute maximum.
     */
    consumerFetchMaxBytes?: number;
    /**
     * Transaction read isolation level. read*uncommitted is the default, but read*committed can be used if consume-exactly-once behavior is desired.
     */
    consumerIsolationLevel?: string;
    /**
     * Records are fetched in batches by the consumer.If the first record batch in the first non-empty partition of the fetch is larger than this limit, the batch will still be returned to ensure that the consumer can make progress. .
     */
    consumerMaxPartitionFetchBytes?: number;
    /**
     * The maximum delay in milliseconds between invocations of poll() when using consumer group management (defaults to 300000).
     */
    consumerMaxPollIntervalMs?: number;
    /**
     * The maximum number of records returned in a single call to poll() (defaults to 500).
     */
    consumerMaxPollRecords?: number;
    /**
     * The interval at which to try committing offsets for tasks (defaults to 60000).
     */
    offsetFlushIntervalMs?: number;
    /**
     * Maximum number of milliseconds to wait for records to flush and partition offset data to be committed to offset storage before cancelling the process and restoring the offset data to be committed in a future attempt (defaults to 5000).
     */
    offsetFlushTimeoutMs?: number;
    /**
     * This setting gives the upper bound of the batch size to be sent. If there are fewer than this many bytes accumulated for this partition, the producer will 'linger' for the linger.ms time waiting for more records to show up. A batch size of zero will disable batching entirely (defaults to 16384).
     */
    producerBatchSize?: number;
    /**
     * The total bytes of memory the producer can use to buffer records waiting to be sent to the broker (defaults to 33554432).
     */
    producerBufferMemory?: number;
    /**
     * Specify the default compression type for producers. This configuration accepts the standard compression codecs ('gzip', 'snappy', 'lz4', 'zstd'). It additionally accepts 'none' which is the default and equivalent to no compression.
     */
    producerCompressionType?: string;
    /**
     * This setting gives the upper bound on the delay for batching: once there is batch.size worth of records for a partition it will be sent immediately regardless of this setting, however if there are fewer than this many bytes accumulated for this partition the producer will 'linger' for the specified time waiting for more records to show up. Defaults to 0.
     */
    producerLingerMs?: number;
    /**
     * This setting will limit the number of record batches the producer will send in a single request to avoid sending huge requests.
     */
    producerMaxRequestSize?: number;
    /**
     * The maximum delay that is scheduled in order to wait for the return of one or more departed workers before rebalancing and reassigning their connectors and tasks to the group. During this period the connectors and tasks of the departed workers remain unassigned.  Defaults to 5 minutes.
     */
    scheduledRebalanceMaxDelayMs?: number;
    /**
     * The timeout in milliseconds used to detect failures when using Kafka’s group management facilities (defaults to 10000).
     */
    sessionTimeoutMs?: number;
}

export interface KafkaKafkaUserConfigKafkaRestConfig {
    /**
     * If true the consumer's offset will be periodically committed to Kafka in the background. The default value is `true`.
     */
    consumerEnableAutoCommit?: boolean;
    /**
     * Maximum number of bytes in unencoded message keys and values by a single request. The default value is `67108864`.
     */
    consumerRequestMaxBytes?: number;
    /**
     * The maximum total time to wait for messages for a request if the maximum number of messages has not yet been reached. The default value is `1000`.
     */
    consumerRequestTimeoutMs?: number;
    /**
     * If true, validate that given schema is registered under expected subject name by the used name strategy when producing messages. The default value is `true`.
     */
    nameStrategyValidation?: boolean;
    /**
     * The number of acknowledgments the producer requires the leader to have received before considering a request complete. If set to 'all' or '-1', the leader will wait for the full set of in-sync replicas to acknowledge the record. The default value is `1`.
     */
    producerAcks?: string;
    /**
     * Specify the default compression type for producers. This configuration accepts the standard compression codecs ('gzip', 'snappy', 'lz4', 'zstd'). It additionally accepts 'none' which is the default and equivalent to no compression.
     */
    producerCompressionType?: string;
    /**
     * This setting gives the upper bound on the delay for batching: once there is batch.size worth of records for a partition it will be sent immediately regardless of this setting, however if there are fewer than this many bytes accumulated for this partition the producer will 'linger' for the specified time waiting for more records to show up. Defaults to 0.
     */
    producerLingerMs?: number;
    /**
     * This setting will limit the number of record batches the producer will send in a single request to avoid sending huge requests.
     */
    producerMaxRequestSize?: number;
    /**
     * Maximum number of SimpleConsumers that can be instantiated per broker. The default value is `25`.
     */
    simpleconsumerPoolSizeMax?: number;
}

export interface KafkaKafkaUserConfigPrivateAccess {
    /**
     * Kafka broker configuration values.
     */
    kafka?: boolean;
    /**
     * Enable Kafka Connect service. The default value is `false`.
     */
    kafkaConnect?: boolean;
    /**
     * Enable Kafka-REST service. The default value is `false`.
     */
    kafkaRest?: boolean;
    /**
     * Allow clients to connect to prometheus with a DNS name that always resolves to the service's private IP addresses. Only available in certain network locations.
     */
    prometheus?: boolean;
    /**
     * Allow clients to connect to schemaRegistry with a DNS name that always resolves to the service's private IP addresses. Only available in certain network locations.
     */
    schemaRegistry?: boolean;
}

export interface KafkaKafkaUserConfigPrivatelinkAccess {
    /**
     * Enable jolokia.
     */
    jolokia?: boolean;
    /**
     * Kafka broker configuration values.
     */
    kafka?: boolean;
    /**
     * Enable Kafka Connect service. The default value is `false`.
     */
    kafkaConnect?: boolean;
    /**
     * Enable Kafka-REST service. The default value is `false`.
     */
    kafkaRest?: boolean;
    /**
     * Allow clients to connect to prometheus with a DNS name that always resolves to the service's private IP addresses. Only available in certain network locations.
     */
    prometheus?: boolean;
    /**
     * Allow clients to connect to schemaRegistry with a DNS name that always resolves to the service's private IP addresses. Only available in certain network locations.
     */
    schemaRegistry?: boolean;
}

export interface KafkaKafkaUserConfigPublicAccess {
    /**
     * Kafka broker configuration values.
     */
    kafka?: boolean;
    /**
     * Enable Kafka Connect service. The default value is `false`.
     */
    kafkaConnect?: boolean;
    /**
     * Enable Kafka-REST service. The default value is `false`.
     */
    kafkaRest?: boolean;
    /**
     * Allow clients to connect to prometheus with a DNS name that always resolves to the service's private IP addresses. Only available in certain network locations.
     */
    prometheus?: boolean;
    /**
     * Allow clients to connect to schemaRegistry with a DNS name that always resolves to the service's private IP addresses. Only available in certain network locations.
     */
    schemaRegistry?: boolean;
}

export interface KafkaKafkaUserConfigSchemaRegistryConfig {
    /**
     * If true, Karapace / Schema Registry on the service nodes can participate in leader election. It might be needed to disable this when the schemas topic is replicated to a secondary cluster and Karapace / Schema Registry there must not participate in leader election. Defaults to `true`.
     */
    leaderEligibility?: boolean;
    /**
     * The durable single partition topic that acts as the durable log for the data. This topic must be compacted to avoid losing data due to retention policy. Please note that changing this configuration in an existing Schema Registry / Karapace setup leads to previous schemas being inaccessible, data encoded with them potentially unreadable and schema ID sequence put out of order. It's only possible to do the switch while Schema Registry / Karapace is disabled. Defaults to `_schemas`.
     */
    topicName?: string;
}

export interface KafkaKafkaUserConfigTieredStorage {
    /**
     * Whether to enable the tiered storage functionality.
     */
    enabled?: boolean;
    /**
     * Local cache configuration.
     */
    localCache?: outputs.KafkaKafkaUserConfigTieredStorageLocalCache;
}

export interface KafkaKafkaUserConfigTieredStorageLocalCache {
    /**
     * Local cache size in bytes.
     */
    size?: number;
}

export interface KafkaMirrorMakerComponent {
    component: string;
    connectionUri: string;
    host: string;
    kafkaAuthenticationMethod: string;
    port: number;
    route: string;
    ssl: boolean;
    usage: string;
}

export interface KafkaMirrorMakerKafkaMirrormaker {
}

export interface KafkaMirrorMakerKafkaMirrormakerUserConfig {
    additionalBackupRegions?: string;
    ipFilterObjects?: outputs.KafkaMirrorMakerKafkaMirrormakerUserConfigIpFilterObject[];
    ipFilterStrings?: string[];
    /**
     * @deprecated This will be removed in v5.0.0 and replaced with ip_filter_string instead.
     */
    ipFilters?: string[];
    kafkaMirrormaker?: outputs.KafkaMirrorMakerKafkaMirrormakerUserConfigKafkaMirrormaker;
    serviceLog?: boolean;
    staticIps?: boolean;
}

export interface KafkaMirrorMakerKafkaMirrormakerUserConfigIpFilterObject {
    description?: string;
    network: string;
}

export interface KafkaMirrorMakerKafkaMirrormakerUserConfigKafkaMirrormaker {
    emitCheckpointsEnabled?: boolean;
    emitCheckpointsIntervalSeconds?: number;
    refreshGroupsEnabled?: boolean;
    refreshGroupsIntervalSeconds?: number;
    refreshTopicsEnabled?: boolean;
    refreshTopicsIntervalSeconds?: number;
    syncGroupOffsetsEnabled?: boolean;
    syncGroupOffsetsIntervalSeconds?: number;
    syncTopicConfigsEnabled?: boolean;
    tasksMaxPerCpu?: number;
}

export interface KafkaMirrorMakerServiceIntegration {
    integrationType: string;
    sourceServiceName: string;
}

export interface KafkaMirrorMakerTag {
    key: string;
    value: string;
}

export interface KafkaMirrorMakerTechEmail {
    email: string;
}

export interface KafkaServiceIntegration {
    /**
     * Type of the service integration. The only supported value at the moment is `readReplica`
     */
    integrationType: string;
    /**
     * Name of the source service
     */
    sourceServiceName: string;
}

export interface KafkaTag {
    /**
     * Service tag key
     */
    key: string;
    /**
     * Service tag value
     */
    value: string;
}

export interface KafkaTechEmail {
    /**
     * An email address to contact for technical issues
     */
    email: string;
}

export interface KafkaTopicConfig {
    /**
     * cleanup.policy value
     */
    cleanupPolicy?: string;
    /**
     * compression.type value
     */
    compressionType?: string;
    /**
     * delete.retention.ms value
     */
    deleteRetentionMs?: string;
    /**
     * file.delete.delay.ms value
     */
    fileDeleteDelayMs?: string;
    /**
     * flush.messages value
     */
    flushMessages?: string;
    /**
     * flush.ms value
     */
    flushMs?: string;
    /**
     * index.interval.bytes value
     */
    indexIntervalBytes?: string;
    /**
     * local.retention.bytes value. This field is temporarily disabled.
     */
    localRetentionBytes?: string;
    /**
     * local.retention.ms value. This field is temporarily disabled.
     */
    localRetentionMs?: string;
    /**
     * max.compaction.lag.ms value
     */
    maxCompactionLagMs?: string;
    /**
     * max.message.bytes value
     */
    maxMessageBytes?: string;
    /**
     * message.downconversion.enable value
     */
    messageDownconversionEnable?: boolean;
    /**
     * message.format.version value
     */
    messageFormatVersion?: string;
    /**
     * message.timestamp.difference.max.ms value
     */
    messageTimestampDifferenceMaxMs?: string;
    /**
     * message.timestamp.type value
     */
    messageTimestampType?: string;
    /**
     * min.cleanable.dirty.ratio value
     */
    minCleanableDirtyRatio?: number;
    /**
     * min.compaction.lag.ms value
     */
    minCompactionLagMs?: string;
    /**
     * min.insync.replicas value
     */
    minInsyncReplicas?: string;
    /**
     * preallocate value
     */
    preallocate?: boolean;
    /**
     * remote.storage.enable value
     */
    remoteStorageEnable?: boolean;
    /**
     * retention.bytes value
     */
    retentionBytes?: string;
    /**
     * retention.ms value
     */
    retentionMs?: string;
    /**
     * segment.bytes value
     */
    segmentBytes?: string;
    /**
     * segment.index.bytes value
     */
    segmentIndexBytes?: string;
    /**
     * segment.jitter.ms value
     */
    segmentJitterMs?: string;
    /**
     * segment.ms value
     */
    segmentMs?: string;
    /**
     * unclean.leader.election.enable value; This field is deprecated and no longer functional.
     *
     * @deprecated This field is deprecated and no longer functional.
     */
    uncleanLeaderElectionEnable?: boolean;
}

export interface KafkaTopicTag {
    /**
     * Topic tag key. Maximum length: `64`.
     */
    key: string;
    /**
     * Topic tag value. Maximum length: `256`.
     */
    value?: string;
}

export interface M3AggregatorComponent {
    component: string;
    connectionUri: string;
    host: string;
    kafkaAuthenticationMethod: string;
    port: number;
    route: string;
    ssl: boolean;
    usage: string;
}

export interface M3AggregatorM3aggregator {
}

export interface M3AggregatorM3aggregatorUserConfig {
    customDomain?: string;
    ipFilterObjects?: outputs.M3AggregatorM3aggregatorUserConfigIpFilterObject[];
    ipFilterStrings?: string[];
    /**
     * @deprecated This will be removed in v5.0.0 and replaced with ip_filter_string instead.
     */
    ipFilters?: string[];
    /**
     * @deprecated Usage of this field is discouraged.
     */
    m3Version?: string;
    m3aggregatorVersion?: string;
    serviceLog?: boolean;
    staticIps?: boolean;
}

export interface M3AggregatorM3aggregatorUserConfigIpFilterObject {
    description?: string;
    network: string;
}

export interface M3AggregatorServiceIntegration {
    integrationType: string;
    sourceServiceName: string;
}

export interface M3AggregatorTag {
    key: string;
    value: string;
}

export interface M3AggregatorTechEmail {
    email: string;
}

export interface M3DbComponent {
    component: string;
    connectionUri: string;
    host: string;
    kafkaAuthenticationMethod: string;
    port: number;
    route: string;
    ssl: boolean;
    usage: string;
}

export interface M3DbM3db {
}

export interface M3DbM3dbUserConfig {
    additionalBackupRegions?: string;
    customDomain?: string;
    ipFilterObjects?: outputs.M3DbM3dbUserConfigIpFilterObject[];
    ipFilterStrings?: string[];
    /**
     * @deprecated This will be removed in v5.0.0 and replaced with ip_filter_string instead.
     */
    ipFilters?: string[];
    limits?: outputs.M3DbM3dbUserConfigLimits;
    m3?: outputs.M3DbM3dbUserConfigM3;
    /**
     * @deprecated Usage of this field is discouraged.
     */
    m3Version?: string;
    m3coordinatorEnableGraphiteCarbonIngest?: boolean;
    m3dbVersion?: string;
    namespaces?: outputs.M3DbM3dbUserConfigNamespace[];
    privateAccess?: outputs.M3DbM3dbUserConfigPrivateAccess;
    projectToForkFrom?: string;
    publicAccess?: outputs.M3DbM3dbUserConfigPublicAccess;
    rules?: outputs.M3DbM3dbUserConfigRules;
    serviceLog?: boolean;
    serviceToForkFrom?: string;
    staticIps?: boolean;
}

export interface M3DbM3dbUserConfigIpFilterObject {
    description?: string;
    network: string;
}

export interface M3DbM3dbUserConfigLimits {
    maxRecentlyQueriedSeriesBlocks?: number;
    maxRecentlyQueriedSeriesDiskBytesRead?: number;
    maxRecentlyQueriedSeriesLookback?: string;
    queryDocs?: number;
    queryRequireExhaustive?: boolean;
    querySeries?: number;
}

export interface M3DbM3dbUserConfigM3 {
    tagOptions?: outputs.M3DbM3dbUserConfigM3TagOptions;
}

export interface M3DbM3dbUserConfigM3TagOptions {
    allowTagNameDuplicates?: boolean;
    allowTagValueEmpty?: boolean;
}

export interface M3DbM3dbUserConfigNamespace {
    name: string;
    options?: outputs.M3DbM3dbUserConfigNamespaceOptions;
    resolution?: string;
    type: string;
}

export interface M3DbM3dbUserConfigNamespaceOptions {
    retentionOptions?: outputs.M3DbM3dbUserConfigNamespaceOptionsRetentionOptions;
    snapshotEnabled?: boolean;
    writesToCommitlog?: boolean;
}

export interface M3DbM3dbUserConfigNamespaceOptionsRetentionOptions {
    blockDataExpiryDuration?: string;
    blocksizeDuration?: string;
    bufferFutureDuration?: string;
    bufferPastDuration?: string;
    retentionPeriodDuration?: string;
}

export interface M3DbM3dbUserConfigPrivateAccess {
    m3coordinator?: boolean;
}

export interface M3DbM3dbUserConfigPublicAccess {
    m3coordinator?: boolean;
}

export interface M3DbM3dbUserConfigRules {
    mappings?: outputs.M3DbM3dbUserConfigRulesMapping[];
}

export interface M3DbM3dbUserConfigRulesMapping {
    aggregations?: string[];
    drop?: boolean;
    filter: string;
    name?: string;
    /**
     * @deprecated This will be removed in v5.0.0 and replaced with namespaces_string instead.
     */
    namespaces?: string[];
    namespacesObjects?: outputs.M3DbM3dbUserConfigRulesMappingNamespacesObject[];
    namespacesStrings?: string[];
    tags?: outputs.M3DbM3dbUserConfigRulesMappingTag[];
}

export interface M3DbM3dbUserConfigRulesMappingNamespacesObject {
    resolution?: string;
    retention?: string;
}

export interface M3DbM3dbUserConfigRulesMappingTag {
    name: string;
    value: string;
}

export interface M3DbServiceIntegration {
    integrationType: string;
    sourceServiceName: string;
}

export interface M3DbTag {
    key: string;
    value: string;
}

export interface M3DbTechEmail {
    email: string;
}

export interface MySqlComponent {
    component: string;
    connectionUri: string;
    host: string;
    kafkaAuthenticationMethod: string;
    port: number;
    route: string;
    ssl: boolean;
    usage: string;
}

export interface MySqlMysql {
}

export interface MySqlMysqlUserConfig {
    additionalBackupRegions?: string;
    adminPassword?: string;
    adminUsername?: string;
    backupHour?: number;
    backupMinute?: number;
    binlogRetentionPeriod?: number;
    ipFilterObjects?: outputs.MySqlMysqlUserConfigIpFilterObject[];
    ipFilterStrings?: string[];
    /**
     * @deprecated This will be removed in v5.0.0 and replaced with ip_filter_string instead.
     */
    ipFilters?: string[];
    migration?: outputs.MySqlMysqlUserConfigMigration;
    mysql?: outputs.MySqlMysqlUserConfigMysql;
    mysqlVersion?: string;
    privateAccess?: outputs.MySqlMysqlUserConfigPrivateAccess;
    privatelinkAccess?: outputs.MySqlMysqlUserConfigPrivatelinkAccess;
    projectToForkFrom?: string;
    publicAccess?: outputs.MySqlMysqlUserConfigPublicAccess;
    recoveryTargetTime?: string;
    serviceLog?: boolean;
    serviceToForkFrom?: string;
    staticIps?: boolean;
}

export interface MySqlMysqlUserConfigIpFilterObject {
    description?: string;
    network: string;
}

export interface MySqlMysqlUserConfigMigration {
    dbname?: string;
    host: string;
    ignoreDbs?: string;
    method?: string;
    password?: string;
    port: number;
    ssl?: boolean;
    username?: string;
}

export interface MySqlMysqlUserConfigMysql {
    connectTimeout?: number;
    defaultTimeZone?: string;
    groupConcatMaxLen?: number;
    informationSchemaStatsExpiry?: number;
    innodbChangeBufferMaxSize?: number;
    innodbFlushNeighbors?: number;
    innodbFtMinTokenSize?: number;
    innodbFtServerStopwordTable?: string;
    innodbLockWaitTimeout?: number;
    innodbLogBufferSize?: number;
    innodbOnlineAlterLogMaxSize?: number;
    innodbPrintAllDeadlocks?: boolean;
    innodbReadIoThreads?: number;
    innodbRollbackOnTimeout?: boolean;
    innodbThreadConcurrency?: number;
    innodbWriteIoThreads?: number;
    interactiveTimeout?: number;
    internalTmpMemStorageEngine?: string;
    longQueryTime?: number;
    maxAllowedPacket?: number;
    maxHeapTableSize?: number;
    netBufferLength?: number;
    netReadTimeout?: number;
    netWriteTimeout?: number;
    slowQueryLog?: boolean;
    sortBufferSize?: number;
    sqlMode?: string;
    sqlRequirePrimaryKey?: boolean;
    tmpTableSize?: number;
    waitTimeout?: number;
}

export interface MySqlMysqlUserConfigPrivateAccess {
    mysql?: boolean;
    mysqlx?: boolean;
    prometheus?: boolean;
}

export interface MySqlMysqlUserConfigPrivatelinkAccess {
    mysql?: boolean;
    mysqlx?: boolean;
    prometheus?: boolean;
}

export interface MySqlMysqlUserConfigPublicAccess {
    mysql?: boolean;
    mysqlx?: boolean;
    prometheus?: boolean;
}

export interface MySqlServiceIntegration {
    integrationType: string;
    sourceServiceName: string;
}

export interface MySqlTag {
    key: string;
    value: string;
}

export interface MySqlTechEmail {
    email: string;
}

export interface OpenSearchComponent {
    component: string;
    connectionUri: string;
    host: string;
    kafkaAuthenticationMethod: string;
    port: number;
    route: string;
    ssl: boolean;
    usage: string;
}

export interface OpenSearchOpensearch {
    opensearchDashboardsUri: string;
}

export interface OpenSearchOpensearchUserConfig {
    /**
     * Additional Cloud Regions for Backup Replication.
     */
    additionalBackupRegions?: string;
    /**
     * Serve the web frontend using a custom CNAME pointing to the Aiven DNS name.
     */
    customDomain?: string;
    /**
     * Disable automatic replication factor adjustment for multi-node services. By default, Aiven ensures all indexes are replicated at least to two nodes. Note: Due to potential data loss in case of losing a service node, this setting can no longer be activated.
     *
     * @deprecated Usage of this field is discouraged.
     */
    disableReplicationFactorAdjustment?: boolean;
    /**
     * Index patterns.
     */
    indexPatterns?: outputs.OpenSearchOpensearchUserConfigIndexPattern[];
    /**
     * Template settings for all new indexes.
     */
    indexTemplate?: outputs.OpenSearchOpensearchUserConfigIndexTemplate;
    /**
     * Allow incoming connections from CIDR address block, e.g. '10.20.0.0/16'.
     */
    ipFilterObjects?: outputs.OpenSearchOpensearchUserConfigIpFilterObject[];
    /**
     * Allow incoming connections from CIDR address block, e.g. '10.20.0.0/16'.
     */
    ipFilterStrings?: string[];
    /**
     * Allow incoming connections from CIDR address block, e.g. '10.20.0.0/16'.
     *
     * @deprecated This will be removed in v5.0.0 and replaced with ip_filter_string instead.
     */
    ipFilters?: string[];
    /**
     * Aiven automation resets index.refresh_interval to default value for every index to be sure that indices are always visible to search. If it doesn't fit your case, you can disable this by setting up this flag to true.
     */
    keepIndexRefreshInterval?: boolean;
    /**
     * Use indexPatterns instead. The default value is `0`.
     *
     * @deprecated Usage of this field is discouraged.
     */
    maxIndexCount?: number;
    /**
     * OpenSearch OpenID Connect Configuration.
     */
    openid?: outputs.OpenSearchOpensearchUserConfigOpenid;
    /**
     * OpenSearch settings.
     */
    opensearch?: outputs.OpenSearchOpensearchUserConfigOpensearch;
    /**
     * OpenSearch Dashboards settings.
     */
    opensearchDashboards?: outputs.OpenSearchOpensearchUserConfigOpensearchDashboards;
    /**
     * OpenSearch major version.
     */
    opensearchVersion?: string;
    /**
     * Allow access to selected service ports from private networks.
     */
    privateAccess?: outputs.OpenSearchOpensearchUserConfigPrivateAccess;
    /**
     * Allow access to selected service components through Privatelink.
     */
    privatelinkAccess?: outputs.OpenSearchOpensearchUserConfigPrivatelinkAccess;
    /**
     * Name of another project to fork a service from. This has effect only when a new service is being created.
     */
    projectToForkFrom?: string;
    /**
     * Allow access to selected service ports from the public Internet.
     */
    publicAccess?: outputs.OpenSearchOpensearchUserConfigPublicAccess;
    /**
     * Name of the basebackup to restore in forked service.
     */
    recoveryBasebackupName?: string;
    /**
     * OpenSearch SAML configuration.
     */
    saml?: outputs.OpenSearchOpensearchUserConfigSaml;
    /**
     * Store logs for the service so that they are available in the HTTP API and console.
     */
    serviceLog?: boolean;
    /**
     * Name of another service to fork from. This has effect only when a new service is being created.
     */
    serviceToForkFrom?: string;
    /**
     * Use static public IP addresses.
     */
    staticIps?: boolean;
}

export interface OpenSearchOpensearchUserConfigIndexPattern {
    /**
     * Maximum number of indexes to keep.
     */
    maxIndexCount: number;
    /**
     * fnmatch pattern.
     */
    pattern: string;
    /**
     * Deletion sorting algorithm. The default value is `creationDate`.
     */
    sortingAlgorithm?: string;
}

export interface OpenSearchOpensearchUserConfigIndexTemplate {
    /**
     * The maximum number of nested JSON objects that a single document can contain across all nested types. This limit helps to prevent out of memory errors when a document contains too many nested objects. Default is 10000.
     */
    mappingNestedObjectsLimit?: number;
    /**
     * The number of replicas each primary shard has.
     */
    numberOfReplicas?: number;
    /**
     * The number of primary shards that an index should have.
     */
    numberOfShards?: number;
}

export interface OpenSearchOpensearchUserConfigIpFilterObject {
    /**
     * Description for IP filter list entry.
     */
    description?: string;
    /**
     * CIDR address block.
     */
    network: string;
}

export interface OpenSearchOpensearchUserConfigOpenid {
    /**
     * The ID of the OpenID Connect client configured in your IdP. Required.
     */
    clientId: string;
    /**
     * The client secret of the OpenID Connect client configured in your IdP. Required.
     */
    clientSecret: string;
    /**
     * The URL of your IdP where the Security plugin can find the OpenID Connect metadata/configuration settings.
     */
    connectUrl: string;
    /**
     * Enables or disables OpenID Connect authentication for OpenSearch. When enabled, users can authenticate using OpenID Connect with an Identity Provider. The default value is `true`.
     */
    enabled: boolean;
    /**
     * HTTP header name of the JWT token. Optional. Default is Authorization. The default value is `Authorization`.
     */
    header?: string;
    /**
     * The HTTP header that stores the token. Typically the Authorization header with the Bearer schema: Authorization: Bearer \n\n. Optional. Default is Authorization.
     */
    jwtHeader?: string;
    /**
     * If the token is not transmitted in the HTTP header, but as an URL parameter, define the name of the parameter here. Optional.
     */
    jwtUrlParameter?: string;
    /**
     * The maximum number of unknown key IDs in the time frame. Default is 10. Optional. The default value is `10`.
     */
    refreshRateLimitCount?: number;
    /**
     * The time frame to use when checking the maximum number of unknown key IDs, in milliseconds. Optional.Default is 10000 (10 seconds). The default value is `10000`.
     */
    refreshRateLimitTimeWindowMs?: number;
    /**
     * The key in the JSON payload that stores the user’s roles. The value of this key must be a comma-separated list of roles. Required only if you want to use roles in the JWT.
     */
    rolesKey?: string;
    /**
     * The scope of the identity token issued by the IdP. Optional. Default is openid profile email address phone.
     */
    scope?: string;
    /**
     * The key in the JSON payload that stores the user’s name. If not defined, the subject registered claim is used. Most IdP providers use the preferredUsername claim. Optional.
     */
    subjectKey?: string;
}

export interface OpenSearchOpensearchUserConfigOpensearch {
    /**
     * Explicitly allow or block automatic creation of indices. Defaults to true.
     */
    actionAutoCreateIndexEnabled?: boolean;
    /**
     * Require explicit index names when deleting.
     */
    actionDestructiveRequiresName?: boolean;
    /**
     * Opensearch Security Plugin Settings.
     */
    authFailureListeners?: outputs.OpenSearchOpensearchUserConfigOpensearchAuthFailureListeners;
    /**
     * Controls the number of shards allowed in the cluster per data node.
     */
    clusterMaxShardsPerNode?: number;
    /**
     * How many concurrent incoming/outgoing shard recoveries (normally replicas) are allowed to happen on a node. Defaults to 2.
     */
    clusterRoutingAllocationNodeConcurrentRecoveries?: number;
    /**
     * This should be identical to the Sender name defined in Opensearch dashboards.
     */
    emailSenderName?: string;
    /**
     * Sender password for Opensearch alerts to authenticate with SMTP server.
     */
    emailSenderPassword?: string;
    /**
     * Sender username for Opensearch alerts.
     */
    emailSenderUsername?: string;
    /**
     * Enable/Disable security audit. The default value is `false`.
     */
    enableSecurityAudit?: boolean;
    /**
     * Maximum content length for HTTP requests to the OpenSearch HTTP API, in bytes.
     */
    httpMaxContentLength?: number;
    /**
     * The max size of allowed headers, in bytes.
     */
    httpMaxHeaderSize?: number;
    /**
     * The max length of an HTTP URL, in bytes.
     */
    httpMaxInitialLineLength?: number;
    /**
     * Relative amount. Maximum amount of heap memory used for field data cache. This is an expert setting; decreasing the value too much will increase overhead of loading field data; too much memory used for field data cache will decrease amount of heap available for other operations.
     */
    indicesFielddataCacheSize?: number;
    /**
     * Percentage value. Default is 10%. Total amount of heap used for indexing buffer, before writing segments to disk. This is an expert setting. Too low value will slow down indexing; too high value will increase indexing performance but causes performance issues for query performance.
     */
    indicesMemoryIndexBufferSize?: number;
    /**
     * Absolute value. Default is unbound. Doesn't work without indices.memory.index*buffer*size. Maximum amount of heap used for query cache, an absolute indices.memory.index*buffer*size maximum hard limit.
     */
    indicesMemoryMaxIndexBufferSize?: number;
    /**
     * Absolute value. Default is 48mb. Doesn't work without indices.memory.index*buffer*size. Minimum amount of heap used for query cache, an absolute indices.memory.index*buffer*size minimal hard limit.
     */
    indicesMemoryMinIndexBufferSize?: number;
    /**
     * Percentage value. Default is 10%. Maximum amount of heap used for query cache. This is an expert setting. Too low value will decrease query performance and increase performance for other operations; too high value will cause issues with other OpenSearch functionality.
     */
    indicesQueriesCacheSize?: number;
    /**
     * Maximum number of clauses Lucene BooleanQuery can have. The default value (1024) is relatively high, and increasing it may cause performance issues. Investigate other approaches first before increasing this value.
     */
    indicesQueryBoolMaxClauseCount?: number;
    /**
     * Limits total inbound and outbound recovery traffic for each node. Applies to both peer recoveries as well as snapshot recoveries (i.e., restores from a snapshot). Defaults to 40mb.
     */
    indicesRecoveryMaxBytesPerSec?: number;
    /**
     * Number of file chunks sent in parallel for each recovery. Defaults to 2.
     */
    indicesRecoveryMaxConcurrentFileChunks?: number;
    /**
     * Specifies whether ISM is enabled or not. The default value is `true`.
     */
    ismEnabled?: boolean;
    /**
     * Specifies whether audit history is enabled or not. The logs from ISM are automatically indexed to a logs document. The default value is `true`.
     */
    ismHistoryEnabled?: boolean;
    /**
     * The maximum age before rolling over the audit history index in hours. The default value is `24`.
     */
    ismHistoryMaxAge?: number;
    /**
     * The maximum number of documents before rolling over the audit history index. The default value is `2500000`.
     */
    ismHistoryMaxDocs?: number;
    /**
     * The time between rollover checks for the audit history index in hours. The default value is `8`.
     */
    ismHistoryRolloverCheckPeriod?: number;
    /**
     * How long audit history indices are kept in days. The default value is `30`.
     */
    ismHistoryRolloverRetentionPeriod?: number;
    /**
     * Compatibility mode sets OpenSearch to report its version as 7.10 so clients continue to work. Default is false.
     */
    overrideMainResponseVersion?: boolean;
    /**
     * Whitelisted addresses for reindexing. Changing this value will cause all OpenSearch instances to restart.
     */
    reindexRemoteWhitelists?: string[];
    /**
     * Script compilation circuit breaker limits the number of inline script compilations within a period of time. Default is use-context.
     */
    scriptMaxCompilationsRate?: string;
    /**
     * Maximum number of aggregation buckets allowed in a single response. OpenSearch default value is used when this is not defined.
     */
    searchMaxBuckets?: number;
    /**
     * Size for the thread pool queue. See documentation for exact details.
     */
    threadPoolAnalyzeQueueSize?: number;
    /**
     * Size for the thread pool. See documentation for exact details. Do note this may have maximum value depending on CPU count - value is automatically lowered if set to higher than maximum value.
     */
    threadPoolAnalyzeSize?: number;
    /**
     * Size for the thread pool. See documentation for exact details. Do note this may have maximum value depending on CPU count - value is automatically lowered if set to higher than maximum value.
     */
    threadPoolForceMergeSize?: number;
    /**
     * Size for the thread pool queue. See documentation for exact details.
     */
    threadPoolGetQueueSize?: number;
    /**
     * Size for the thread pool. See documentation for exact details. Do note this may have maximum value depending on CPU count - value is automatically lowered if set to higher than maximum value.
     */
    threadPoolGetSize?: number;
    /**
     * Size for the thread pool queue. See documentation for exact details.
     */
    threadPoolSearchQueueSize?: number;
    /**
     * Size for the thread pool. See documentation for exact details. Do note this may have maximum value depending on CPU count - value is automatically lowered if set to higher than maximum value.
     */
    threadPoolSearchSize?: number;
    /**
     * Size for the thread pool queue. See documentation for exact details.
     */
    threadPoolSearchThrottledQueueSize?: number;
    /**
     * Size for the thread pool. See documentation for exact details. Do note this may have maximum value depending on CPU count - value is automatically lowered if set to higher than maximum value.
     */
    threadPoolSearchThrottledSize?: number;
    /**
     * Size for the thread pool queue. See documentation for exact details.
     */
    threadPoolWriteQueueSize?: number;
    /**
     * Size for the thread pool. See documentation for exact details. Do note this may have maximum value depending on CPU count - value is automatically lowered if set to higher than maximum value.
     */
    threadPoolWriteSize?: number;
}

export interface OpenSearchOpensearchUserConfigOpensearchAuthFailureListeners {
    /**
     * .
     */
    internalAuthenticationBackendLimiting?: outputs.OpenSearchOpensearchUserConfigOpensearchAuthFailureListenersInternalAuthenticationBackendLimiting;
    /**
     * IP address rate limiting settings.
     */
    ipRateLimiting?: outputs.OpenSearchOpensearchUserConfigOpensearchAuthFailureListenersIpRateLimiting;
}

export interface OpenSearchOpensearchUserConfigOpensearchAuthFailureListenersInternalAuthenticationBackendLimiting {
    /**
     * The number of login attempts allowed before login is blocked.
     */
    allowedTries?: number;
    /**
     * The internal backend. Enter `internal`.
     */
    authenticationBackend?: string;
    /**
     * The duration of time that login remains blocked after a failed login.
     */
    blockExpirySeconds?: number;
    /**
     * The maximum number of blocked IP addresses.
     */
    maxBlockedClients?: number;
    /**
     * The maximum number of tracked IP addresses that have failed login.
     */
    maxTrackedClients?: number;
    /**
     * The window of time in which the value for `allowedTries` is enforced.
     */
    timeWindowSeconds?: number;
    /**
     * The type of rate limiting.
     */
    type?: string;
}

export interface OpenSearchOpensearchUserConfigOpensearchAuthFailureListenersIpRateLimiting {
    /**
     * The number of login attempts allowed before login is blocked.
     */
    allowedTries?: number;
    /**
     * The duration of time that login remains blocked after a failed login.
     */
    blockExpirySeconds?: number;
    /**
     * The maximum number of blocked IP addresses.
     */
    maxBlockedClients?: number;
    /**
     * The maximum number of tracked IP addresses that have failed login.
     */
    maxTrackedClients?: number;
    /**
     * The window of time in which the value for `allowedTries` is enforced.
     */
    timeWindowSeconds?: number;
    /**
     * The type of rate limiting.
     */
    type?: string;
}

export interface OpenSearchOpensearchUserConfigOpensearchDashboards {
    /**
     * Enables or disables OpenID Connect authentication for OpenSearch. When enabled, users can authenticate using OpenID Connect with an Identity Provider. The default value is `true`.
     */
    enabled?: boolean;
    /**
     * Limits the maximum amount of memory (in MiB) the OpenSearch Dashboards process can use. This sets the max*old*space_size option of the nodejs running the OpenSearch Dashboards. Note: the memory reserved by OpenSearch Dashboards is not available for OpenSearch. The default value is `128`.
     */
    maxOldSpaceSize?: number;
    /**
     * Timeout in milliseconds for requests made by OpenSearch Dashboards towards OpenSearch. The default value is `30000`.
     */
    opensearchRequestTimeout?: number;
}

export interface OpenSearchOpensearchUserConfigPrivateAccess {
    /**
     * OpenSearch settings.
     */
    opensearch?: boolean;
    /**
     * OpenSearch Dashboards settings.
     */
    opensearchDashboards?: boolean;
    /**
     * Allow clients to connect to prometheus with a DNS name that always resolves to the service's private IP addresses. Only available in certain network locations.
     */
    prometheus?: boolean;
}

export interface OpenSearchOpensearchUserConfigPrivatelinkAccess {
    /**
     * OpenSearch settings.
     */
    opensearch?: boolean;
    /**
     * OpenSearch Dashboards settings.
     */
    opensearchDashboards?: boolean;
    /**
     * Allow clients to connect to prometheus with a DNS name that always resolves to the service's private IP addresses. Only available in certain network locations.
     */
    prometheus?: boolean;
}

export interface OpenSearchOpensearchUserConfigPublicAccess {
    /**
     * OpenSearch settings.
     */
    opensearch?: boolean;
    /**
     * OpenSearch Dashboards settings.
     */
    opensearchDashboards?: boolean;
    /**
     * Allow clients to connect to prometheus with a DNS name that always resolves to the service's private IP addresses. Only available in certain network locations.
     */
    prometheus?: boolean;
}

export interface OpenSearchOpensearchUserConfigSaml {
    /**
     * Enables or disables OpenID Connect authentication for OpenSearch. When enabled, users can authenticate using OpenID Connect with an Identity Provider. The default value is `true`.
     */
    enabled: boolean;
    /**
     * The unique identifier for the Identity Provider (IdP) entity that is used for SAML authentication. This value is typically provided by the IdP.
     */
    idpEntityId: string;
    /**
     * The URL of the SAML metadata for the Identity Provider (IdP). This is used to configure SAML-based authentication with the IdP.
     */
    idpMetadataUrl: string;
    /**
     * This parameter specifies the PEM-encoded root certificate authority (CA) content for the SAML identity provider (IdP) server verification. The root CA content is used to verify the SSL/TLS certificate presented by the server.
     */
    idpPemtrustedcasContent?: string;
    /**
     * The key in the JSON payload that stores the user’s roles. The value of this key must be a comma-separated list of roles. Required only if you want to use roles in the JWT.
     */
    rolesKey?: string;
    /**
     * The unique identifier for the Service Provider (SP) entity that is used for SAML authentication. This value is typically provided by the SP.
     */
    spEntityId: string;
    /**
     * The key in the JSON payload that stores the user’s name. If not defined, the subject registered claim is used. Most IdP providers use the preferredUsername claim. Optional.
     */
    subjectKey?: string;
}

export interface OpenSearchServiceIntegration {
    /**
     * Type of the service integration. The only supported value at the moment is `readReplica`
     */
    integrationType: string;
    /**
     * Name of the source service
     */
    sourceServiceName: string;
}

export interface OpenSearchTag {
    /**
     * Service tag key
     */
    key: string;
    /**
     * Service tag value
     */
    value: string;
}

export interface OpenSearchTechEmail {
    /**
     * An email address to contact for technical issues
     */
    email: string;
}

export interface OrganizationTimeouts {
    /**
     * A string that can be [parsed as a duration](https://pkg.go.dev/time#ParseDuration) consisting of numbers and unit suffixes, such as "30s" or "2h45m". Valid time units are "s" (seconds), "m" (minutes), "h" (hours).
     */
    create?: string;
    /**
     * A string that can be [parsed as a duration](https://pkg.go.dev/time#ParseDuration) consisting of numbers and unit suffixes, such as "30s" or "2h45m". Valid time units are "s" (seconds), "m" (minutes), "h" (hours). Setting a timeout for a Delete operation is only applicable if changes are saved into state before the destroy operation occurs.
     */
    delete?: string;
    /**
     * A string that can be [parsed as a duration](https://pkg.go.dev/time#ParseDuration) consisting of numbers and unit suffixes, such as "30s" or "2h45m". Valid time units are "s" (seconds), "m" (minutes), "h" (hours). Read operations occur during any refresh or planning operation when refresh is enabled.
     */
    read?: string;
    /**
     * A string that can be [parsed as a duration](https://pkg.go.dev/time#ParseDuration) consisting of numbers and unit suffixes, such as "30s" or "2h45m". Valid time units are "s" (seconds), "m" (minutes), "h" (hours).
     */
    update?: string;
}

export interface PgComponent {
    component: string;
    connectionUri: string;
    host: string;
    kafkaAuthenticationMethod: string;
    port: number;
    route: string;
    ssl: boolean;
    usage: string;
}

export interface PgPg {
    /**
     * Primary PostgreSQL database name
     */
    dbname: string;
    /**
     * PostgreSQL master node host IP or name
     */
    host: string;
    /**
     * Connection limit
     */
    maxConnections: number;
    /**
     * PostgreSQL admin user password
     */
    password: string;
    /**
     * PostgreSQL port
     */
    port: number;
    /**
     * PostgreSQL replica URI for services with a replica
     */
    replicaUri: string;
    /**
     * PostgreSQL sslmode setting (currently always "require")
     */
    sslmode: string;
    /**
     * PostgreSQL master connection URI
     */
    uri: string;
    /**
     * PostgreSQL admin user name
     */
    user: string;
}

export interface PgPgUserConfig {
    /**
     * Additional Cloud Regions for Backup Replication.
     */
    additionalBackupRegions?: string;
    /**
     * Custom password for admin user. Defaults to random string. This must be set only when a new service is being created.
     */
    adminPassword?: string;
    /**
     * Custom username for admin user. This must be set only when a new service is being created.
     */
    adminUsername?: string;
    /**
     * The hour of day (in UTC) when backup for the service is started. New backup is only started if previous backup has already completed.
     */
    backupHour?: number;
    /**
     * The minute of an hour when backup for the service is started. New backup is only started if previous backup has already completed.
     */
    backupMinute?: number;
    /**
     * Register AAAA DNS records for the service, and allow IPv6 packets to service ports.
     */
    enableIpv6?: boolean;
    /**
     * Allow incoming connections from CIDR address block, e.g. '10.20.0.0/16'.
     */
    ipFilterObjects?: outputs.PgPgUserConfigIpFilterObject[];
    /**
     * Allow incoming connections from CIDR address block, e.g. '10.20.0.0/16'.
     */
    ipFilterStrings?: string[];
    /**
     * Allow incoming connections from CIDR address block, e.g. '10.20.0.0/16'.
     *
     * @deprecated This will be removed in v5.0.0 and replaced with ip_filter_string instead.
     */
    ipFilters?: string[];
    /**
     * Migrate data from existing server.
     */
    migration?: outputs.PgPgUserConfigMigration;
    /**
     * postgresql.conf configuration values.
     */
    pg?: outputs.PgPgUserConfigPg;
    /**
     * System-wide settings for the pg*qualstats extension.
     */
    pgQualstats?: outputs.PgPgUserConfigPgQualstats;
    /**
     * Use readReplica service integration instead.
     *
     * @deprecated Usage of this field is discouraged.
     */
    pgReadReplica?: boolean;
    /**
     * Name of the PG Service from which to fork (deprecated, use service*to*fork_from). This has effect only when a new service is being created.
     *
     * @deprecated Usage of this field is discouraged.
     */
    pgServiceToForkFrom?: string;
    /**
     * Enable the pg*stat*monitor extension. Enabling this extension will cause the cluster to be restarted.When this extension is enabled, pg*stat*statements results for utility commands are unreliable. The default value is `false`.
     */
    pgStatMonitorEnable?: boolean;
    /**
     * PostgreSQL major version.
     */
    pgVersion?: string;
    /**
     * PGBouncer connection pooling settings.
     */
    pgbouncer?: outputs.PgPgUserConfigPgbouncer;
    /**
     * System-wide settings for pglookout.
     */
    pglookout?: outputs.PgPgUserConfigPglookout;
    /**
     * Allow access to selected service ports from private networks.
     */
    privateAccess?: outputs.PgPgUserConfigPrivateAccess;
    /**
     * Allow access to selected service components through Privatelink.
     */
    privatelinkAccess?: outputs.PgPgUserConfigPrivatelinkAccess;
    /**
     * Name of another project to fork a service from. This has effect only when a new service is being created.
     */
    projectToForkFrom?: string;
    /**
     * Allow access to selected service ports from the public Internet.
     */
    publicAccess?: outputs.PgPgUserConfigPublicAccess;
    /**
     * Recovery target time when forking a service. This has effect only when a new service is being created.
     */
    recoveryTargetTime?: string;
    /**
     * Store logs for the service so that they are available in the HTTP API and console.
     */
    serviceLog?: boolean;
    /**
     * Name of another service to fork from. This has effect only when a new service is being created.
     */
    serviceToForkFrom?: string;
    /**
     * Percentage of total RAM that the database server uses for shared memory buffers. Valid range is 20-60 (float), which corresponds to 20% - 60%. This setting adjusts the sharedBuffers configuration value.
     */
    sharedBuffersPercentage?: number;
    /**
     * Use static public IP addresses.
     */
    staticIps?: boolean;
    /**
     * Synchronous replication type. Note that the service plan also needs to support synchronous replication.
     */
    synchronousReplication?: string;
    /**
     * System-wide settings for the timescaledb extension.
     */
    timescaledb?: outputs.PgPgUserConfigTimescaledb;
    /**
     * Variant of the PostgreSQL service, may affect the features that are exposed by default.
     */
    variant?: string;
    /**
     * Sets the maximum amount of memory to be used by a query operation (such as a sort or hash table) before writing to temporary disk files, in MB. Default is 1MB + 0.075% of total RAM (up to 32MB).
     */
    workMem?: number;
}

export interface PgPgUserConfigIpFilterObject {
    /**
     * Description for IP filter list entry.
     */
    description?: string;
    /**
     * CIDR address block.
     */
    network: string;
}

export interface PgPgUserConfigMigration {
    /**
     * Primary PostgreSQL database name
     */
    dbname?: string;
    host: string;
    /**
     * Comma-separated list of databases, which should be ignored during migration (supported by MySQL and PostgreSQL only at the moment).
     */
    ignoreDbs?: string;
    /**
     * The migration method to be used (currently supported only by Redis, Dragonfly, MySQL and PostgreSQL service types).
     */
    method?: string;
    /**
     * PostgreSQL admin user password
     */
    password?: string;
    port: number;
    ssl?: boolean;
    /**
     * User name for authentication with the server where to migrate data from.
     */
    username?: string;
}

export interface PgPgUserConfigPg {
    /**
     * Specifies a fraction of the table size to add to autovacuum*analyze*threshold when deciding whether to trigger an ANALYZE. The default is 0.2 (20% of table size).
     */
    autovacuumAnalyzeScaleFactor?: number;
    /**
     * Specifies the minimum number of inserted, updated or deleted tuples needed to trigger an  ANALYZE in any one table. The default is 50 tuples.
     */
    autovacuumAnalyzeThreshold?: number;
    /**
     * Specifies the maximum age (in transactions) that a table's pg_class.relfrozenxid field can attain before a VACUUM operation is forced to prevent transaction ID wraparound within the table. Note that the system will launch autovacuum processes to prevent wraparound even when autovacuum is otherwise disabled. This parameter will cause the server to be restarted.
     */
    autovacuumFreezeMaxAge?: number;
    /**
     * Specifies the maximum number of autovacuum processes (other than the autovacuum launcher) that may be running at any one time. The default is three. This parameter can only be set at server start.
     */
    autovacuumMaxWorkers?: number;
    /**
     * Specifies the minimum delay between autovacuum runs on any given database. The delay is measured in seconds, and the default is one minute.
     */
    autovacuumNaptime?: number;
    /**
     * Specifies the cost delay value that will be used in automatic VACUUM operations. If -1 is specified, the regular vacuum*cost*delay value will be used. The default value is 20 milliseconds.
     */
    autovacuumVacuumCostDelay?: number;
    /**
     * Specifies the cost limit value that will be used in automatic VACUUM operations. If -1 is specified (which is the default), the regular vacuum*cost*limit value will be used.
     */
    autovacuumVacuumCostLimit?: number;
    /**
     * Specifies a fraction of the table size to add to autovacuum*vacuum*threshold when deciding whether to trigger a VACUUM. The default is 0.2 (20% of table size).
     */
    autovacuumVacuumScaleFactor?: number;
    /**
     * Specifies the minimum number of updated or deleted tuples needed to trigger a VACUUM in any one table. The default is 50 tuples.
     */
    autovacuumVacuumThreshold?: number;
    /**
     * Specifies the delay between activity rounds for the background writer in milliseconds. Default is 200.
     */
    bgwriterDelay?: number;
    /**
     * Whenever more than bgwriter*flush*after bytes have been written by the background writer, attempt to force the OS to issue these writes to the underlying storage. Specified in kilobytes, default is 512. Setting of 0 disables forced writeback.
     */
    bgwriterFlushAfter?: number;
    /**
     * In each round, no more than this many buffers will be written by the background writer. Setting this to zero disables background writing. Default is 100.
     */
    bgwriterLruMaxpages?: number;
    /**
     * The average recent need for new buffers is multiplied by bgwriter*lru*multiplier to arrive at an estimate of the number that will be needed during the next round, (up to bgwriter*lru*maxpages). 1.0 represents a “just in time” policy of writing exactly the number of buffers predicted to be needed. Larger values provide some cushion against spikes in demand, while smaller values intentionally leave writes to be done by server processes. The default is 2.0.
     */
    bgwriterLruMultiplier?: number;
    /**
     * This is the amount of time, in milliseconds, to wait on a lock before checking to see if there is a deadlock condition.
     */
    deadlockTimeout?: number;
    /**
     * Specifies the default TOAST compression method for values of compressible columns (the default is lz4).
     */
    defaultToastCompression?: string;
    /**
     * Time out sessions with open transactions after this number of milliseconds.
     */
    idleInTransactionSessionTimeout?: number;
    /**
     * Controls system-wide use of Just-in-Time Compilation (JIT).
     */
    jit?: boolean;
    /**
     * Causes each action executed by autovacuum to be logged if it ran for at least the specified number of milliseconds. Setting this to zero logs all autovacuum actions. Minus-one (the default) disables logging autovacuum actions.
     */
    logAutovacuumMinDuration?: number;
    /**
     * Controls the amount of detail written in the server log for each message that is logged.
     */
    logErrorVerbosity?: string;
    /**
     * Choose from one of the available log-formats. These can support popular log analyzers like pgbadger, pganalyze etc.
     */
    logLinePrefix?: string;
    /**
     * Log statements that take more than this number of milliseconds to run, -1 disables.
     */
    logMinDurationStatement?: number;
    /**
     * Log statements for each temporary file created larger than this number of kilobytes, -1 disables.
     */
    logTempFiles?: number;
    /**
     * PostgreSQL maximum number of files that can be open per process.
     */
    maxFilesPerProcess?: number;
    /**
     * PostgreSQL maximum locks per transaction.
     */
    maxLocksPerTransaction?: number;
    /**
     * PostgreSQL maximum logical replication workers (taken from the pool of max*parallel*workers).
     */
    maxLogicalReplicationWorkers?: number;
    /**
     * Sets the maximum number of workers that the system can support for parallel queries.
     */
    maxParallelWorkers?: number;
    /**
     * Sets the maximum number of workers that can be started by a single Gather or Gather Merge node.
     */
    maxParallelWorkersPerGather?: number;
    /**
     * PostgreSQL maximum predicate locks per transaction.
     */
    maxPredLocksPerTransaction?: number;
    /**
     * PostgreSQL maximum prepared transactions.
     */
    maxPreparedTransactions?: number;
    /**
     * PostgreSQL maximum replication slots.
     */
    maxReplicationSlots?: number;
    /**
     * PostgreSQL maximum WAL size (MB) reserved for replication slots. Default is -1 (unlimited). wal*keep*size minimum WAL size setting takes precedence over this.
     */
    maxSlotWalKeepSize?: number;
    /**
     * Maximum depth of the stack in bytes.
     */
    maxStackDepth?: number;
    /**
     * Max standby archive delay in milliseconds.
     */
    maxStandbyArchiveDelay?: number;
    /**
     * Max standby streaming delay in milliseconds.
     */
    maxStandbyStreamingDelay?: number;
    /**
     * PostgreSQL maximum WAL senders.
     */
    maxWalSenders?: number;
    /**
     * Sets the maximum number of background processes that the system can support.
     */
    maxWorkerProcesses?: number;
    /**
     * Sets the time interval to run pg_partman's scheduled tasks.
     */
    pgPartmanBgwDotInterval?: number;
    /**
     * Controls which role to use for pg_partman's scheduled background tasks.
     */
    pgPartmanBgwDotRole?: string;
    /**
     * Enables or disables query plan monitoring.
     */
    pgStatMonitorDotPgsmEnableQueryPlan?: boolean;
    /**
     * Sets the maximum number of buckets .
     */
    pgStatMonitorDotPgsmMaxBuckets?: number;
    /**
     * Controls which statements are counted. Specify top to track top-level statements (those issued directly by clients), all to also track nested statements (such as statements invoked within functions), or none to disable statement statistics collection. The default value is top.
     */
    pgStatStatementsDotTrack?: string;
    /**
     * PostgreSQL temporary file limit in KiB, -1 for unlimited.
     */
    tempFileLimit?: number;
    /**
     * PostgreSQL service timezone.
     */
    timezone?: string;
    /**
     * Specifies the number of bytes reserved to track the currently executing command for each active session.
     */
    trackActivityQuerySize?: number;
    /**
     * Record commit time of transactions.
     */
    trackCommitTimestamp?: string;
    /**
     * Enables tracking of function call counts and time used.
     */
    trackFunctions?: string;
    /**
     * Enables timing of database I/O calls. This parameter is off by default, because it will repeatedly query the operating system for the current time, which may cause significant overhead on some platforms.
     */
    trackIoTiming?: string;
    /**
     * Terminate replication connections that are inactive for longer than this amount of time, in milliseconds. Setting this value to zero disables the timeout.
     */
    walSenderTimeout?: number;
    /**
     * WAL flush interval in milliseconds. Note that setting this value to lower than the default 200ms may negatively impact performance.
     */
    walWriterDelay?: number;
}

export interface PgPgUserConfigPgQualstats {
    /**
     * Enable / Disable pg_qualstats. The default value is `false`.
     */
    enabled?: boolean;
    /**
     * Error estimation num threshold to save quals. The default value is `0`.
     */
    minErrEstimateNum?: number;
    /**
     * Error estimation ratio threshold to save quals. The default value is `0`.
     */
    minErrEstimateRatio?: number;
    /**
     * Enable / Disable pgQualstats constants tracking. The default value is `true`.
     */
    trackConstants?: boolean;
    /**
     * Track quals on system catalogs too. The default value is `false`.
     */
    trackPgCatalog?: boolean;
}

export interface PgPgUserConfigPgbouncer {
    /**
     * If the automatically created database pools have been unused this many seconds, they are freed. If 0 then timeout is disabled. (seconds).
     */
    autodbIdleTimeout?: number;
    /**
     * Do not allow more than this many server connections per database (regardless of user). Setting it to 0 means unlimited.
     */
    autodbMaxDbConnections?: number;
    /**
     * PGBouncer pool mode.
     */
    autodbPoolMode?: string;
    /**
     * If non-zero then create automatically a pool of that size per user when a pool doesn't exist.
     */
    autodbPoolSize?: number;
    /**
     * List of parameters to ignore when given in startup packet.
     */
    ignoreStartupParameters?: string[];
    /**
     * Add more server connections to pool if below this number. Improves behavior when usual load comes suddenly back after period of total inactivity. The value is effectively capped at the pool size.
     */
    minPoolSize?: number;
    /**
     * If a server connection has been idle more than this many seconds it will be dropped. If 0 then timeout is disabled. (seconds).
     */
    serverIdleTimeout?: number;
    /**
     * The pooler will close an unused server connection that has been connected longer than this. (seconds).
     */
    serverLifetime?: number;
    /**
     * Run server*reset*query (DISCARD ALL) in all pooling modes.
     */
    serverResetQueryAlways?: boolean;
}

export interface PgPgUserConfigPglookout {
    /**
     * Number of seconds of master unavailability before triggering database failover to standby. The default value is `60`.
     */
    maxFailoverReplicationTimeLag?: number;
}

export interface PgPgUserConfigPrivateAccess {
    /**
     * postgresql.conf configuration values.
     */
    pg?: boolean;
    /**
     * PGBouncer connection pooling settings.
     */
    pgbouncer?: boolean;
    /**
     * Allow clients to connect to prometheus with a DNS name that always resolves to the service's private IP addresses. Only available in certain network locations.
     */
    prometheus?: boolean;
}

export interface PgPgUserConfigPrivatelinkAccess {
    /**
     * postgresql.conf configuration values.
     */
    pg?: boolean;
    /**
     * PGBouncer connection pooling settings.
     */
    pgbouncer?: boolean;
    /**
     * Allow clients to connect to prometheus with a DNS name that always resolves to the service's private IP addresses. Only available in certain network locations.
     */
    prometheus?: boolean;
}

export interface PgPgUserConfigPublicAccess {
    /**
     * postgresql.conf configuration values.
     */
    pg?: boolean;
    /**
     * PGBouncer connection pooling settings.
     */
    pgbouncer?: boolean;
    /**
     * Allow clients to connect to prometheus with a DNS name that always resolves to the service's private IP addresses. Only available in certain network locations.
     */
    prometheus?: boolean;
}

export interface PgPgUserConfigTimescaledb {
    /**
     * The number of background workers for timescaledb operations. You should configure this setting to the sum of your number of databases and the total number of concurrent background workers you want running at any given point in time. The default value is `16`.
     */
    maxBackgroundWorkers?: number;
}

export interface PgServiceIntegration {
    /**
     * Type of the service integration. The only supported value at the moment is `readReplica`
     */
    integrationType: string;
    /**
     * Name of the source service
     */
    sourceServiceName: string;
}

export interface PgTag {
    /**
     * Service tag key
     */
    key: string;
    /**
     * Service tag value
     */
    value: string;
}

export interface PgTechEmail {
    /**
     * An email address to contact for technical issues
     */
    email: string;
}

export interface ProjectTag {
    /**
     * Project tag key
     */
    key: string;
    /**
     * Project tag value
     */
    value: string;
}

export interface RedisComponent {
    component: string;
    connectionUri: string;
    host: string;
    kafkaAuthenticationMethod: string;
    port: number;
    route: string;
    ssl: boolean;
    usage: string;
}

export interface RedisRedi {
}

export interface RedisRedisUserConfig {
    additionalBackupRegions?: string;
    ipFilterObjects?: outputs.RedisRedisUserConfigIpFilterObject[];
    ipFilterStrings?: string[];
    /**
     * @deprecated This will be removed in v5.0.0 and replaced with ip_filter_string instead.
     */
    ipFilters?: string[];
    migration?: outputs.RedisRedisUserConfigMigration;
    privateAccess?: outputs.RedisRedisUserConfigPrivateAccess;
    privatelinkAccess?: outputs.RedisRedisUserConfigPrivatelinkAccess;
    projectToForkFrom?: string;
    publicAccess?: outputs.RedisRedisUserConfigPublicAccess;
    recoveryBasebackupName?: string;
    redisAclChannelsDefault?: string;
    redisIoThreads?: number;
    redisLfuDecayTime?: number;
    redisLfuLogFactor?: number;
    redisMaxmemoryPolicy?: string;
    redisNotifyKeyspaceEvents?: string;
    redisNumberOfDatabases?: number;
    redisPersistence?: string;
    redisPubsubClientOutputBufferLimit?: number;
    redisSsl?: boolean;
    redisTimeout?: number;
    serviceLog?: boolean;
    serviceToForkFrom?: string;
    staticIps?: boolean;
}

export interface RedisRedisUserConfigIpFilterObject {
    description?: string;
    network: string;
}

export interface RedisRedisUserConfigMigration {
    dbname?: string;
    host: string;
    ignoreDbs?: string;
    method?: string;
    password?: string;
    port: number;
    ssl?: boolean;
    username?: string;
}

export interface RedisRedisUserConfigPrivateAccess {
    prometheus?: boolean;
    redis?: boolean;
}

export interface RedisRedisUserConfigPrivatelinkAccess {
    prometheus?: boolean;
    redis?: boolean;
}

export interface RedisRedisUserConfigPublicAccess {
    prometheus?: boolean;
    redis?: boolean;
}

export interface RedisServiceIntegration {
    integrationType: string;
    sourceServiceName: string;
}

export interface RedisTag {
    key: string;
    value: string;
}

export interface RedisTechEmail {
    email: string;
}

export interface ServiceIntegrationClickhouseKafkaUserConfig {
    /**
     * Tables to create.
     */
    tables?: outputs.ServiceIntegrationClickhouseKafkaUserConfigTable[];
}

export interface ServiceIntegrationClickhouseKafkaUserConfigTable {
    /**
     * Action to take when there is no initial offset in offset store or the desired offset is out of range. The default value is `earliest`.
     */
    autoOffsetReset?: string;
    /**
     * Table columns.
     */
    columns?: outputs.ServiceIntegrationClickhouseKafkaUserConfigTableColumn[];
    /**
     * Message data format. The default value is `JSONEachRow`.
     */
    dataFormat: string;
    /**
     * Method to read DateTime from text input formats. The default value is `basic`.
     */
    dateTimeInputFormat?: string;
    /**
     * Kafka consumers group. The default value is `clickhouse`.
     */
    groupName: string;
    /**
     * How to handle errors for Kafka engine. The default value is `default`.
     */
    handleErrorMode?: string;
    /**
     * Number of row collected by poll(s) for flushing data from Kafka. The default value is `0`.
     */
    maxBlockSize?: number;
    /**
     * The maximum number of rows produced in one kafka message for row-based formats. The default value is `1`.
     */
    maxRowsPerMessage?: number;
    /**
     * Column name.
     */
    name: string;
    /**
     * The number of consumers per table per replica. The default value is `1`.
     */
    numConsumers?: number;
    /**
     * Maximum amount of messages to be polled in a single Kafka poll. The default value is `0`.
     */
    pollMaxBatchSize?: number;
    /**
     * Skip at least this number of broken messages from Kafka topic per block. The default value is `0`.
     */
    skipBrokenMessages?: number;
    /**
     * Kafka topics.
     */
    topics?: outputs.ServiceIntegrationClickhouseKafkaUserConfigTableTopic[];
}

export interface ServiceIntegrationClickhouseKafkaUserConfigTableColumn {
    /**
     * Column name.
     */
    name: string;
    /**
     * Column type.
     */
    type: string;
}

export interface ServiceIntegrationClickhouseKafkaUserConfigTableTopic {
    /**
     * Column name.
     */
    name: string;
}

export interface ServiceIntegrationClickhousePostgresqlUserConfig {
    /**
     * Databases to expose.
     */
    databases?: outputs.ServiceIntegrationClickhousePostgresqlUserConfigDatabase[];
}

export interface ServiceIntegrationClickhousePostgresqlUserConfigDatabase {
    /**
     * PostgreSQL database to expose. The default value is `defaultdb`.
     */
    database?: string;
    /**
     * PostgreSQL schema to expose. The default value is `public`.
     */
    schema?: string;
}

export interface ServiceIntegrationDatadogUserConfig {
    /**
     * Enable Datadog Database Monitoring.
     */
    datadogDbmEnabled?: boolean;
    /**
     * Custom tags provided by user.
     */
    datadogTags?: outputs.ServiceIntegrationDatadogUserConfigDatadogTag[];
    /**
     * List of custom metrics.
     */
    excludeConsumerGroups?: string[];
    /**
     * List of topics to exclude.
     */
    excludeTopics?: string[];
    /**
     * List of custom metrics.
     */
    includeConsumerGroups?: string[];
    /**
     * List of topics to include.
     */
    includeTopics?: string[];
    /**
     * List of custom metrics.
     */
    kafkaCustomMetrics?: string[];
    /**
     * Maximum number of JMX metrics to send.
     */
    maxJmxMetrics?: number;
    /**
     * Datadog Opensearch Options.
     */
    opensearch?: outputs.ServiceIntegrationDatadogUserConfigOpensearch;
    /**
     * Datadog Redis Options.
     */
    redis?: outputs.ServiceIntegrationDatadogUserConfigRedis;
}

export interface ServiceIntegrationDatadogUserConfigDatadogTag {
    /**
     * Optional tag explanation.
     */
    comment?: string;
    /**
     * Tag format and usage are described here: https://docs.datadoghq.com/getting_started/tagging. Tags with prefix 'aiven-' are reserved for Aiven.
     */
    tag: string;
}

export interface ServiceIntegrationDatadogUserConfigOpensearch {
    /**
     * Enable Datadog Opensearch Index Monitoring.
     */
    indexStatsEnabled?: boolean;
    /**
     * Enable Datadog Opensearch Pending Task Monitoring.
     */
    pendingTaskStatsEnabled?: boolean;
    /**
     * Enable Datadog Opensearch Primary Shard Monitoring.
     */
    pshardStatsEnabled?: boolean;
}

export interface ServiceIntegrationDatadogUserConfigRedis {
    /**
     * Enable commandStats option in the agent's configuration. The default value is `false`.
     */
    commandStatsEnabled?: boolean;
}

export interface ServiceIntegrationEndpointDatadogUserConfig {
    /**
     * Datadog API key.
     */
    datadogApiKey: string;
    /**
     * Custom tags provided by user.
     */
    datadogTags?: outputs.ServiceIntegrationEndpointDatadogUserConfigDatadogTag[];
    /**
     * Disable consumer group metrics.
     */
    disableConsumerStats?: boolean;
    /**
     * Number of separate instances to fetch kafka consumer statistics with.
     */
    kafkaConsumerCheckInstances?: number;
    /**
     * Number of seconds that datadog will wait to get consumer statistics from brokers.
     */
    kafkaConsumerStatsTimeout?: number;
    /**
     * Maximum number of partition contexts to send.
     */
    maxPartitionContexts?: number;
    /**
     * Datadog intake site. Defaults to datadoghq.com.
     */
    site?: string;
}

export interface ServiceIntegrationEndpointDatadogUserConfigDatadogTag {
    /**
     * Optional tag explanation.
     */
    comment?: string;
    /**
     * Tag format and usage are described here: https://docs.datadoghq.com/getting_started/tagging. Tags with prefix 'aiven-' are reserved for Aiven.
     */
    tag: string;
}

export interface ServiceIntegrationEndpointExternalAwsCloudwatchLogsUserConfig {
    /**
     * AWS access key. Required permissions are logs:CreateLogGroup, logs:CreateLogStream, logs:PutLogEvents and logs:DescribeLogStreams.
     */
    accessKey: string;
    /**
     * AWS CloudWatch log group name.
     */
    logGroupName?: string;
    /**
     * AWS region.
     */
    region: string;
    /**
     * AWS secret key.
     */
    secretKey: string;
}

export interface ServiceIntegrationEndpointExternalAwsCloudwatchMetricsUserConfig {
    /**
     * AWS access key. Required permissions are cloudwatch:PutMetricData.
     */
    accessKey: string;
    /**
     * AWS CloudWatch Metrics Namespace.
     */
    namespace: string;
    /**
     * AWS region.
     */
    region: string;
    /**
     * AWS secret key.
     */
    secretKey: string;
}

export interface ServiceIntegrationEndpointExternalElasticsearchLogsUserConfig {
    /**
     * PEM encoded CA certificate.
     */
    ca?: string;
    /**
     * Maximum number of days of logs to keep. The default value is `3`.
     */
    indexDaysMax?: number;
    /**
     * Elasticsearch index prefix. The default value is `logs`.
     */
    indexPrefix: string;
    /**
     * Elasticsearch request timeout limit. The default value is `10.0`.
     */
    timeout?: number;
    /**
     * Elasticsearch connection URL.
     */
    url: string;
}

export interface ServiceIntegrationEndpointExternalGoogleCloudBigquery {
    /**
     * GCP project id.
     */
    projectId: string;
    /**
     * This is a JSON object with the fields documented in https://cloud.google.com/iam/docs/creating-managing-service-account-keys .
     */
    serviceAccountCredentials: string;
}

export interface ServiceIntegrationEndpointExternalGoogleCloudLoggingUserConfig {
    /**
     * Google Cloud Logging log id.
     */
    logId: string;
    /**
     * GCP project id.
     */
    projectId: string;
    /**
     * This is a JSON object with the fields documented in https://cloud.google.com/iam/docs/creating-managing-service-account-keys .
     */
    serviceAccountCredentials: string;
}

export interface ServiceIntegrationEndpointExternalKafkaUserConfig {
    /**
     * Bootstrap servers.
     */
    bootstrapServers: string;
    /**
     * SASL mechanism used for connections to the Kafka server.
     */
    saslMechanism?: string;
    /**
     * Password for SASL PLAIN mechanism in the Kafka server.
     */
    saslPlainPassword?: string;
    /**
     * Username for SASL PLAIN mechanism in the Kafka server.
     */
    saslPlainUsername?: string;
    /**
     * Security protocol.
     */
    securityProtocol: string;
    /**
     * PEM-encoded CA certificate.
     */
    sslCaCert?: string;
    /**
     * PEM-encoded client certificate.
     */
    sslClientCert?: string;
    /**
     * PEM-encoded client key.
     */
    sslClientKey?: string;
    /**
     * The endpoint identification algorithm to validate server hostname using server certificate.
     */
    sslEndpointIdentificationAlgorithm?: string;
}

export interface ServiceIntegrationEndpointExternalOpensearchLogsUserConfig {
    /**
     * PEM encoded CA certificate.
     */
    ca?: string;
    /**
     * Maximum number of days of logs to keep. The default value is `3`.
     */
    indexDaysMax?: number;
    /**
     * OpenSearch index prefix. The default value is `logs`.
     */
    indexPrefix: string;
    /**
     * OpenSearch request timeout limit. The default value is `10.0`.
     */
    timeout?: number;
    /**
     * OpenSearch connection URL.
     */
    url: string;
}

export interface ServiceIntegrationEndpointExternalPostgresql {
    /**
     * Hostname or IP address of the server.
     */
    host: string;
    /**
     * Password.
     */
    password: string;
    /**
     * Port number of the server.
     */
    port: number;
    /**
     * SSL Mode. The default value is `verify-full`.
     */
    sslMode?: string;
    /**
     * SSL Root Cert.
     */
    sslRootCert?: string;
    /**
     * User name.
     */
    username: string;
}

export interface ServiceIntegrationEndpointExternalSchemaRegistryUserConfig {
    /**
     * Authentication method.
     */
    authentication: string;
    /**
     * Basic authentication password.
     */
    basicAuthPassword?: string;
    /**
     * Basic authentication user name.
     */
    basicAuthUsername?: string;
    /**
     * Schema Registry URL.
     */
    url: string;
}

export interface ServiceIntegrationEndpointJolokiaUserConfig {
    /**
     * Jolokia basic authentication password.
     */
    basicAuthPassword?: string;
    /**
     * Jolokia basic authentication username.
     */
    basicAuthUsername?: string;
}

export interface ServiceIntegrationEndpointPrometheusUserConfig {
    /**
     * Prometheus basic authentication password.
     */
    basicAuthPassword?: string;
    /**
     * Prometheus basic authentication username.
     */
    basicAuthUsername?: string;
}

export interface ServiceIntegrationEndpointRsyslogUserConfig {
    /**
     * PEM encoded CA certificate.
     */
    ca?: string;
    /**
     * PEM encoded client certificate.
     */
    cert?: string;
    /**
     * message format. The default value is `rfc5424`.
     */
    format: string;
    /**
     * PEM encoded client key.
     */
    key?: string;
    /**
     * custom syslog message format.
     */
    logline?: string;
    /**
     * rsyslog server port. The default value is `514`.
     */
    port: number;
    /**
     * Structured data block for log message.
     */
    sd?: string;
    /**
     * rsyslog server IP address or hostname.
     */
    server: string;
    /**
     * Require TLS. The default value is `true`.
     */
    tls: boolean;
}

export interface ServiceIntegrationExternalAwsCloudwatchMetricsUserConfig {
    /**
     * Metrics to not send to AWS CloudWatch (takes precedence over extra*metrics).
     */
    droppedMetrics?: outputs.ServiceIntegrationExternalAwsCloudwatchMetricsUserConfigDroppedMetric[];
    /**
     * Metrics to allow through to AWS CloudWatch (in addition to default metrics).
     */
    extraMetrics?: outputs.ServiceIntegrationExternalAwsCloudwatchMetricsUserConfigExtraMetric[];
}

export interface ServiceIntegrationExternalAwsCloudwatchMetricsUserConfigDroppedMetric {
    /**
     * Identifier of a value in the metric.
     */
    field: string;
    /**
     * Identifier of the metric.
     */
    metric: string;
}

export interface ServiceIntegrationExternalAwsCloudwatchMetricsUserConfigExtraMetric {
    /**
     * Identifier of a value in the metric.
     */
    field: string;
    /**
     * Identifier of the metric.
     */
    metric: string;
}

export interface ServiceIntegrationKafkaConnectUserConfig {
    /**
     * Kafka Connect service configuration values.
     */
    kafkaConnect?: outputs.ServiceIntegrationKafkaConnectUserConfigKafkaConnect;
}

export interface ServiceIntegrationKafkaConnectUserConfigKafkaConnect {
    /**
     * The name of the topic where connector and task configuration data are stored.This must be the same for all workers with the same group_id.
     */
    configStorageTopic?: string;
    /**
     * A unique string that identifies the Connect cluster group this worker belongs to.
     */
    groupId?: string;
    /**
     * The name of the topic where connector and task configuration offsets are stored.This must be the same for all workers with the same group_id.
     */
    offsetStorageTopic?: string;
    /**
     * The name of the topic where connector and task configuration status updates are stored.This must be the same for all workers with the same group_id.
     */
    statusStorageTopic?: string;
}

export interface ServiceIntegrationKafkaLogsUserConfig {
    /**
     * Topic name.
     */
    kafkaTopic: string;
    /**
     * The list of logging fields that will be sent to the integration logging service. The MESSAGE and timestamp fields are always sent.
     */
    selectedLogFields?: string[];
}

export interface ServiceIntegrationKafkaMirrormakerUserConfig {
    /**
     * The alias under which the Kafka cluster is known to MirrorMaker. Can contain the following symbols: ASCII alphanumerics, '.', '_', and '-'.
     */
    clusterAlias?: string;
    /**
     * Kafka MirrorMaker configuration values.
     */
    kafkaMirrormaker?: outputs.ServiceIntegrationKafkaMirrormakerUserConfigKafkaMirrormaker;
}

export interface ServiceIntegrationKafkaMirrormakerUserConfigKafkaMirrormaker {
    /**
     * The minimum amount of data the server should return for a fetch request.
     */
    consumerFetchMinBytes?: number;
    /**
     * The batch size in bytes producer will attempt to collect before publishing to broker.
     */
    producerBatchSize?: number;
    /**
     * The amount of bytes producer can use for buffering data before publishing to broker.
     */
    producerBufferMemory?: number;
    /**
     * Specify the default compression type for producers. This configuration accepts the standard compression codecs ('gzip', 'snappy', 'lz4', 'zstd'). It additionally accepts 'none' which is the default and equivalent to no compression.
     */
    producerCompressionType?: string;
    /**
     * The linger time (ms) for waiting new data to arrive for publishing.
     */
    producerLingerMs?: number;
    /**
     * The maximum request size in bytes.
     */
    producerMaxRequestSize?: number;
}

export interface ServiceIntegrationLogsUserConfig {
    /**
     * Elasticsearch index retention limit. The default value is `3`.
     */
    elasticsearchIndexDaysMax?: number;
    /**
     * Elasticsearch index prefix. The default value is `logs`.
     */
    elasticsearchIndexPrefix?: string;
    /**
     * The list of logging fields that will be sent to the integration logging service. The MESSAGE and timestamp fields are always sent.
     */
    selectedLogFields?: string[];
}

export interface ServiceIntegrationMetricsUserConfig {
    /**
     * Name of the database where to store metric datapoints. Only affects PostgreSQL destinations. Defaults to 'metrics'. Note that this must be the same for all metrics integrations that write data to the same PostgreSQL service.
     */
    database?: string;
    /**
     * Number of days to keep old metrics. Only affects PostgreSQL destinations. Set to 0 for no automatic cleanup. Defaults to 30 days.
     */
    retentionDays?: number;
    /**
     * Name of a user that can be used to read metrics. This will be used for Grafana integration (if enabled) to prevent Grafana users from making undesired changes. Only affects PostgreSQL destinations. Defaults to 'metrics_reader'. Note that this must be the same for all metrics integrations that write data to the same PostgreSQL service.
     */
    roUsername?: string;
    /**
     * Configuration options for metrics where source service is MySQL.
     */
    sourceMysql?: outputs.ServiceIntegrationMetricsUserConfigSourceMysql;
    /**
     * Name of the user used to write metrics. Only affects PostgreSQL destinations. Defaults to 'metrics_writer'. Note that this must be the same for all metrics integrations that write data to the same PostgreSQL service.
     */
    username?: string;
}

export interface ServiceIntegrationMetricsUserConfigSourceMysql {
    /**
     * Configuration options for Telegraf MySQL input plugin.
     */
    telegraf?: outputs.ServiceIntegrationMetricsUserConfigSourceMysqlTelegraf;
}

export interface ServiceIntegrationMetricsUserConfigSourceMysqlTelegraf {
    /**
     * Gather metrics from PERFORMANCE*SCHEMA.EVENT*WAITS.
     */
    gatherEventWaits?: boolean;
    /**
     * gather metrics from PERFORMANCE*SCHEMA.FILE*SUMMARY*BY*EVENT_NAME.
     */
    gatherFileEventsStats?: boolean;
    /**
     * Gather metrics from PERFORMANCE*SCHEMA.TABLE*IO*WAITS*SUMMARY*BY*INDEX_USAGE.
     */
    gatherIndexIoWaits?: boolean;
    /**
     * Gather autoIncrement columns and max values from information schema.
     */
    gatherInfoSchemaAutoInc?: boolean;
    /**
     * Gather metrics from INFORMATION*SCHEMA.INNODB*METRICS.
     */
    gatherInnodbMetrics?: boolean;
    /**
     * Gather metrics from PERFORMANCE*SCHEMA.EVENTS*STATEMENTS*SUMMARY*BY_DIGEST.
     */
    gatherPerfEventsStatements?: boolean;
    /**
     * Gather thread state counts from INFORMATION_SCHEMA.PROCESSLIST.
     */
    gatherProcessList?: boolean;
    /**
     * Gather metrics from SHOW SLAVE STATUS command output.
     */
    gatherSlaveStatus?: boolean;
    /**
     * Gather metrics from PERFORMANCE*SCHEMA.TABLE*IO*WAITS*SUMMARY*BY*TABLE.
     */
    gatherTableIoWaits?: boolean;
    /**
     * Gather metrics from PERFORMANCE*SCHEMA.TABLE*LOCK_WAITS.
     */
    gatherTableLockWaits?: boolean;
    /**
     * Gather metrics from INFORMATION_SCHEMA.TABLES.
     */
    gatherTableSchema?: boolean;
    /**
     * Truncates digest text from perf*events*statements into this many characters.
     */
    perfEventsStatementsDigestTextLimit?: number;
    /**
     * Limits metrics from perf*events*statements.
     */
    perfEventsStatementsLimit?: number;
    /**
     * Only include perf*events*statements whose last seen is less than this many seconds.
     */
    perfEventsStatementsTimeLimit?: number;
}

