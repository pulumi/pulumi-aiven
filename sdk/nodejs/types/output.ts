// *** WARNING: this file was generated by the Pulumi Terraform Bridge (tfgen) Tool. ***
// *** Do not edit by hand unless you're certain you know what you are doing! ***

import * as pulumi from "@pulumi/pulumi";
import { input as inputs, output as outputs } from "../types";

export interface CassandraCassandra {
}

export interface CassandraCassandraUserConfig {
    /**
     * Cassandra configuration values
     */
    cassandra?: outputs.CassandraCassandraUserConfigCassandra;
    cassandraVersion?: string;
    /**
     * allow incoming connections from CIDR address block, e.g. `10.20.0.0/16`
     */
    ipFilters?: string[];
    /**
     * sets the service into migration mode enabling the sstableloader 
     * utility to be used to upload Cassandra data files. Available only on service create.
     */
    migrateSstableloader?: string;
    /**
     * Allow access to selected service ports from private networks.
     */
    privateAccess?: outputs.CassandraCassandraUserConfigPrivateAccess;
    projectToForkFrom?: string;
    /**
     * Allow access to selected service ports from the public Internet
     */
    publicAccess?: outputs.CassandraCassandraUserConfigPublicAccess;
    /**
     * Name of another service to fork from. This has effect only 
     * when a new service is being created.
     */
    serviceToForkFrom?: string;
    staticIps?: string;
}

export interface CassandraCassandraUserConfigCassandra {
    /**
     * Fail any multiple-partition batch exceeding this value. 
     * 50kb (10x warn threshold) by default.
     */
    batchSizeFailThresholdInKb?: string;
    /**
     * Log a warning message on any multiple-partition 
     * batch size exceeding this value.5kb per batch by default.Caution should be taken on increasing
     * the size of this thresholdas it can lead to node instability.
     */
    batchSizeWarnThresholdInKb?: string;
}

export interface CassandraCassandraUserConfigPrivateAccess {
    /**
     * Allow clients to connect to prometheus from the public internet 
     * for service nodes that are in a project VPC or another type of private network.
     */
    prometheus?: string;
}

export interface CassandraCassandraUserConfigPublicAccess {
    /**
     * Allow clients to connect to prometheus from the public internet 
     * for service nodes that are in a project VPC or another type of private network.
     */
    prometheus?: string;
}

export interface CassandraComponent {
    component: string;
    host: string;
    kafkaAuthenticationMethod: string;
    port: number;
    route: string;
    ssl: boolean;
    usage: string;
}

export interface CassandraServiceIntegration {
    integrationType: string;
    sourceServiceName: string;
}

export interface ElasticSearchAclAcl {
    rules: outputs.ElasticSearchAclAclRule[];
    username: string;
}

export interface ElasticSearchAclAclRule {
    index: string;
    permission: string;
}

export interface ElasticSearchComponent {
    component: string;
    host: string;
    kafkaAuthenticationMethod: string;
    port: number;
    route: string;
    ssl: boolean;
    usage: string;
}

export interface ElasticSearchElasticsearch {
    /**
     * URI for Kibana frontend.
     */
    kibanaUri: string;
}

export interface ElasticSearchElasticsearchUserConfig {
    /**
     * Serve the web frontend using a custom CNAME pointing to the 
     * Aiven DNS name.
     */
    customDomain?: string;
    /**
     * Disable automatic replication factor 
     * adjustment for multi-node services. By default, Aiven ensures all indexes are replicated at
     * least to two nodes. Note: setting this to true increases a risk of data loss in case of
     * virtual machine failure.
     */
    disableReplicationFactorAdjustment?: string;
    /**
     * Allow clients to connect to elasticsearch from the public 
     * internet for service nodes that are in a project VPC or another type of private network.
     */
    elasticsearch?: outputs.ElasticSearchElasticsearchUserConfigElasticsearch;
    /**
     * Elasticsearch major version.
     */
    elasticsearchVersion?: string;
    /**
     * Glob pattern and number of indexes matching that pattern to 
     * be kept.
     */
    indexPatterns?: outputs.ElasticSearchElasticsearchUserConfigIndexPattern[];
    /**
     * Template settings for all new indexe.
     */
    indexTemplate?: outputs.ElasticSearchElasticsearchUserConfigIndexTemplate;
    /**
     * allow incoming connections from CIDR address block, e.g. `10.20.0.0/16`
     */
    ipFilters?: string[];
    keepIndexRefreshInterval?: string;
    /**
     * Allow clients to connect to kibana from the public internet for 
     * service nodes that are in a project VPC or another type of private network.
     */
    kibana?: outputs.ElasticSearchElasticsearchUserConfigKibana;
    /**
     * Maximum number of indexes to keep before deleting the oldest one.
     */
    maxIndexCount?: string;
    opensearchVersion?: string;
    /**
     * Allow access to selected service ports from private networks.
     */
    privateAccess?: outputs.ElasticSearchElasticsearchUserConfigPrivateAccess;
    /**
     * Allow access to selected service components through Privatelink
     */
    privatelinkAccess?: outputs.ElasticSearchElasticsearchUserConfigPrivatelinkAccess;
    /**
     * Name of another project to fork a service from. This has 
     * effect only when a new service is being created.
     */
    projectToForkFrom?: string;
    /**
     * Allow access to selected service ports from the public Internet.
     */
    publicAccess?: outputs.ElasticSearchElasticsearchUserConfigPublicAccess;
    /**
     * Name of the basebackup to restore in forked service.
     */
    recoveryBasebackupName?: string;
    /**
     * Name of another service to fork from. This has effect 
     * only when a new service is being created.
     */
    serviceToForkFrom?: string;
    staticIps?: string;
}

export interface ElasticSearchElasticsearchUserConfigElasticsearch {
    /**
     * Explicitly allow or block automatic 
     * creation of indices. Defaults to true
     */
    actionAutoCreateIndexEnabled?: string;
    /**
     * Require explicit index names when deleting
     */
    actionDestructiveRequiresName?: string;
    /**
     * Controls the number of shards allowed in the
     * cluster per data node.
     */
    clusterMaxShardsPerNode?: string;
    /**
     * Maximum content length for HTTP requests to 
     * the Elasticsearch HTTP API, in bytes.
     */
    httpMaxContentLength?: string;
    /**
     * The max size of allowed headers, in bytes.
     */
    httpMaxHeaderSize?: string;
    /**
     * The max length of an HTTP URL, in bytes.
     */
    httpMaxInitialLineLength?: string;
    /**
     * Relative amount. Maximum amount of 
     * heap memory used for field data cache. This is an expert setting; decreasing the
     * value too much will increase overhead of loading field data; too much memory used
     * for field data cache will decrease amount of heap available for other operations.
     */
    indicesFielddataCacheSize?: string;
    /**
     * Percentage value. Default is 10%. 
     * Total amount of heap used for indexing buffer, before writing segments to disk.
     * This is an expert setting. Too low value will slow down indexing; too high value
     * will increase indexing performance but causes performance issues for query performance.
     */
    indicesMemoryIndexBufferSize?: string;
    /**
     * Percentage value. Default is 10%. 
     * Maximum amount of heap used for query cache. This is an expert setting.
     * Too low value will decrease query performance and increase performance for other
     * operations; too high value will cause issues with other Elasticsearch functionality.
     */
    indicesQueriesCacheSize?: string;
    /**
     * Maximum number of clauses Lucene 
     * BooleanQuery can have. The default value (1024) is relatively high, and increasing it
     * may cause performance issues. Investigate other approaches first before increasing this value.
     */
    indicesQueryBoolMaxClauseCount?: string;
    /**
     * Whitelisted addresses for reindexing. 
     * Changing this value will cause all Elasticsearch instances to restart.
     */
    reindexRemoteWhitelists?: string[];
    /**
     * Maximum number of aggregation buckets allowed 
     * in a single response. Elasticsearch default value is used when this is not defined.
     */
    searchMaxBuckets?: string;
    /**
     * Size for the thread pool queue. 
     * See documentation for exact details.
     */
    threadPoolAnalyzeQueueSize?: string;
    /**
     * Size for the thread pool. See documentation 
     * for exact details. Do note this may have maximum value depending on CPU count -
     * value is automatically lowered if set to higher than maximum value.
     */
    threadPoolAnalyzeSize?: string;
    /**
     * Size for the thread pool. See 
     * documentation for exact details. Do note this may have maximum value depending on
     * CPU count - value is automatically lowered if set to higher than maximum value.
     */
    threadPoolForceMergeSize?: string;
    /**
     * Size for the thread pool queue. See 
     * documentation for exact details.
     */
    threadPoolGetQueueSize?: string;
    /**
     * Size for the thread pool. See documentation 
     * for exact details. Do note this may have maximum value depending on CPU count -
     * value is automatically lowered if set to higher than maximum value.
     */
    threadPoolGetSize?: string;
    /**
     * Size for the thread pool queue. 
     * See documentation for exact details.
     */
    threadPoolIndexQueueSize?: string;
    /**
     * Size for the thread pool. See documentation 
     * for exact details. Do note this may have maximum value depending on CPU count -
     * value is automatically lowered if set to higher than maximum value.
     */
    threadPoolIndexSize?: string;
    /**
     * Size for the thread pool queue. See 
     * documentation for exact details.
     */
    threadPoolSearchQueueSize?: string;
    /**
     * Size for the thread pool. See documentation 
     * for exact details. Do note this may have maximum value depending on CPU count - value
     * is automatically lowered if set to higher than maximum value.
     */
    threadPoolSearchSize?: string;
    /**
     * Size for the thread pool queue. 
     * See documentation for exact details.
     */
    threadPoolSearchThrottledQueueSize?: string;
    /**
     * Size for the thread pool. See 
     * documentation for exact details. Do note this may have maximum value depending on
     * CPU count - value is automatically lowered if set to higher than maximum value.
     */
    threadPoolSearchThrottledSize?: string;
    /**
     * Size for the thread pool queue. See 
     * documentation for exact details.
     */
    threadPoolWriteQueueSize?: string;
    /**
     * Size for the thread pool. See documentation 
     * for exact details. Do note this may have maximum value depending on CPU count - value
     * is automatically lowered if set to higher than maximum value.
     */
    threadPoolWriteSize?: string;
}

export interface ElasticSearchElasticsearchUserConfigIndexPattern {
    /**
     * Maximum number of indexes to keep before deleting the oldest one.
     */
    maxIndexCount?: string;
    /**
     * Must consist of alpha-numeric characters, dashes, underscores, 
     * dots and glob characters (* and ?)
     */
    pattern?: string;
    /**
     * Deletion sorting algorithm
     */
    sortingAlgorithm?: string;
}

export interface ElasticSearchElasticsearchUserConfigIndexTemplate {
    /**
     * The maximum number of nested JSON objects that 
     * a single document can contain across all nested types. This limit helps to prevent out of
     * memory errors when a document contains too many nested objects. Default is 10000.
     */
    mappingNestedObjectsLimit?: string;
    /**
     * The number of replicas each primary shard has.
     */
    numberOfReplicas?: string;
    /**
     * The number of primary shards that an index should have.
     */
    numberOfShards?: string;
}

export interface ElasticSearchElasticsearchUserConfigKibana {
    /**
     * Timeout in milliseconds for requests 
     * made by Kibana towards Elasticsearch.
     */
    elasticsearchRequestTimeout?: string;
    /**
     * Enable or disable Kibana.
     */
    enabled?: string;
    /**
     * Limits the maximum amount of memory (in MiB) the 
     * Kibana process can use. This sets the maxOldSpaceSize option of the nodejs running
     * the Kibana. Note: the memory reserved by Kibana is not available for Elasticsearch.
     */
    maxOldSpaceSize?: string;
}

export interface ElasticSearchElasticsearchUserConfigPrivateAccess {
    /**
     * Allow clients to connect to elasticsearch from the public 
     * internet for service nodes that are in a project VPC or another type of private network.
     */
    elasticsearch?: string;
    /**
     * Allow clients to connect to kibana from the public internet for 
     * service nodes that are in a project VPC or another type of private network.
     */
    kibana?: string;
    /**
     * Allow clients to connect to prometheus from the public 
     * internet for service nodes that are in a project VPC or another type of private network.
     */
    prometheus?: string;
}

export interface ElasticSearchElasticsearchUserConfigPrivatelinkAccess {
    /**
     * Allow clients to connect to elasticsearch from the public 
     * internet for service nodes that are in a project VPC or another type of private network.
     */
    elasticsearch?: string;
    /**
     * Allow clients to connect to kibana from the public internet for 
     * service nodes that are in a project VPC or another type of private network.
     */
    kibana?: string;
}

export interface ElasticSearchElasticsearchUserConfigPublicAccess {
    /**
     * Allow clients to connect to elasticsearch from the public 
     * internet for service nodes that are in a project VPC or another type of private network.
     */
    elasticsearch?: string;
    /**
     * Allow clients to connect to kibana from the public internet for 
     * service nodes that are in a project VPC or another type of private network.
     */
    kibana?: string;
    /**
     * Allow clients to connect to prometheus from the public 
     * internet for service nodes that are in a project VPC or another type of private network.
     */
    prometheus?: string;
}

export interface ElasticSearchServiceIntegration {
    integrationType: string;
    sourceServiceName: string;
}

export interface GetCassandaCassandra {
}

export interface GetCassandaCassandraUserConfig {
    /**
     * Cassandra specific server provided values.
     */
    cassandra?: outputs.GetCassandaCassandraUserConfigCassandra;
    cassandraVersion?: string;
    /**
     * allow incoming connections from CIDR address block, e.g. `10.20.0.0/16`
     */
    ipFilters?: string[];
    /**
     * sets the service into migration mode enabling the sstableloader 
     * utility to be used to upload Cassandra data files. Available only on service create.
     */
    migrateSstableloader?: string;
    /**
     * Allow access to selected service ports from private networks.
     */
    privateAccess?: outputs.GetCassandaCassandraUserConfigPrivateAccess;
    projectToForkFrom?: string;
    /**
     * Allow access to selected service ports from the public Internet
     */
    publicAccess?: outputs.GetCassandaCassandraUserConfigPublicAccess;
    /**
     * Name of another service to fork from. This has effect only 
     * when a new service is being created.
     */
    serviceToForkFrom?: string;
    staticIps?: string;
}

export interface GetCassandaCassandraUserConfigCassandra {
    /**
     * Fail any multiple-partition batch exceeding this value.
     * 50kb (10x warn threshold) by default.
     */
    batchSizeFailThresholdInKb?: string;
    /**
     * Log a warning message on any multiple-partition
     * batch size exceeding this value.5kb per batch by default.Caution should be taken on increasing
     * the size of this thresholdas it can lead to node instability.
     */
    batchSizeWarnThresholdInKb?: string;
}

export interface GetCassandaCassandraUserConfigPrivateAccess {
    /**
     * Allow clients to connect to prometheus from the public internet 
     * for service nodes that are in a project VPC or another type of private network.
     */
    prometheus?: string;
}

export interface GetCassandaCassandraUserConfigPublicAccess {
    /**
     * Allow clients to connect to prometheus from the public internet 
     * for service nodes that are in a project VPC or another type of private network.
     */
    prometheus?: string;
}

export interface GetCassandaComponent {
    component: string;
    host: string;
    kafkaAuthenticationMethod: string;
    port: number;
    route: string;
    ssl: boolean;
    usage: string;
}

export interface GetCassandaServiceIntegration {
    integrationType: string;
    sourceServiceName: string;
}

export interface GetElasticSearchAclAcl {
    rules: outputs.GetElasticSearchAclAclRule[];
    username: string;
}

export interface GetElasticSearchAclAclRule {
    index: string;
    permission: string;
}

export interface GetElasticSearchComponent {
    component: string;
    host: string;
    kafkaAuthenticationMethod: string;
    port: number;
    route: string;
    ssl: boolean;
    usage: string;
}

export interface GetElasticSearchElasticsearch {
    /**
     * URI for Kibana frontend.
     */
    kibanaUri: string;
}

export interface GetElasticSearchElasticsearchUserConfig {
    /**
     * Serve the web frontend using a custom CNAME pointing to the 
     * Aiven DNS name.
     */
    customDomain?: string;
    /**
     * Disable automatic replication factor 
     * adjustment for multi-node services. By default, Aiven ensures all indexes are replicated at
     * least to two nodes. Note: setting this to true increases a risk of data loss in case of
     * virtual machine failure.
     */
    disableReplicationFactorAdjustment?: string;
    /**
     * Elasticsearch specific server provided values.
     */
    elasticsearch?: outputs.GetElasticSearchElasticsearchUserConfigElasticsearch;
    /**
     * Elasticsearch major version.
     */
    elasticsearchVersion?: string;
    /**
     * Glob pattern and number of indexes matching that pattern to 
     * be kept.
     */
    indexPatterns?: outputs.GetElasticSearchElasticsearchUserConfigIndexPattern[];
    /**
     * Template settings for all new indexe.
     */
    indexTemplate?: outputs.GetElasticSearchElasticsearchUserConfigIndexTemplate;
    /**
     * allow incoming connections from CIDR address block, e.g. `10.20.0.0/16`
     */
    ipFilters?: string[];
    keepIndexRefreshInterval?: string;
    /**
     * Allow clients to connect to kibana from the public internet for 
     * service nodes that are in a project VPC or another type of private network.
     */
    kibana?: outputs.GetElasticSearchElasticsearchUserConfigKibana;
    /**
     * Maximum number of indexes to keep before deleting the oldest one.
     */
    maxIndexCount?: string;
    opensearchVersion?: string;
    /**
     * Allow access to selected service ports from private networks.
     */
    privateAccess?: outputs.GetElasticSearchElasticsearchUserConfigPrivateAccess;
    /**
     * Allow access to selected service components through Privatelink
     */
    privatelinkAccess?: outputs.GetElasticSearchElasticsearchUserConfigPrivatelinkAccess;
    /**
     * Name of another project to fork a service from. This has
     * effect only when a new service is being created.
     */
    projectToForkFrom?: string;
    /**
     * Allow access to selected service ports from the public Internet.
     */
    publicAccess?: outputs.GetElasticSearchElasticsearchUserConfigPublicAccess;
    /**
     * Name of the basebackup to restore in forked service.
     */
    recoveryBasebackupName?: string;
    /**
     * Name of another service to fork from. This has effect 
     * only when a new service is being created.
     */
    serviceToForkFrom?: string;
    staticIps?: string;
}

export interface GetElasticSearchElasticsearchUserConfigElasticsearch {
    /**
     * Explicitly allow or block automatic 
     * creation of indices. Defaults to true
     */
    actionAutoCreateIndexEnabled?: string;
    /**
     * Require explicit index names when deleting
     */
    actionDestructiveRequiresName?: string;
    /**
     * Controls the number of shards allowed in the
     * cluster per data node
     */
    clusterMaxShardsPerNode?: string;
    /**
     * Maximum content length for HTTP requests to 
     * the Elasticsearch HTTP API, in bytes.
     */
    httpMaxContentLength?: string;
    /**
     * The max size of allowed headers, in bytes.
     */
    httpMaxHeaderSize?: string;
    /**
     * The max length of an HTTP URL, in bytes.
     */
    httpMaxInitialLineLength?: string;
    /**
     * Relative amount. Maximum amount of 
     * heap memory used for field data cache. This is an expert setting; decreasing the
     * value too much will increase overhead of loading field data; too much memory used
     * for field data cache will decrease amount of heap available for other operations.
     */
    indicesFielddataCacheSize?: string;
    /**
     * Percentage value. Default is 10%. 
     * Total amount of heap used for indexing buffer, before writing segments to disk.
     * This is an expert setting. Too low value will slow down indexing; too high value
     * will increase indexing performance but causes performance issues for query performance.
     */
    indicesMemoryIndexBufferSize?: string;
    /**
     * Percentage value. Default is 10%. 
     * Maximum amount of heap used for query cache. This is an expert setting.
     * Too low value will decrease query performance and increase performance for other
     * operations; too high value will cause issues with other Elasticsearch functionality.
     */
    indicesQueriesCacheSize?: string;
    /**
     * Maximum number of clauses Lucene 
     * BooleanQuery can have. The default value (1024) is relatively high, and increasing it
     * may cause performance issues. Investigate other approaches first before increasing this value.
     */
    indicesQueryBoolMaxClauseCount?: string;
    /**
     * Whitelisted addresses for reindexing. 
     * Changing this value will cause all Elasticsearch instances to restart.
     */
    reindexRemoteWhitelists?: string[];
    /**
     * Maximum number of aggregation buckets allowed 
     * in a single response. Elasticsearch default value is used when this is not defined.
     */
    searchMaxBuckets?: string;
    /**
     * Size for the thread pool queue. 
     * See documentation for exact details.
     */
    threadPoolAnalyzeQueueSize?: string;
    /**
     * Size for the thread pool. See documentation 
     * for exact details. Do note this may have maximum value depending on CPU count -
     * value is automatically lowered if set to higher than maximum value.
     */
    threadPoolAnalyzeSize?: string;
    /**
     * Size for the thread pool. See 
     * documentation for exact details. Do note this may have maximum value depending on
     * CPU count - value is automatically lowered if set to higher than maximum value.
     */
    threadPoolForceMergeSize?: string;
    /**
     * Size for the thread pool queue. See 
     * documentation for exact details.
     */
    threadPoolGetQueueSize?: string;
    /**
     * Size for the thread pool. See documentation 
     * for exact details. Do note this may have maximum value depending on CPU count -
     * value is automatically lowered if set to higher than maximum value.
     */
    threadPoolGetSize?: string;
    /**
     * Size for the thread pool queue. 
     * See documentation for exact details.
     */
    threadPoolIndexQueueSize?: string;
    /**
     * Size for the thread pool. See documentation 
     * for exact details. Do note this may have maximum value depending on CPU count -
     * value is automatically lowered if set to higher than maximum value.
     */
    threadPoolIndexSize?: string;
    /**
     * Size for the thread pool queue. See 
     * documentation for exact details.
     */
    threadPoolSearchQueueSize?: string;
    /**
     * Size for the thread pool. See documentation 
     * for exact details. Do note this may have maximum value depending on CPU count - value
     * is automatically lowered if set to higher than maximum value.
     */
    threadPoolSearchSize?: string;
    /**
     * Size for the thread pool queue. 
     * See documentation for exact details.
     */
    threadPoolSearchThrottledQueueSize?: string;
    /**
     * Size for the thread pool. See 
     * documentation for exact details. Do note this may have maximum value depending on
     * CPU count - value is automatically lowered if set to higher than maximum value.
     */
    threadPoolSearchThrottledSize?: string;
    /**
     * Size for the thread pool queue. See 
     * documentation for exact details.
     */
    threadPoolWriteQueueSize?: string;
    /**
     * Size for the thread pool. See documentation 
     * for exact details. Do note this may have maximum value depending on CPU count - value
     * is automatically lowered if set to higher than maximum value.
     */
    threadPoolWriteSize?: string;
}

export interface GetElasticSearchElasticsearchUserConfigIndexPattern {
    /**
     * Maximum number of indexes to keep before deleting the oldest one.
     */
    maxIndexCount?: string;
    /**
     * Must consist of alpha-numeric characters, dashes, underscores, 
     * dots and glob characters (* and ?)
     */
    pattern?: string;
    /**
     * Deletion sorting algorithm
     */
    sortingAlgorithm?: string;
}

export interface GetElasticSearchElasticsearchUserConfigIndexTemplate {
    /**
     * The maximum number of nested JSON objects that
     * a single document can contain across all nested types. This limit helps to prevent out of
     * memory errors when a document contains too many nested objects. Default is 10000.
     */
    mappingNestedObjectsLimit?: string;
    /**
     * The number of replicas each primary shard has.
     */
    numberOfReplicas?: string;
    /**
     * The number of primary shards that an index should have.
     */
    numberOfShards?: string;
}

export interface GetElasticSearchElasticsearchUserConfigKibana {
    /**
     * Timeout in milliseconds for requests 
     * made by Kibana towards Elasticsearch.
     */
    elasticsearchRequestTimeout?: string;
    /**
     * Enable or disable Kibana.
     */
    enabled?: string;
    /**
     * Limits the maximum amount of memory (in MiB) the 
     * Kibana process can use. This sets the maxOldSpaceSize option of the nodejs running
     * the Kibana. Note: the memory reserved by Kibana is not available for Elasticsearch.
     */
    maxOldSpaceSize?: string;
}

export interface GetElasticSearchElasticsearchUserConfigPrivateAccess {
    /**
     * Elasticsearch specific server provided values.
     */
    elasticsearch?: string;
    /**
     * Allow clients to connect to kibana from the public internet for 
     * service nodes that are in a project VPC or another type of private network.
     */
    kibana?: string;
    /**
     * Allow clients to connect to prometheus from the public 
     * internet for service nodes that are in a project VPC or another type of private network.
     */
    prometheus?: string;
}

export interface GetElasticSearchElasticsearchUserConfigPrivatelinkAccess {
    /**
     * Elasticsearch specific server provided values.
     */
    elasticsearch?: string;
    /**
     * Allow clients to connect to kibana from the public internet for 
     * service nodes that are in a project VPC or another type of private network.
     */
    kibana?: string;
}

export interface GetElasticSearchElasticsearchUserConfigPublicAccess {
    /**
     * Elasticsearch specific server provided values.
     */
    elasticsearch?: string;
    /**
     * Allow clients to connect to kibana from the public internet for 
     * service nodes that are in a project VPC or another type of private network.
     */
    kibana?: string;
    /**
     * Allow clients to connect to prometheus from the public 
     * internet for service nodes that are in a project VPC or another type of private network.
     */
    prometheus?: string;
}

export interface GetElasticSearchServiceIntegration {
    integrationType: string;
    sourceServiceName: string;
}

export interface GetGrafanaComponent {
    component: string;
    /**
     * Server hostname or IP
     */
    host: string;
    kafkaAuthenticationMethod: string;
    /**
     * SMTP server port
     */
    port: number;
    route: string;
    ssl: boolean;
    usage: string;
}

export interface GetGrafanaGrafana {
}

export interface GetGrafanaGrafanaUserConfig {
    /**
     * Enable or disable Grafana alerting functionality
     */
    alertingEnabled?: string;
    /**
     * Default error or timeout setting for new alerting rules
     */
    alertingErrorOrTimeout?: string;
    alertingMaxAnnotationsToKeep?: string;
    /**
     * Default value for 'no data or null values' for
     * new alerting rules
     */
    alertingNodataOrNullvalues?: string;
    /**
     * Allow embedding Grafana dashboards with iframe/frame/object/embed 
     * tags. Disabled by default to limit impact of clickjacking
     */
    allowEmbedding?: string;
    authAzuread?: outputs.GetGrafanaGrafanaUserConfigAuthAzuread;
    /**
     * Enable or disable basic authentication form, used by Grafana 
     * built-in login.
     */
    authBasicEnabled?: string;
    /**
     * Generic OAuth integration.
     */
    authGenericOauth?: outputs.GetGrafanaGrafanaUserConfigAuthGenericOauth;
    /**
     * Automatically sign-up users on successful sign-in
     */
    authGithub?: outputs.GetGrafanaGrafanaUserConfigAuthGithub;
    /**
     * GitLab Auth integration.
     */
    authGitlab?: outputs.GetGrafanaGrafanaUserConfigAuthGitlab;
    /**
     * Google Auth integration
     */
    authGoogle?: outputs.GetGrafanaGrafanaUserConfigAuthGoogle;
    /**
     * Cookie SameSite attribute: 'strict' prevents sending cookie for 
     * cross-site requests, effectively disabling direct linking from other sites to Grafana. 'lax' is the default value.
     */
    cookieSamesite?: string;
    /**
     * Serve the web frontend using a custom CNAME pointing to the Aiven DNS name.
     */
    customDomain?: string;
    /**
     * Signed sequence of decimal numbers, followed
     * by a unit suffix (ms, s, m, h, d), e.g. 30s, 1h.
     */
    dashboardsMinRefreshInterval?: string;
    /**
     * Dashboard versions to keep per dashboard.
     */
    dashboardsVersionsToKeep?: string;
    /**
     * Send 'X-Grafana-User' header to data source.
     */
    dataproxySendUserHeader?: string;
    /**
     * Timeout for data proxy requests in seconds.
     */
    dataproxyTimeout?: string;
    dateFormats?: outputs.GetGrafanaGrafanaUserConfigDateFormats;
    /**
     * Set to true to disable gravatar. Defaults to false 
     * (gravatar is enabled).
     */
    disableGravatar?: string;
    /**
     * Editors can manage folders, teams and dashboards created by them.
     */
    editorsCanAdmin?: string;
    /**
     * External image store settings
     */
    externalImageStorage?: outputs.GetGrafanaGrafanaUserConfigExternalImageStorage;
    /**
     * Google Analytics Universal Analytics ID for tracking Grafana usage
     */
    googleAnalyticsUaId?: string;
    /**
     * Allow incoming connections from CIDR address block, e.g. '10.20.0.0/16'
     */
    ipFilters?: string[];
    /**
     * Enable Grafana /metrics endpoint
     */
    metricsEnabled?: string;
    privateAccess?: outputs.GetGrafanaGrafanaUserConfigPrivateAccess;
    /**
     * Allow access to selected service components through Privatelink
     */
    privatelinkAccess?: outputs.GetGrafanaGrafanaUserConfigPrivatelinkAccess;
    /**
     * Name of another project to fork a service from. This has 
     * effect only when a new service is being created.
     */
    projectToForkFrom?: string;
    /**
     * Allow access to selected service ports from the public Internet.
     */
    publicAccess?: outputs.GetGrafanaGrafanaUserConfigPublicAccess;
    /**
     * Name of the basebackup to restore in forked service.
     */
    recoveryBasebackupName?: string;
    /**
     * Name of another service to fork from. This has effect only 
     * when a new service is being created.
     */
    serviceToForkFrom?: string;
    /**
     * SMTP server settings.
     */
    smtpServer?: outputs.GetGrafanaGrafanaUserConfigSmtpServer;
    staticIps?: string;
    /**
     * Auto-assign new users on signup to main organization. 
     * Defaults to false.
     */
    userAutoAssignOrg?: string;
    /**
     * Set role for new signups. Defaults to Viewer.
     */
    userAutoAssignOrgRole?: string;
    /**
     * Users with view-only permission can edit but not save dashboards.
     */
    viewersCanEdit?: string;
}

export interface GetGrafanaGrafanaUserConfigAuthAzuread {
    /**
     * Automatically sign-up users on successful sign-in
     */
    allowSignUp?: string;
    /**
     * Allowed domain
     */
    allowedDomains?: string[];
    /**
     * Require users to belong to one of given groups
     */
    allowedGroups?: string[];
    /**
     * Authorization URL. This only needs to be set when using self hosted GitLab
     */
    authUrl?: string;
    /**
     * Client ID from provider
     */
    clientId?: string;
    /**
     * Client secret from provider
     */
    clientSecret?: string;
    /**
     * Token URL. This only needs to be set when using self hosted GitLab
     */
    tokenUrl?: string;
}

export interface GetGrafanaGrafanaUserConfigAuthGenericOauth {
    /**
     * Automatically sign-up users on successful sign-in
     */
    allowSignUp?: string;
    /**
     * Allowed domain
     */
    allowedDomains?: string[];
    /**
     * Must consist of alpha-numeric characters and dashes"
     */
    allowedOrganizations?: string[];
    /**
     * API URL. This only needs to be set when using self hosted GitLab
     */
    apiUrl?: string;
    /**
     * Authorization URL. This only needs to be set when using self hosted GitLab
     */
    authUrl?: string;
    /**
     * Client ID from provider
     */
    clientId?: string;
    /**
     * Client secret from provider
     */
    clientSecret?: string;
    /**
     * Name of the OAuth integration
     */
    name?: string;
    /**
     * Scope must be non-empty string without whitespace
     */
    scopes?: string[];
    /**
     * Token URL. This only needs to be set when using self hosted GitLab
     */
    tokenUrl?: string;
}

export interface GetGrafanaGrafanaUserConfigAuthGithub {
    /**
     * Automatically sign-up users on successful sign-in
     */
    allowSignUp?: string;
    /**
     * Must consist of alpha-numeric characters and dashes"
     */
    allowedOrganizations?: string[];
    /**
     * Client ID from provider
     */
    clientId?: string;
    /**
     * Client secret from provider
     */
    clientSecret?: string;
    /**
     * Require users to belong to one of given team IDs
     */
    teamIds?: string[];
}

export interface GetGrafanaGrafanaUserConfigAuthGitlab {
    /**
     * Automatically sign-up users on successful sign-in
     */
    allowSignUp?: string;
    /**
     * Require users to belong to one of given groups
     */
    allowedGroups?: string[];
    /**
     * API URL. This only needs to be set when using self hosted GitLab
     */
    apiUrl?: string;
    /**
     * Authorization URL. This only needs to be set when using self hosted GitLab
     */
    authUrl?: string;
    /**
     * Client ID from provider
     */
    clientId?: string;
    /**
     * Client secret from provider
     */
    clientSecret?: string;
    /**
     * Token URL. This only needs to be set when using self hosted GitLab
     */
    tokenUrl?: string;
}

export interface GetGrafanaGrafanaUserConfigAuthGoogle {
    /**
     * Automatically sign-up users on successful sign-in
     */
    allowSignUp?: string;
    /**
     * Allowed domain
     */
    allowedDomains?: string[];
    /**
     * Client ID from provider
     */
    clientId?: string;
    /**
     * Client secret from provider
     */
    clientSecret?: string;
}

export interface GetGrafanaGrafanaUserConfigDateFormats {
    defaultTimezone?: string;
    fullDate?: string;
    intervalDay?: string;
    intervalHour?: string;
    intervalMinute?: string;
    intervalMonth?: string;
    intervalSecond?: string;
    intervalYear?: string;
}

export interface GetGrafanaGrafanaUserConfigExternalImageStorage {
    /**
     * S3 access key. Requires permissions to the S3 bucket for the 
     * s3:PutObject and s3:PutObjectAcl actions
     */
    accessKey?: string;
    /**
     * Bucket URL for S3
     */
    bucketUrl?: string;
    /**
     * Provider type
     */
    provider?: string;
    /**
     * S3 secret key
     */
    secretKey?: string;
}

export interface GetGrafanaGrafanaUserConfigPrivateAccess {
    /**
     * Grafana specific server provided values.
     */
    grafana?: string;
}

export interface GetGrafanaGrafanaUserConfigPrivatelinkAccess {
    /**
     * Grafana specific server provided values.
     */
    grafana?: string;
}

export interface GetGrafanaGrafanaUserConfigPublicAccess {
    /**
     * Grafana specific server provided values.
     */
    grafana?: string;
}

export interface GetGrafanaGrafanaUserConfigSmtpServer {
    /**
     * Address used for sending emails
     */
    fromAddress?: string;
    /**
     * Name used in outgoing emails, defaults to Grafana
     */
    fromName?: string;
    /**
     * Server hostname or IP
     */
    host?: string;
    /**
     * Password for SMTP authentication
     */
    password?: string;
    /**
     * SMTP server port
     */
    port?: string;
    /**
     * Skip verifying server certificate. Defaults to false
     */
    skipVerify?: string;
    /**
     * Either OpportunisticStartTLS, MandatoryStartTLS or NoStartTLS. 
     * Default is OpportunisticStartTLS.
     */
    starttlsPolicy?: string;
    /**
     * Username for SMTP authentication
     */
    username?: string;
}

export interface GetGrafanaServiceIntegration {
    integrationType: string;
    sourceServiceName: string;
}

export interface GetInfluxDbComponent {
    component: string;
    host: string;
    kafkaAuthenticationMethod: string;
    port: number;
    route: string;
    ssl: boolean;
    usage: string;
}

export interface GetInfluxDbInfluxdb {
    databaseName: string;
}

export interface GetInfluxDbInfluxdbUserConfig {
    /**
     * Serve the web frontend using a custom CNAME pointing to the Aiven DNS name
     */
    customDomain?: string;
    /**
     * InfluxDB specific server provided values.
     */
    influxdb?: outputs.GetInfluxDbInfluxdbUserConfigInfluxdb;
    /**
     * allow incoming connections from CIDR address block, e.g. `10.20.0.0/16`
     */
    ipFilters?: string[];
    /**
     * Allow access to selected service ports from private networks
     */
    privateAccess?: outputs.GetInfluxDbInfluxdbUserConfigPrivateAccess;
    /**
     * Allow access to selected service components through Privatelink
     */
    privatelinkAccess?: outputs.GetInfluxDbInfluxdbUserConfigPrivatelinkAccess;
    /**
     * Name of another project to fork a service from. This has
     * effect only when a new service is being created.
     */
    projectToForkFrom?: string;
    /**
     * Allow access to selected service ports from the public Internet
     */
    publicAccess?: outputs.GetInfluxDbInfluxdbUserConfigPublicAccess;
    /**
     * Name of the basebackup to restore in forked service
     */
    recoveryBasebackupName?: string;
    /**
     * Name of another service to fork from. This has effect 
     * only when a new service is being created.
     */
    serviceToForkFrom?: string;
    staticIps?: string;
}

export interface GetInfluxDbInfluxdbUserConfigInfluxdb {
    /**
     * The maximum duration in seconds before a query is 
     * logged as a slow query. Setting this to 0 (the default) will never log slow queries.
     */
    logQueriesAfter?: string;
    maxConnectionLimit?: string;
    /**
     * The maximum number of rows returned in a non-chunked query. 
     * Setting this to 0 (the default) allows an unlimited number to be returned.
     */
    maxRowLimit?: string;
    /**
     * The maximum number of `GROUP BY time()` buckets that 
     * can be processed in a query. Setting this to 0 (the default) allows an unlimited number to
     * be processed.
     */
    maxSelectBuckets?: string;
    /**
     * The maximum number of points that can be processed in a 
     * SELECT statement. Setting this to 0 (the default) allows an unlimited number to be processed.
     */
    maxSelectPoint?: string;
    /**
     * The maximum duration in seconds before a query is killed. 
     * Setting this to 0 (the default) will never kill slow queries.
     */
    queryTimeout?: string;
}

export interface GetInfluxDbInfluxdbUserConfigPrivateAccess {
    /**
     * InfluxDB specific server provided values.
     */
    influxdb?: string;
}

export interface GetInfluxDbInfluxdbUserConfigPrivatelinkAccess {
    /**
     * InfluxDB specific server provided values.
     */
    influxdb?: string;
}

export interface GetInfluxDbInfluxdbUserConfigPublicAccess {
    /**
     * InfluxDB specific server provided values.
     */
    influxdb?: string;
}

export interface GetInfluxDbServiceIntegration {
    integrationType: string;
    sourceServiceName: string;
}

export interface GetKafkaComponent {
    component: string;
    host: string;
    kafkaAuthenticationMethod: string;
    port: number;
    route: string;
    ssl: boolean;
    usage: string;
}

export interface GetKafkaConnectComponent {
    component: string;
    host: string;
    kafkaAuthenticationMethod: string;
    port: number;
    route: string;
    ssl: boolean;
    usage: string;
}

export interface GetKafkaConnectKafkaConnect {
}

export interface GetKafkaConnectKafkaConnectUserConfig {
    /**
     * allow incoming connections from CIDR address block, e.g. `10.20.0.0/16`
     */
    ipFilters?: string[];
    /**
     * Kafka Connect specific server provided values.
     */
    kafkaConnect?: outputs.GetKafkaConnectKafkaConnectUserConfigKafkaConnect;
    /**
     * Allow access to selected service ports from private networks.
     */
    privateAccess?: outputs.GetKafkaConnectKafkaConnectUserConfigPrivateAccess;
    /**
     * Allow access to selected service components through Privatelink
     */
    privatelinkAccess?: outputs.GetKafkaConnectKafkaConnectUserConfigPrivatelinkAccess;
    publicAccess?: outputs.GetKafkaConnectKafkaConnectUserConfigPublicAccess;
    staticIps?: string;
}

export interface GetKafkaConnectKafkaConnectUserConfigKafkaConnect {
    /**
     * Defines what client configurations can be 
     * overridden by the connector. Default is None.
     */
    connectorClientConfigOverridePolicy?: string;
    /**
     * What to do when there is no initial offset in Kafka or 
     * if the current offset does not exist any more on the server. Default is earliest.
     */
    consumerAutoOffsetReset?: string;
    /**
     * Records are fetched in batches by the consumer, and if 
     * the first record batch in the first non-empty partition of the fetch is larger than this value,
     * the record batch will still be returned to ensure that the consumer can make progress. As such,
     * this is not a absolute maximum.
     */
    consumerFetchMaxBytes?: string;
    /**
     * Transaction read isolation level. readUncommitted is 
     * the default, but readCommitted can be used if consume-exactly-once behavior is desired.
     */
    consumerIsolationLevel?: string;
    /**
     * Records are fetched in batches by the consumer.If 
     * the first record batch in the first non-empty partition of the fetch is larger than this limit,
     * the batch will still be returned to ensure that the consumer can make progress.
     */
    consumerMaxPartitionFetchBytes?: string;
    /**
     * The maximum delay in milliseconds between invocations 
     * of poll() when using consumer group management (defaults to 300000).
     * * `consumerMaxPollRecords` The maximum number of records returned by a single poll.
     */
    consumerMaxPollIntervalMs?: string;
    consumerMaxPollRecords?: string;
    /**
     * The interval at which to try committing offsets for tasks 
     * (defaults to 60000).
     */
    offsetFlushIntervalMs?: string;
    /**
     * Maximum number of milliseconds to wait for records to flush 
     * and partition offset data to be committed to offset storage before cancelling the process and restoring
     * the offset data to be committed in a future attempt (defaults to 5000).
     */
    offsetFlushTimeoutMs?: string;
    /**
     * This setting will limit the number of record batches the 
     * producer will send in a single request to avoid sending huge requests.
     */
    producerMaxRequestSize?: string;
    /**
     * The timeout in milliseconds used to detect failures when using Kafkaâ€™s 
     * group management facilities (defaults to 10000).
     */
    sessionTimeoutMs?: string;
}

export interface GetKafkaConnectKafkaConnectUserConfigPrivateAccess {
    /**
     * Kafka Connect specific server provided values.
     */
    kafkaConnect?: string;
    /**
     * Allow clients to connect to prometheus with a DNS name that always resolves to 
     * the service's private IP addresses. Only available in certain network locations.
     */
    prometheus?: string;
}

export interface GetKafkaConnectKafkaConnectUserConfigPrivatelinkAccess {
    /**
     * Kafka Connect specific server provided values.
     */
    kafkaConnect?: string;
}

export interface GetKafkaConnectKafkaConnectUserConfigPublicAccess {
    /**
     * Kafka Connect specific server provided values.
     */
    kafkaConnect?: string;
    /**
     * Allow clients to connect to prometheus with a DNS name that always resolves to 
     * the service's private IP addresses. Only available in certain network locations.
     */
    prometheus?: string;
}

export interface GetKafkaConnectServiceIntegration {
    integrationType: string;
    sourceServiceName: string;
}

export interface GetKafkaConnectorTask {
    connector: string;
    /**
     * List of tasks of a connector, each element contains `connector` 
     * (Related connector name) and `task` (Task id / number).
     */
    task: number;
}

export interface GetKafkaKafka {
    /**
     * The Kafka client certificate
     */
    accessCert: string;
    /**
     * The Kafka client certificate key
     */
    accessKey: string;
    /**
     * The Kafka Connect URI, if any
     */
    connectUri: string;
    /**
     * The Kafka REST URI, if any
     */
    restUri: string;
    /**
     * The Schema Registry URI, if any
     */
    schemaRegistryUri: string;
}

export interface GetKafkaKafkaUserConfig {
    /**
     * Serve the web frontend using a custom CNAME pointing to the Aiven DNS name.
     */
    customDomain?: string;
    /**
     * Allow incoming connections from CIDR address block, e.g. '10.20.0.0/16'.
     */
    ipFilters?: string[];
    /**
     * Kafka server provided values:
     */
    kafka?: outputs.GetKafkaKafkaUserConfigKafka;
    /**
     * Kafka authentication methods
     */
    kafkaAuthenticationMethods?: outputs.GetKafkaKafkaUserConfigKafkaAuthenticationMethods;
    /**
     * Enable kafka_connect
     */
    kafkaConnect?: string;
    /**
     * Kafka Connect configuration values
     */
    kafkaConnectConfig?: outputs.GetKafkaKafkaUserConfigKafkaConnectConfig;
    /**
     * Enable kafka_rest
     */
    kafkaRest?: string;
    /**
     * Kafka-REST configuration
     */
    kafkaRestConfig?: outputs.GetKafkaKafkaUserConfigKafkaRestConfig;
    /**
     * Kafka major version
     */
    kafkaVersion?: string;
    /**
     * Allow access to selected service ports from private networks
     */
    privateAccess?: outputs.GetKafkaKafkaUserConfigPrivateAccess;
    /**
     * Allow access to selected service components through Privatelink
     */
    privatelinkAccess?: outputs.GetKafkaKafkaUserConfigPrivatelinkAccess;
    /**
     * Allow access to selected service ports from the public Internet
     */
    publicAccess?: outputs.GetKafkaKafkaUserConfigPublicAccess;
    /**
     * Enable schema_registry
     */
    schemaRegistry?: string;
    /**
     * Schema Registry configuration
     */
    schemaRegistryConfig?: outputs.GetKafkaKafkaUserConfigSchemaRegistryConfig;
    staticIps?: string;
}

export interface GetKafkaKafkaUserConfigKafka {
    /**
     * Enable auto creation of topics
     */
    autoCreateTopicsEnable?: string;
    /**
     * Specify the final compression type for a given topic. This 
     * configuration accepts the standard compression codecs ('gzip', 'snappy', 'lz4', 'zstd').
     * It additionally accepts 'uncompressed' which is equivalent to no compression; and 'producer'
     * which means retain the original compression codec set by the producer.
     */
    compressionType?: string;
    /**
     * Idle connections timeout: the server socket processor 
     * threads close the connections that idle for longer than this.
     */
    connectionsMaxIdleMs?: string;
    /**
     * Replication factor for autocreated topics
     */
    defaultReplicationFactor?: string;
    /**
     * The amount of time, in milliseconds, the group
     * coordinator will wait for more consumers to join a new group before performing the first rebalance.
     * A longer delay means potentially fewer rebalances, but increases the time until processing begins.
     * The default value for this is 3 seconds. During development and testing it might be desirable to set
     * this to 0 in order to not delay test execution time.
     */
    groupInitialRebalanceDelayMs?: string;
    /**
     * The maximum allowed session timeout for registered 
     * consumers. Longer timeouts give consumers more time to process messages in between heartbeats
     * at the cost of a longer time to detect failures.
     */
    groupMaxSessionTimeoutMs?: string;
    /**
     * The minimum allowed session timeout for registered 
     * consumers. Longer timeouts give consumers more time to process messages in between heartbeats
     * at the cost of a longer time to detect failures.
     */
    groupMinSessionTimeoutMs?: string;
    logCleanerDeleteRetentionMs?: string;
    /**
     * The maximum amount of time message will 
     * remain uncompacted. Only applicable for logs that are being compacted
     */
    logCleanerMaxCompactionLagMs?: string;
    /**
     * Controls log compactor frequency. Larger 
     * value means more frequent compactions but also more space wasted for logs. Consider setting
     * log.cleaner.max.compaction.lag.ms to enforce compactions sooner, instead of setting a very
     * high value for this option.
     */
    logCleanerMinCleanableRatio?: string;
    /**
     * The minimum time a message will remain 
     * uncompacted in the log. Only applicable for logs that are being compacted.
     */
    logCleanerMinCompactionLagMs?: string;
    /**
     * The default cleanup policy for segments beyond the retention window.
     */
    logCleanupPolicy?: string;
    /**
     * The number of messages accumulated on a log partition 
     * before messages are flushed to disk.
     */
    logFlushIntervalMessages?: string;
    /**
     * The maximum time in ms that a message in any topic is kept 
     * in memory before flushed to disk. If not set, the value in log.flush.scheduler.interval.ms is used.
     */
    logFlushIntervalMs?: string;
    /**
     * The interval with which Kafka adds an entry to the offset index.
     */
    logIndexIntervalBytes?: string;
    /**
     * The maximum size in bytes of the offset index.
     */
    logIndexSizeMaxBytes?: string;
    /**
     * This configuration controls whether down-conversion 
     * of message formats is enabled to satisfy consume requests.
     */
    logMessageDownconversionEnable?: string;
    /**
     * The maximum difference allowed between 
     * the timestamp when a broker receives a message and the timestamp specified in the message
     */
    logMessageTimestampDifferenceMaxMs?: string;
    /**
     * Define whether the timestamp in the message is 
     * message create time or log append time.
     */
    logMessageTimestampType?: string;
    /**
     * Should pre allocate file when create new segment?
     */
    logPreallocate?: string;
    /**
     * The maximum size of the log before deleting messages
     */
    logRetentionBytes?: string;
    /**
     * The number of hours to keep a log file before deleting it.
     */
    logRetentionHours?: string;
    /**
     * The number of milliseconds to keep a log file before deleting it 
     * (in milliseconds), If not set, the value in log.retention.minutes is used. If set to -1, no
     * time limit is applied.
     */
    logRetentionMs?: string;
    /**
     * The maximum jitter to subtract from logRollTimeMillis 
     * (in milliseconds). If not set, the value in log.roll.jitter.hours is used.
     */
    logRollJitterMs?: string;
    /**
     * The maximum time before a new log segment is rolled out (in milliseconds).
     */
    logRollMs?: string;
    /**
     * The maximum size of a single log file
     */
    logSegmentBytes?: string;
    /**
     * The amount of time to wait before deleting a file 
     * from the filesystem.
     */
    logSegmentDeleteDelayMs?: string;
    /**
     * The maximum number of connections allowed from each ip 
     * address (defaults to 2147483647).
     */
    maxConnectionsPerIp?: string;
    /**
     * The maximum number of incremental fetch 
     * sessions that the broker will maintain.
     */
    maxIncrementalFetchSessionCacheSlots?: string;
    /**
     * The maximum size of message that the server can receive.
     */
    messageMaxBytes?: string;
    /**
     * When a producer sets acks to 'all' (or '-1'), 
     * min.insync.replicas specifies the minimum number of replicas that must acknowledge a write for
     * the write to be considered successful.
     */
    minInsyncReplicas?: string;
    /**
     * Number of partitions for autocreated topics
     */
    numPartitions?: string;
    /**
     * Log retention window in minutes for offsets topic.
     */
    offsetsRetentionMinutes?: string;
    /**
     * The purge interval (in number of 
     * requests) of the producer request purgatory(defaults to 1000).
     */
    producerPurgatoryPurgeIntervalRequests?: string;
    /**
     * The number of bytes of messages to attempt to fetch 
     * for each partition (defaults to 1048576). This is not an absolute maximum, if the first record
     * batch in the first non-empty partition of the fetch is larger than this value, the record batch
     * will still be returned to ensure that progress can be made.
     */
    replicaFetchMaxBytes?: string;
    /**
     * Maximum bytes expected for the entire fetch 
     * response (defaults to 10485760). Records are fetched in batches, and if the first record batch
     * in the first non-empty partition of the fetch is larger than this value, the record batch will
     * still be returned to ensure that progress can be made. As such, this is not an absolute maximum.
     */
    replicaFetchResponseMaxBytes?: string;
    /**
     * The maximum number of bytes in a socket request 
     * (defaults to 104857600).
     */
    socketRequestMaxBytes?: string;
    /**
     * The interval at which 
     * to remove transactions that have expired due to transactional.id.expiration.ms passing (defaults
     * to 3600000 (1 hour)).
     */
    transactionRemoveExpiredTransactionCleanupIntervalMs?: string;
    /**
     * The transaction topic segment bytes should 
     * be kept relatively small in order to facilitate faster log compaction and cache loads (defaults
     * to 104857600 (100 mebibytes)).
     */
    transactionStateLogSegmentBytes?: string;
}

export interface GetKafkaKafkaUserConfigKafkaAuthenticationMethods {
    /**
     * Enable certificate/SSL authentication
     */
    certificate?: string;
    /**
     * Enable SASL authentication
     */
    sasl?: string;
}

export interface GetKafkaKafkaUserConfigKafkaConnectConfig {
    /**
     * Defines what client configurations can 
     * be overridden by the connector. Default is None
     */
    connectorClientConfigOverridePolicy?: string;
    /**
     * What to do when there is no initial offset in Kafka or 
     * if the current offset does not exist any more on the server. Default is earliest.
     */
    consumerAutoOffsetReset?: string;
    /**
     * Records are fetched in batches by the consumer, and 
     * if the first record batch in the first non-empty partition of the fetch is larger than this value,
     * the record batch will still be returned to ensure that the consumer can make progress. As such,
     * this is not a absolute maximum.
     */
    consumerFetchMaxBytes?: string;
    /**
     * Transaction read isolation level. readUncommitted is 
     * the default, but readCommitted can be used if consume-exactly-once behavior is desired.
     */
    consumerIsolationLevel?: string;
    /**
     * Records are fetched in batches by the consumer.If 
     * the first record batch in the first non-empty partition of the fetch is larger than this limit,
     * the batch will still be returned to ensure that the consumer can make progress.
     */
    consumerMaxPartitionFetchBytes?: string;
    /**
     * The maximum delay in milliseconds between invocations 
     * of poll() when using consumer group management (defaults to 300000).
     */
    consumerMaxPollIntervalMs?: string;
    /**
     * The maximum number of records returned in a single call 
     * to poll() (defaults to 500).
     */
    consumerMaxPollRecords?: string;
    /**
     * The interval at which to try committing offsets for 
     * tasks (defaults to 60000).
     */
    offsetFlushIntervalMs?: string;
    /**
     * Maximum number of milliseconds to wait for records to 
     * flush and partition offset data to be committed to offset storage before cancelling the process
     * and restoring the offset data to be committed in a future attempt (defaults to 5000).
     */
    offsetFlushTimeoutMs?: string;
    /**
     * This setting will limit the number of record batches 
     * the producer will send in a single request to avoid sending huge requests.
     */
    producerMaxRequestSize?: string;
    /**
     * The timeout in milliseconds used to detect failures when 
     * using Kafkaâ€™s group management facilities (defaults to 10000).
     */
    sessionTimeoutMs?: string;
}

export interface GetKafkaKafkaUserConfigKafkaRestConfig {
    /**
     * If true the consumer's offset will be periodically 
     * committed to Kafka in the background
     */
    consumerEnableAutoCommit?: string;
    /**
     * Maximum number of bytes in unencoded message keys and 
     * values by a single request
     */
    consumerRequestMaxBytes?: string;
    /**
     * The maximum total time to wait for messages for a 
     * request if the maximum number of messages has not yet been reached
     */
    consumerRequestTimeoutMs?: string;
    /**
     * The number of acknowledgments the producer requires the leader to 
     * have received before considering a request complete. If set to 'all' or '-1', the leader will wait
     * for the full set of in-sync replicas to acknowledge the record.
     */
    producerAcks?: string;
    /**
     * Wait for up to the given delay to allow batching records together
     */
    producerLingerMs?: string;
    /**
     * Maximum number of SimpleConsumers that can be 
     * instantiated per broker.
     */
    simpleconsumerPoolSizeMax?: string;
}

export interface GetKafkaKafkaUserConfigPrivateAccess {
    /**
     * Allow clients to connect to prometheus from the public internet for 
     * service nodes that are in a project VPC or another type of private network
     */
    prometheus?: string;
}

export interface GetKafkaKafkaUserConfigPrivatelinkAccess {
    /**
     * Kafka server provided values:
     */
    kafka?: string;
    /**
     * Enable kafka_connect
     */
    kafkaConnect?: string;
    /**
     * Enable kafka_rest
     */
    kafkaRest?: string;
    /**
     * Enable schema_registry
     */
    schemaRegistry?: string;
}

export interface GetKafkaKafkaUserConfigPublicAccess {
    /**
     * Kafka server provided values:
     */
    kafka?: string;
    /**
     * Enable kafka_connect
     */
    kafkaConnect?: string;
    /**
     * Enable kafka_rest
     */
    kafkaRest?: string;
    /**
     * Allow clients to connect to prometheus from the public internet for 
     * service nodes that are in a project VPC or another type of private network
     */
    prometheus?: string;
    /**
     * Enable schema_registry
     */
    schemaRegistry?: string;
}

export interface GetKafkaKafkaUserConfigSchemaRegistryConfig {
    /**
     * If true, Karapace / Schema Registry on the service nodes can 
     * participate in leader election. It might be needed to disable this when the schemas topic is replicated
     * to a secondary cluster and Karapace / Schema Registry there must not participate in leader election.
     * Defaults to 'true'.
     */
    leaderEligibility?: string;
    /**
     * The durable single partition topic that acts as the durable log for the 
     * data. This topic must be compacted to avoid losing data due to retention policy. Please note that
     * changing this configuration in an existing Schema Registry / Karapace setup leads to previous
     * schemas being inaccessible, data encoded with them potentially unreadable and schema ID sequence
     * put out of order. It's only possible to do the switch while Schema Registry / Karapace is disabled.
     * Defaults to '_schemas'.
     */
    topicName?: string;
}

export interface GetKafkaMirrorMakerComponent {
    component: string;
    host: string;
    kafkaAuthenticationMethod: string;
    port: number;
    route: string;
    ssl: boolean;
    usage: string;
}

export interface GetKafkaMirrorMakerKafkaMirrormaker {
}

export interface GetKafkaMirrorMakerKafkaMirrormakerUserConfig {
    /**
     * allow incoming connections from CIDR address block, e.g. `10.20.0.0/16`
     */
    ipFilters?: string[];
    /**
     * Kafka MirrorMaker 2 specific server provided values.
     */
    kafkaMirrormaker?: outputs.GetKafkaMirrorMakerKafkaMirrormakerUserConfigKafkaMirrormaker;
    staticIps?: string;
}

export interface GetKafkaMirrorMakerKafkaMirrormakerUserConfigKafkaMirrormaker {
    /**
     * Whether to periodically write the translated offsets
     * of replicated consumer groups (in the source cluster) to __consumer_offsets topic in target cluster,
     * as long as no active consumers in that group are connected to the target cluster.
     */
    emitCheckpointsEnabled?: string;
    emitCheckpointsIntervalSeconds?: string;
    /**
     * Whether to periodically check for new consumer groups. 
     * Defaults to 'true'.
     */
    refreshGroupsEnabled?: string;
    /**
     * Whether to periodically check for new topics and 
     * partitions. Defaults to 'true'.
     */
    refreshGroupsIntervalSeconds?: string;
    refreshTopicsEnabled?: string;
    /**
     * Frequency of topic and partitions refresh in 
     * seconds. Defaults to 600 seconds (10 minutes).
     */
    refreshTopicsIntervalSeconds?: string;
    syncGroupOffsetsEnabled?: string;
    /**
     * Frequency at which consumer group offsets
     * are synced (default: 60, every minute).
     */
    syncGroupOffsetsIntervalSeconds?: string;
    syncTopicConfigsEnabled?: string;
    tasksMaxPerCpu?: string;
}

export interface GetKafkaMirrorMakerServiceIntegration {
    integrationType: string;
    sourceServiceName: string;
}

export interface GetKafkaServiceIntegration {
    integrationType: string;
    sourceServiceName: string;
}

export interface GetKafkaTopicConfig {
    /**
     * cleanup.policy value, can be `create`, `delete` or `compact,delete`
     */
    cleanupPolicy?: string;
    /**
     * compression.type value
     */
    compressionType?: string;
    /**
     * delete.retention.ms value
     */
    deleteRetentionMs?: string;
    /**
     * file.delete.delay.ms value
     */
    fileDeleteDelayMs?: string;
    /**
     * flush.messages value
     */
    flushMessages?: string;
    /**
     * flush.ms value
     */
    flushMs?: string;
    /**
     * index.interval.bytes value
     */
    indexIntervalBytes?: string;
    /**
     * max.compaction.lag.ms value
     */
    maxCompactionLagMs?: string;
    /**
     * max.message.bytes value
     */
    maxMessageBytes?: string;
    /**
     * message.downconversion.enable value
     */
    messageDownconversionEnable?: string;
    /**
     * message.format.version value
     */
    messageFormatVersion?: string;
    /**
     * message.timestamp.difference.max.ms value
     */
    messageTimestampDifferenceMaxMs?: string;
    /**
     * message.timestamp.type value
     */
    messageTimestampType?: string;
    /**
     * min.cleanable.dirty.ratio value
     */
    minCleanableDirtyRatio?: string;
    /**
     * min.compaction.lag.ms value
     */
    minCompactionLagMs?: string;
    /**
     * min.insync.replicas value
     */
    minInsyncReplicas?: string;
    /**
     * preallocate value
     */
    preallocate?: string;
    /**
     * retention.bytes value
     */
    retentionBytes?: string;
    /**
     * retention.ms value
     */
    retentionMs?: string;
    /**
     * segment.bytes value
     */
    segmentBytes?: string;
    /**
     * segment.index.bytes value
     */
    segmentIndexBytes?: string;
    /**
     * segment.jitter.ms value
     */
    segmentJitterMs?: string;
    /**
     * segment.ms value
     */
    segmentMs?: string;
    /**
     * unclean.leader.election.enable value
     */
    uncleanLeaderElectionEnable?: string;
}

export interface GetKafkaTopicTag {
    key: string;
    value?: string;
}

export interface GetM3AggregatorComponent {
    component: string;
    host: string;
    kafkaAuthenticationMethod: string;
    port: number;
    route: string;
    ssl: boolean;
    usage: string;
}

export interface GetM3AggregatorM3aggregator {
}

export interface GetM3AggregatorM3aggregatorUserConfig {
    /**
     * Serve the web frontend using a custom CNAME pointing to the Aiven DNS name.
     */
    customDomain?: string;
    /**
     * Allow incoming connections from CIDR address block, e.g. '10.20.0.0/16'
     */
    ipFilters?: string[];
    m3Version?: string;
    /**
     * M3 major version
     */
    m3aggregatorVersion?: string;
    staticIps?: string;
}

export interface GetM3AggregatorServiceIntegration {
    integrationType: string;
    sourceServiceName: string;
}

export interface GetM3DbComponent {
    component: string;
    host: string;
    kafkaAuthenticationMethod: string;
    port: number;
    route: string;
    ssl: boolean;
    usage: string;
}

export interface GetM3DbM3db {
}

export interface GetM3DbM3dbUserConfig {
    /**
     * Serve the web frontend using a custom CNAME pointing to the Aiven DNS name.
     */
    customDomain?: string;
    /**
     * Allow incoming connections from CIDR address block, e.g. '10.20.0.0/16'
     */
    ipFilters?: string[];
    /**
     * M3 limits
     */
    limits?: outputs.GetM3DbM3dbUserConfigLimits;
    m3Version?: string;
    /**
     * Enables access to Graphite Carbon 
     * plaintext metrics ingestion. It can be enabled only for services inside VPCs. The
     * metrics are written to aggregated namespaces only.
     */
    m3coordinatorEnableGraphiteCarbonIngest?: string;
    /**
     * M3 major version
     */
    m3dbVersion?: string;
    /**
     * List of M3 namespaces
     */
    namespaces?: outputs.GetM3DbM3dbUserConfigNamespace[];
    /**
     * Allow access to selected service ports from private networks.
     */
    privateAccess?: outputs.GetM3DbM3dbUserConfigPrivateAccess;
    /**
     * Name of another project to fork a service from. This has
     * effect only when a new service is being created.
     */
    projectToForkFrom?: string;
    /**
     * Allow access to selected service ports from the public Internet.
     */
    publicAccess?: outputs.GetM3DbM3dbUserConfigPublicAccess;
    /**
     * Mapping rules allow more granular use of aggregation, not simply sending
     * everything to a namespace. If mapping rules exist that target a namespace, only data matching mapping
     * rules will be sent to it and nothing else.
     */
    rules?: outputs.GetM3DbM3dbUserConfigRules;
    /**
     * Name of another service to fork from. This has effect only 
     * when a new service is being created.
     */
    serviceToForkFrom?: string;
    staticIps?: string;
}

export interface GetM3DbM3dbUserConfigLimits {
    /**
     * The maximum number of data points fetched during request
     */
    globalDatapoints?: string;
    /**
     * The maximum number of data points fetched in single query
     */
    queryDatapoints?: string;
    /**
     * When query limits are exceeded, whether to return error 
     * (if True) or return partial results (False)
     */
    queryRequireExhaustive?: string;
    /**
     * The maximum number of series fetched in single query
     */
    querySeries?: string;
}

export interface GetM3DbM3dbUserConfigNamespace {
    /**
     * The name of the namespace
     */
    name?: string;
    /**
     * Namespace options
     */
    options?: outputs.GetM3DbM3dbUserConfigNamespaceOptions;
    /**
     * The resolution for an aggregated namespace
     */
    resolution?: string;
    /**
     * The type of aggregation (aggregated/unaggregated)
     */
    type?: string;
}

export interface GetM3DbM3dbUserConfigNamespaceOptions {
    /**
     * Retention options
     */
    retentionOptions?: outputs.GetM3DbM3dbUserConfigNamespaceOptionsRetentionOptions;
    /**
     * Controls whether M3DB will create snapshot files for 
     * this namespace
     */
    snapshotEnabled?: string;
    /**
     * Controls whether M3DB will include writes to this 
     * namespace in the commitlog.
     */
    writesToCommitlog?: string;
}

export interface GetM3DbM3dbUserConfigNamespaceOptionsRetentionOptions {
    /**
     * Controls how long we wait before expiring stale data
     */
    blockDataExpiryDuration?: string;
    /**
     * Controls how long to keep a block in memory before 
     * flushing to a fileset on disk
     */
    blocksizeDuration?: string;
    /**
     * Controls how far into the future writes to 
     * the namespace will be accepted
     */
    bufferFutureDuration?: string;
    /**
     * Controls how far into the past writes to the 
     * namespace will be accepted
     */
    bufferPastDuration?: string;
    /**
     * Controls the duration of time that M3DB will 
     * retain data for the namespace
     */
    retentionPeriodDuration?: string;
}

export interface GetM3DbM3dbUserConfigPrivateAccess {
    /**
     * Allow clients to connect to m3coordinator from the public internet 
     * for service nodes that are in a project VPC or another type of private network.
     */
    m3coordinator?: string;
}

export interface GetM3DbM3dbUserConfigPublicAccess {
    /**
     * Allow clients to connect to m3coordinator from the public internet 
     * for service nodes that are in a project VPC or another type of private network.
     */
    m3coordinator?: string;
}

export interface GetM3DbM3dbUserConfigRules {
    mappings?: outputs.GetM3DbM3dbUserConfigRulesMapping[];
}

export interface GetM3DbM3dbUserConfigRulesMapping {
    /**
     * List of aggregations to be applied
     */
    aggregations?: string[];
    /**
     * Drop the matching metric; Only store the derived metric (as specified in the roll-up rules), if any.
     */
    drop?: string;
    /**
     * The metrics to be used with this particular rule; Matching metric names with wildcards (using
     * __name__:wildcard) or matching tags and their (optionally wildcarded) values. For value, !
     * can be used at start of value for negation, and multiple filters can be supplied using space as separator.
     */
    filter?: string;
    /**
     * The name of the namespace
     */
    name?: string;
    /**
     * List of tags to be appended to matching metrics.
     */
    tags?: outputs.GetM3DbM3dbUserConfigRulesMappingTag[];
}

export interface GetM3DbM3dbUserConfigRulesMappingTag {
    /**
     * The name of the namespace
     */
    name?: string;
    /**
     * Value of the tag.
     */
    value?: string;
}

export interface GetM3DbServiceIntegration {
    integrationType: string;
    sourceServiceName: string;
}

export interface GetMySqlComponent {
    component: string;
    /**
     * Hostname or IP address of the server where to migrate data from
     */
    host: string;
    kafkaAuthenticationMethod: string;
    /**
     * Port number of the server where to migrate data from
     */
    port: number;
    route: string;
    /**
     * The server where to migrate data from is secured with SSL
     */
    ssl: boolean;
    usage: string;
}

export interface GetMySqlMysql {
}

export interface GetMySqlMysqlUserConfig {
    /**
     * Custom password for admin user. Defaults to random string. 
     * This must be set only when a new service is being created.
     */
    adminPassword?: string;
    /**
     * Custom username for admin user. This must be set only when a 
     * new service is being created.
     */
    adminUsername?: string;
    /**
     * The hour of day (in UTC) when backup for the service is started. 
     * New backup is only started if previous backup has already completed.
     */
    backupHour?: string;
    /**
     * The minute of an hour when backup for the service is started. 
     * New backup is only started if previous backup has already completed.
     */
    backupMinute?: string;
    /**
     * The minimum amount of time in seconds to keep binlog entries
     * before deletion. This may be extended for services that require binlog entries for longer than the
     * default for example if using the MySQL Debezium Kafka connector.
     */
    binlogRetentionPeriod?: string;
    /**
     * Allow incoming connections from CIDR address block, e.g. '10.20.0.0/16'
     */
    ipFilters?: string[];
    /**
     * Migrate data from existing server
     */
    migration?: outputs.GetMySqlMysqlUserConfigMigration;
    /**
     * MySQL specific server provided values.
     */
    mysql?: outputs.GetMySqlMysqlUserConfigMysql;
    /**
     * MySQL major version
     */
    mysqlVersion?: string;
    /**
     * Allow access to selected service ports from private networks
     */
    privateAccess?: outputs.GetMySqlMysqlUserConfigPrivateAccess;
    /**
     * Allow access to selected service components through Privatelink
     */
    privatelinkAccess?: outputs.GetMySqlMysqlUserConfigPrivatelinkAccess;
    /**
     * Name of another project to fork a service from. This has
     * effect only when a new service is being created.
     */
    projectToForkFrom?: string;
    /**
     * Allow access to selected service ports from the public Internet
     */
    publicAccess?: outputs.GetMySqlMysqlUserConfigPublicAccess;
    /**
     * Recovery target time when forking a service. This has effect 
     * only when a new service is being created.
     */
    recoveryTargetTime?: string;
    /**
     * Name of another service to fork from. This has effect only when 
     * a new service is being created.
     */
    serviceToForkFrom?: string;
    staticIps?: string;
}

export interface GetMySqlMysqlUserConfigMigration {
    /**
     * Database name for bootstrapping the initial connection
     */
    dbname?: string;
    /**
     * Hostname or IP address of the server where to migrate data from
     */
    host?: string;
    /**
     * Comma-separated list of databases, which should be ignored
     * during migration (supported by MySQL only at the moment)
     */
    ignoreDbs?: string;
    /**
     * Password for authentication with the server where to migrate data from
     */
    password?: string;
    /**
     * Port number of the server where to migrate data from
     */
    port?: string;
    /**
     * The server where to migrate data from is secured with SSL
     */
    ssl?: string;
    /**
     * User name for authentication with the server where to migrate data from
     */
    username?: string;
}

export interface GetMySqlMysqlUserConfigMysql {
    /**
     * The number of seconds that the mysqld server waits for a 
     * connect packet before responding with Bad handshake
     */
    connectTimeout?: string;
    /**
     * Default server time zone as an offset from UTC 
     * (from -12:00 to +12:00), a time zone name, or 'SYSTEM' to use the MySQL server default.
     */
    defaultTimeZone?: string;
    /**
     * The maximum permitted result length in bytes for 
     * the GROUP_CONCAT() function.
     */
    groupConcatMaxLen?: string;
    /**
     * The time, in seconds, before cached 
     * statistics expire
     */
    informationSchemaStatsExpiry?: string;
    /**
     * Minimum length of words that are stored in 
     * an InnoDB FULLTEXT index.
     */
    innodbFtMinTokenSize?: string;
    /**
     * This option is used to specify your 
     * own InnoDB FULLTEXT index stopword list for all InnoDB tables.
     */
    innodbFtServerStopwordTable?: string;
    /**
     * The length of time in seconds an InnoDB 
     * transaction waits for a row lock before giving up.
     */
    innodbLockWaitTimeout?: string;
    /**
     * The size in bytes of the buffer that InnoDB 
     * uses to write to the log files on disk.
     */
    innodbLogBufferSize?: string;
    /**
     * The upper limit in bytes on the 
     * size of the temporary log files used during online DDL operations for InnoDB tables.
     */
    innodbOnlineAlterLogMaxSize?: string;
    /**
     * When enabled, information about all 
     * deadlocks in InnoDB user transactions is recorded in the error log. Disabled by default.
     */
    innodbPrintAllDeadlocks?: string;
    /**
     * When enabled a transaction timeout 
     * causes InnoDB to abort and roll back the entire transaction.
     */
    innodbRollbackOnTimeout?: string;
    /**
     * The number of seconds the server waits for 
     * activity on an interactive connection before closing it.
     */
    interactiveTimeout?: string;
    internalTmpMemStorageEngine?: string;
    /**
     * The slowQueryLogs work as SQL statements that take
     * more than longQueryTime seconds to execute. Default is 10s
     */
    longQueryTime?: string;
    /**
     * Size of the largest message in bytes that can 
     * be received by the server. Default is 67108864 (64M)
     */
    maxAllowedPacket?: string;
    /**
     * Limits the size of internal in-memory tables. 
     * Also set tmp_table_size. Default is 16777216 (16M)
     */
    maxHeapTableSize?: string;
    /**
     * The number of seconds to wait for more data from 
     * a connection before aborting the read.
     */
    netReadTimeout?: string;
    /**
     * The number of seconds to wait for a block to be 
     * written to a connection before aborting the write.
     */
    netWriteTimeout?: string;
    /**
     * Slow query log enables capturing of slow queries.
     * Setting slowQueryLog to false also truncates the mysql.slow_log table. Default is off
     */
    slowQueryLog?: string;
    /**
     * Sort buffer size in bytes for ORDER BY optimization. 
     * Default is 262144 (256K)
     */
    sortBufferSize?: string;
    /**
     * Global SQL mode. Set to empty to use MySQL server defaults. 
     * When creating a new service and not setting this field Aiven default SQL mode (strict,
     * SQL standard compliant) will be assigned.
     */
    sqlMode?: string;
    /**
     * Require primary key to be defined for new 
     * tables or old tables modified with ALTER TABLE and fail if missing. It is recommended
     * to always have primary keys because various functionality may break if any large table
     * is missing them.
     */
    sqlRequirePrimaryKey?: string;
    /**
     * Limits the size of internal in-memory tables. Also set 
     * max_heap_table_size. Default is 16777216 (16M)
     */
    tmpTableSize?: string;
    /**
     * The number of seconds the server waits for activity on 
     * a noninteractive connection before closing it.
     */
    waitTimeout?: string;
}

export interface GetMySqlMysqlUserConfigPrivateAccess {
    /**
     * MySQL specific server provided values.
     */
    mysql?: string;
    /**
     * (Optional) Allow clients to connect to mysqlx from the public internet for service
     * nodes that are in a project VPC or another type of private network
     */
    mysqlx?: string;
    /**
     * Allow clients to connect to prometheus from the public internet 
     * for service nodes that are in a project VPC or another type of private network
     */
    prometheus?: string;
}

export interface GetMySqlMysqlUserConfigPrivatelinkAccess {
    /**
     * MySQL specific server provided values.
     */
    mysql?: string;
    /**
     * (Optional) Allow clients to connect to mysqlx from the public internet for service
     * nodes that are in a project VPC or another type of private network
     */
    mysqlx?: string;
}

export interface GetMySqlMysqlUserConfigPublicAccess {
    /**
     * MySQL specific server provided values.
     */
    mysql?: string;
    /**
     * (Optional) Allow clients to connect to mysqlx from the public internet for service
     * nodes that are in a project VPC or another type of private network
     */
    mysqlx?: string;
    /**
     * Allow clients to connect to prometheus from the public internet 
     * for service nodes that are in a project VPC or another type of private network
     */
    prometheus?: string;
}

export interface GetMySqlServiceIntegration {
    integrationType: string;
    sourceServiceName: string;
}

export interface GetOpenSearchComponent {
    component: string;
    host: string;
    kafkaAuthenticationMethod: string;
    port: number;
    route: string;
    ssl: boolean;
    usage: string;
}

export interface GetOpenSearchOpensearch {
    /**
     * URI for Opensearch dashboards frontend.
     */
    opensearchDashboardsUri: string;
}

export interface GetOpenSearchOpensearchUserConfig {
    /**
     * Serve the web frontend using a custom CNAME pointing to the Aiven DNS name.
     */
    customDomain?: string;
    /**
     * Disable automatic replication factor adjustment for multi-node services.
     * By default, Aiven ensures all indexes are replicated at least to two nodes. Note: setting this to true increases a
     * risk of data loss in case of virtual machine failure.
     */
    disableReplicationFactorAdjustment?: string;
    /**
     * Glob pattern and number of indexes matching that pattern to be kept.
     */
    indexPatterns?: outputs.GetOpenSearchOpensearchUserConfigIndexPattern[];
    /**
     * Template settings for all new indexe.
     */
    indexTemplate?: outputs.GetOpenSearchOpensearchUserConfigIndexTemplate;
    /**
     * allow incoming connections from CIDR address block, e.g. `10.20.0.0/16`
     */
    ipFilters?: string[];
    keepIndexRefreshInterval?: string;
    /**
     * Maximum number of indexes to keep before deleting the oldest one.
     */
    maxIndexCount?: string;
    /**
     * Allow clients to connect to opensearch from the public internet for service nodes that are in a
     * project VPC or another type of private network.
     */
    opensearch?: outputs.GetOpenSearchOpensearchUserConfigOpensearch;
    /**
     * Allow clients to connect to opensearchDashboards from the public internet for
     * service nodes that are in a project VPC or another type of private network.
     */
    opensearchDashboards?: outputs.GetOpenSearchOpensearchUserConfigOpensearchDashboards;
    /**
     * Opensearch major version.
     */
    opensearchVersion?: string;
    /**
     * Allow access to selected service ports from private networks.
     */
    privateAccess?: outputs.GetOpenSearchOpensearchUserConfigPrivateAccess;
    /**
     * Allow access to selected service components through Privatelink
     */
    privatelinkAccess?: outputs.GetOpenSearchOpensearchUserConfigPrivatelinkAccess;
    /**
     * Name of another project to fork a service from. This has effect only when a new service
     * is being created.
     */
    projectToForkFrom?: string;
    /**
     * Allow access to selected service ports from the public Internet.
     */
    publicAccess?: outputs.GetOpenSearchOpensearchUserConfigPublicAccess;
    /**
     * Name of the basebackup to restore in forked service.
     */
    recoveryBasebackupName?: string;
    /**
     * Name of another service to fork from. This has effect only when a new service is being
     * created.
     */
    serviceToForkFrom?: string;
    staticIps?: string;
}

export interface GetOpenSearchOpensearchUserConfigIndexPattern {
    /**
     * Maximum number of indexes to keep before deleting the oldest one.
     */
    maxIndexCount?: string;
    /**
     * Must consist of alpha-numeric characters, dashes, underscores, dots and glob characters (* and ?)
     */
    pattern?: string;
    /**
     * Deletion sorting algorithm
     */
    sortingAlgorithm?: string;
}

export interface GetOpenSearchOpensearchUserConfigIndexTemplate {
    /**
     * The maximum number of nested JSON objects that a single document can contain
     * across all nested types. This limit helps to prevent out of memory errors when a document contains too many
     * nested objects. Default is 10000.
     */
    mappingNestedObjectsLimit?: string;
    /**
     * The number of replicas each primary shard has.
     */
    numberOfReplicas?: string;
    /**
     * The number of primary shards that an index should have.
     */
    numberOfShards?: string;
}

export interface GetOpenSearchOpensearchUserConfigOpensearch {
    /**
     * Explicitly allow or block automatic creation of indices. Defaults to true
     */
    actionAutoCreateIndexEnabled?: string;
    /**
     * Require explicit index names when deleting
     */
    actionDestructiveRequiresName?: string;
    /**
     * Controls the number of shards allowed in the cluster per data node
     */
    clusterMaxShardsPerNode?: string;
    /**
     * Maximum content length for HTTP requests to the Opensearch HTTP API, in bytes.
     */
    httpMaxContentLength?: string;
    /**
     * The max size of allowed headers, in bytes.
     */
    httpMaxHeaderSize?: string;
    /**
     * The max length of an HTTP URL, in bytes.
     */
    httpMaxInitialLineLength?: string;
    /**
     * Relative amount. Maximum amount of heap memory used for field data cache.
     * This is an expert setting; decreasing the value too much will increase overhead of loading field data; too
     * much memory used for field data cache will decrease amount of heap available for other operations.
     */
    indicesFielddataCacheSize?: string;
    /**
     * Percentage value. Default is 10%. Total amount of heap used for indexing
     * buffer, before writing segments to disk. This is an expert setting. Too low value will slow down indexing; too
     * high value will increase indexing performance but causes performance issues for query performance.
     */
    indicesMemoryIndexBufferSize?: string;
    /**
     * Percentage value. Default is 10%. Maximum amount of heap used for query cache.
     * This is an expert setting. Too low value will decrease query performance and increase performance for other
     * operations; too high value will cause issues with other Opensearch functionality.
     */
    indicesQueriesCacheSize?: string;
    /**
     * Maximum number of clauses Lucene BooleanQuery can have. The default
     * value (1024) is relatively high, and increasing it may cause performance issues. Investigate other approaches
     * first before increasing this value.
     */
    indicesQueryBoolMaxClauseCount?: string;
    /**
     * Whitelisted addresses for reindexing. Changing this value will cause all
     * Opensearch instances to restart.
     */
    reindexRemoteWhitelists?: string[];
    /**
     * Maximum number of aggregation buckets allowed in a single response. Opensearch default
     * value is used when this is not defined.
     */
    searchMaxBuckets?: string;
    /**
     * Size for the thread pool queue. See documentation for exact details.
     */
    threadPoolAnalyzeQueueSize?: string;
    /**
     * Size for the thread pool. See documentation for exact details. Do note this may
     * have maximum value depending on CPU count - value is automatically lowered if set to higher than maximum
     * value.
     */
    threadPoolAnalyzeSize?: string;
    /**
     * Size for the thread pool. See documentation for exact details. Do note this
     * may have maximum value depending on CPU count - value is automatically lowered if set to higher than maximum
     * value.
     */
    threadPoolForceMergeSize?: string;
    /**
     * Size for the thread pool queue. See documentation for exact details.
     */
    threadPoolGetQueueSize?: string;
    /**
     * Size for the thread pool. See documentation for exact details. Do note this may have
     * maximum value depending on CPU count - value is automatically lowered if set to higher than maximum value.
     */
    threadPoolGetSize?: string;
    /**
     * Size for the thread pool. See documentation for exact details. Do note this may
     * have maximum value depending on CPU count - value is automatically lowered if set to higher than maximum
     * value.
     */
    threadPoolIndexSize?: string;
    /**
     * Size for the thread pool queue. See documentation for exact details.
     */
    threadPoolSearchQueueSize?: string;
    /**
     * Size for the thread pool. See documentation for exact details. Do note this may
     * have maximum value depending on CPU count - value is automatically lowered if set to higher than maximum
     * value.
     */
    threadPoolSearchSize?: string;
    /**
     * Size for the thread pool queue. See documentation for exact
     * details.
     */
    threadPoolSearchThrottledQueueSize?: string;
    /**
     * Size for the thread pool. See documentation for exact details. Do note
     * this may have maximum value depending on CPU count - value is automatically lowered if set to higher than
     * maximum value.
     */
    threadPoolSearchThrottledSize?: string;
    /**
     * Size for the thread pool queue. See documentation for exact details.
     */
    threadPoolWriteQueueSize?: string;
    /**
     * Size for the thread pool. See documentation for exact details. Do note this may
     * have maximum value depending on CPU count - value is automatically lowered if set to higher than maximum
     * value.
     */
    threadPoolWriteSize?: string;
}

export interface GetOpenSearchOpensearchUserConfigOpensearchDashboards {
    /**
     * Enable or disable Opensearch dashboards.
     */
    enabled?: string;
    /**
     * Limits the maximum amount of memory (in MiB) the Opensearch dashboards process can use.
     * This sets the maxOldSpaceSize option of the nodejs running the Opensearch dashboards. Note: the memory
     * reserved by Opensearch dashboards is not available for Opensearch.
     */
    maxOldSpaceSize?: string;
    /**
     * Timeout in milliseconds for requests made by Opensearch dashboards towards
     * Opensearch.
     */
    opensearchRequestTimeout?: string;
}

export interface GetOpenSearchOpensearchUserConfigPrivateAccess {
    /**
     * Allow clients to connect to opensearch from the public internet for service nodes that are in a
     * project VPC or another type of private network.
     */
    opensearch?: string;
    /**
     * Allow clients to connect to opensearchDashboards from the public internet for
     * service nodes that are in a project VPC or another type of private network.
     */
    opensearchDashboards?: string;
    /**
     * Allow clients to connect to prometheus from the public internet for service nodes that are in a
     * project VPC or another type of private network.
     */
    prometheus?: string;
}

export interface GetOpenSearchOpensearchUserConfigPrivatelinkAccess {
    /**
     * Allow clients to connect to opensearch from the public internet for service nodes that are in a
     * project VPC or another type of private network.
     */
    opensearch?: string;
    /**
     * Allow clients to connect to opensearchDashboards from the public internet for
     * service nodes that are in a project VPC or another type of private network.
     */
    opensearchDashboards?: string;
}

export interface GetOpenSearchOpensearchUserConfigPublicAccess {
    /**
     * Allow clients to connect to opensearch from the public internet for service nodes that are in a
     * project VPC or another type of private network.
     */
    opensearch?: string;
    /**
     * Allow clients to connect to opensearchDashboards from the public internet for
     * service nodes that are in a project VPC or another type of private network.
     */
    opensearchDashboards?: string;
    /**
     * Allow clients to connect to prometheus from the public internet for service nodes that are in a
     * project VPC or another type of private network.
     */
    prometheus?: string;
}

export interface GetOpenSearchServiceIntegration {
    integrationType: string;
    sourceServiceName: string;
}

export interface GetPgComponent {
    component: string;
    /**
     * PostgreSQL master node host IP or name
     */
    host: string;
    kafkaAuthenticationMethod: string;
    /**
     * PostgreSQL port
     */
    port: number;
    route: string;
    /**
     * the server where to migrate data from is secured with SSL.
     */
    ssl: boolean;
    usage: string;
}

export interface GetPgPg {
    /**
     * Primary PostgreSQL database name
     */
    dbname: string;
    /**
     * PostgreSQL master node host IP or name
     */
    host: string;
    /**
     * PostgreSQL admin user password
     */
    password: string;
    /**
     * PostgreSQL port
     */
    port: number;
    /**
     * PostgreSQL replica URI for services with a replica
     */
    replicaUri: string;
    /**
     * PostgreSQL sslmode setting (currently always `require`)
     */
    sslmode: string;
    /**
     * PostgreSQL master connection URI
     */
    uri: string;
    /**
     * PostgreSQL admin user name
     */
    user: string;
}

export interface GetPgPgUserConfig {
    /**
     * custom password for admin user. Defaults to random string. *This must
     * be set only when a new service is being created.*
     */
    adminPassword?: string;
    /**
     * custom username for admin user. *This must be set only when a new service
     * is being created.*
     */
    adminUsername?: string;
    /**
     * the hour of day (in UTC) when backup for the service is started. New backup 
     * is only started if previous backup has already completed.
     */
    backupHour?: string;
    /**
     * the minute of an hour when backup for the service is started. New backup 
     * is only started if previous backup has already completed.
     */
    backupMinute?: string;
    /**
     * allow incoming connections from CIDR address block, e.g. `10.20.0.0/16`
     */
    ipFilters?: string[];
    /**
     * migrate data from existing server, has the following options:
     */
    migration?: outputs.GetPgPgUserConfigMigration;
    /**
     * PostgreSQL specific server provided values.
     */
    pg?: outputs.GetPgPgUserConfigPg;
    /**
     * This setting is deprecated. Use read-replica service integration instead.
     */
    pgReadReplica?: string;
    /**
     * Name of the PG Service from which to fork (deprecated, use service_to_fork_from). 
     * This has effect only when a new service is being created.
     */
    pgServiceToForkFrom?: string;
    /**
     * PostgreSQL major version.
     */
    pgVersion?: string;
    /**
     * Enable pgbouncer.
     */
    pgbouncer?: outputs.GetPgPgUserConfigPgbouncer;
    /**
     * PGLookout settings.
     */
    pglookout?: outputs.GetPgPgUserConfigPglookout;
    /**
     * Allow access to selected service ports from private networks.
     */
    privateAccess?: outputs.GetPgPgUserConfigPrivateAccess;
    /**
     * Allow access to selected service components through Privatelink.
     */
    privatelinkAccess?: outputs.GetPgPgUserConfigPrivatelinkAccess;
    /**
     * Name of another project to fork a service from. This has
     * effect only when a new service is being created.
     */
    projectToForkFrom?: string;
    /**
     * Allow access to selected service ports from the public Internet
     */
    publicAccess?: outputs.GetPgPgUserConfigPublicAccess;
    /**
     * Recovery target time when forking a service. This has effect 
     * only when a new service is being created.
     */
    recoveryTargetTime?: string;
    /**
     * Name of another service to fork from. This has effect only 
     * when a new service is being created.
     */
    serviceToForkFrom?: string;
    /**
     * Percentage of total RAM that the database server uses for 
     * memory buffers. Valid range is 20-60 (float), which corresponds to 20% - 60%. This setting adjusts
     * the sharedBuffers configuration value. The absolute maximum is 12 GB.
     */
    sharedBuffersPercentage?: string;
    staticIps?: string;
    /**
     * Synchronous replication type. Note that the service plan 
     * also needs to support synchronous replication.
     */
    synchronousReplication?: string;
    /**
     * TimescaleDB extension configuration values.
     */
    timescaledb?: outputs.GetPgPgUserConfigTimescaledb;
    /**
     * Variant of the PostgreSQL service, may affect the features that are 
     * exposed by default. Options: `aiven` or `timescale`.
     */
    variant?: string;
    /**
     * Sets the maximum amount of memory to be used by a query operation (such 
     * as a sort or hash table) before writing to temporary disk files, in MB. Default is 1MB + 0.075% of
     * total RAM (up to 32MB).
     */
    workMem?: string;
}

export interface GetPgPgUserConfigMigration {
    /**
     * Primary PostgreSQL database name
     */
    dbname?: string;
    /**
     * PostgreSQL master node host IP or name
     */
    host?: string;
    /**
     * Comma-separated list of databases, which should be ignored during
     * migration (supported by MySQL only at the moment)
     */
    ignoreDbs?: string;
    /**
     * PostgreSQL admin user password
     */
    password?: string;
    /**
     * PostgreSQL port
     */
    port?: string;
    /**
     * the server where to migrate data from is secured with SSL.
     */
    ssl?: string;
    /**
     * user name for authentication with the server where to migrate data from.
     */
    username?: string;
}

export interface GetPgPgUserConfigPg {
    /**
     * Specifies a fraction of the table size to add to 
     * autovacuumAnalyzeThreshold when deciding whether to trigger an ANALYZE. The default is 0.2
     * (20% of table size).
     */
    autovacuumAnalyzeScaleFactor?: string;
    /**
     * specifies the minimum number of inserted, updated 
     * or deleted tuples needed to trigger an ANALYZE in any one table. The default is 50 tuples.
     */
    autovacuumAnalyzeThreshold?: string;
    /**
     * specifies the maximum age (in transactions) that a table's 
     * pg_class.relfrozenxid field can attain before a VACUUM operation is forced to prevent transaction ID
     * wraparound within the table. Note that the system will launch autovacuum processes to prevent wraparound
     * even when autovacuum is otherwise disabled. This parameter will cause the server to be restarted.
     */
    autovacuumFreezeMaxAge?: string;
    /**
     * specifies the maximum number of autovacuum processes (other 
     * than the autovacuum launcher) that may be running at any one time. The default is three. This parameter
     * can only be set at server start.
     */
    autovacuumMaxWorkers?: string;
    /**
     * specifies the minimum delay between autovacuum runs on any 
     * given database. The delay is measured in seconds, and the default is one minute.
     */
    autovacuumNaptime?: string;
    /**
     * specifies the cost delay value that will be used 
     * in automatic VACUUM operations. If -1 is specified, the regular vacuumCostDelay value will be
     * used. The default value is 20 milliseconds.
     */
    autovacuumVacuumCostDelay?: string;
    /**
     * specifies the cost limit value that will be used in 
     * automatic VACUUM operations. If -1 is specified (which is the default), the regular vacuumCostLimit
     * value will be used.
     */
    autovacuumVacuumCostLimit?: string;
    /**
     * specifies a fraction of the table size to add to 
     * autovacuumVacuumThreshold when deciding whether to trigger a VACUUM. The default is 0.2 (20% of table size).
     */
    autovacuumVacuumScaleFactor?: string;
    /**
     * specifies the minimum number of updated or deleted tuples 
     * needed to trigger a VACUUM in any one table. The default is 50 tuples
     */
    autovacuumVacuumThreshold?: string;
    bgwriterDelay?: string;
    bgwriterFlushAfter?: string;
    bgwriterLruMaxpages?: string;
    bgwriterLruMultiplier?: string;
    /**
     * this is the amount of time, in milliseconds, to wait on a lock before 
     * checking to see if there is a deadlock condition.
     */
    deadlockTimeout?: string;
    /**
     * Time out sessions with open transactions after 
     * this number of milliseconds.
     */
    idleInTransactionSessionTimeout?: string;
    /**
     * Controls system-wide use of Just-in-Time Compilation (JIT).
     */
    jit?: string;
    /**
     * Causes each action executed by autovacuum to be logged 
     * if it ran for at least the specified number of milliseconds. Setting this to zero logs all autovacuum
     * actions. Minus-one (the default) disables logging autovacuum actions.
     */
    logAutovacuumMinDuration?: string;
    /**
     * Controls the amount of detail written in the server log for 
     * each message that is logged. Possible values: `TERSE`, `DEFAULT` and `VERBOSE`.
     */
    logErrorVerbosity?: string;
    /**
     * Choose from one of the available log-formats. These can support
     * popular log analyzers like pgbadger, pganalyze etc.
     */
    logLinePrefix?: string;
    /**
     * Log statements that take more than this number of 
     * milliseconds to run, -1 disables
     */
    logMinDurationStatement?: string;
    /**
     * PostgreSQL maximum number of files that can be open per process
     */
    maxFilesPerProcess?: string;
    /**
     * PostgreSQL maximum locks per transaction
     */
    maxLocksPerTransaction?: string;
    /**
     * PostgreSQL maximum logical replication workers 
     * (taken from the pool of max_parallel_workers)
     */
    maxLogicalReplicationWorkers?: string;
    /**
     * Sets the maximum number of workers that the system can 
     * support for parallel queries.
     */
    maxParallelWorkers?: string;
    /**
     * Sets the maximum number of workers that can be 
     * started by a single Gather or Gather Merge node.
     */
    maxParallelWorkersPerGather?: string;
    /**
     * PostgreSQL maximum predicate locks per transaction
     */
    maxPredLocksPerTransaction?: string;
    /**
     * PostgreSQL maximum prepared transactions
     */
    maxPreparedTransactions?: string;
    /**
     * PostgreSQL maximum replication slots
     */
    maxReplicationSlots?: string;
    /**
     * Maximum depth of the stack in bytes
     */
    maxStackDepth?: string;
    /**
     * Max standby archive delay in milliseconds
     */
    maxStandbyArchiveDelay?: string;
    /**
     * Max standby streaming delay in milliseconds
     */
    maxStandbyStreamingDelay?: string;
    /**
     * PostgreSQL maximum WAL senders
     */
    maxWalSenders?: string;
    /**
     * Sets the maximum number of background processes that the system
     * can support
     * * `pg_partman_bgw.interval` - Sets the time interval to run pg_partman's scheduled tasks
     * * `pg_partman_bgw.role` - Controls which role to use for pg_partman's scheduled
     * background tasks.
     * * `pg_stat_statements.track` - Controls which statements are counted. Specify top
     * to track top-level statements (those issued directly by clients), all to also track nested
     * statements (such as statements invoked within functions), or none to disable statement statistics
     * collection. The default value is top.
     */
    maxWorkerProcesses?: string;
    pgPartmanBgwDotInterval?: string;
    pgPartmanBgwDotRole?: string;
    pgStatStatementsDotTrack?: string;
    /**
     * PostgreSQL temporary file limit in KiB, -1 for unlimited
     */
    tempFileLimit?: string;
    /**
     * PostgreSQL service timezone
     */
    timezone?: string;
    /**
     * Specifies the number of bytes reserved to track the currently 
     * executing command for each active session.
     */
    trackActivityQuerySize?: string;
    /**
     * Record commit time of transactions
     */
    trackCommitTimestamp?: string;
    /**
     * Enables tracking of function call counts and time used.
     */
    trackFunctions?: string;
    /**
     * Enables timing of database I/O calls. This parameter is off by default,
     * because it will repeatedly query the operating system for the current time, which may cause
     * significant overhead on some platforms.
     */
    trackIoTiming?: string;
    /**
     * Terminate replication connections that are inactive for longer than 
     * this amount of time, in milliseconds.
     */
    walSenderTimeout?: string;
    /**
     * WAL flush interval in milliseconds. Note that setting this value 
     * to lower than the default 200ms may negatively impact performance
     */
    walWriterDelay?: string;
}

export interface GetPgPgUserConfigPgbouncer {
    /**
     * If the automatically created database pools have been unused this 
     * many seconds, they are freed. If 0 then timeout is disabled.
     */
    autodbIdleTimeout?: string;
    /**
     * Do not allow more than this many server connections per database 
     * (regardless of user). Setting it to 0 means unlimited.
     */
    autodbMaxDbConnections?: string;
    /**
     * PGBouncer pool mode
     */
    autodbPoolMode?: string;
    /**
     * If non-zero then create automatically a pool of that size per user 
     * when a pool doesn't exist.
     */
    autodbPoolSize?: string;
    /**
     * Enum of parameters to ignore when given in startup packet.
     */
    ignoreStartupParameters?: string[];
    /**
     * Add more server connections to pool if below this number. Improves 
     * behavior when usual load comes suddenly back after period of total inactivity. The value is
     * effectively capped at the pool size.
     */
    minPoolSize?: string;
    /**
     * If a server connection has been idle more than this many seconds 
     * it will be dropped. If 0 then timeout is disabled.
     */
    serverIdleTimeout?: string;
    /**
     * The pooler will close an unused server connection that has been connected 
     * longer than this.
     */
    serverLifetime?: string;
    /**
     * Run serverResetQuery (DISCARD ALL) in all pooling modes.
     */
    serverResetQueryAlways?: string;
}

export interface GetPgPgUserConfigPglookout {
    /**
     * Number of seconds of master unavailability before 
     * triggering database failover to standby
     */
    maxFailoverReplicationTimeLag?: string;
}

export interface GetPgPgUserConfigPrivateAccess {
    /**
     * PostgreSQL specific server provided values.
     */
    pg?: string;
    /**
     * Enable pgbouncer.
     */
    pgbouncer?: string;
    /**
     * Allow clients to connect to prometheus from the public internet for 
     * service nodes that are in a project VPC or another type of private network
     */
    prometheus?: string;
}

export interface GetPgPgUserConfigPrivatelinkAccess {
    /**
     * PostgreSQL specific server provided values.
     */
    pg?: string;
    /**
     * Enable pgbouncer.
     */
    pgbouncer?: string;
}

export interface GetPgPgUserConfigPublicAccess {
    /**
     * PostgreSQL specific server provided values.
     */
    pg?: string;
    /**
     * Enable pgbouncer.
     */
    pgbouncer?: string;
    /**
     * Allow clients to connect to prometheus from the public internet for 
     * service nodes that are in a project VPC or another type of private network
     */
    prometheus?: string;
}

export interface GetPgPgUserConfigTimescaledb {
    /**
     * The number of background workers for timescaledb 
     * operations. You should configure this setting to the sum of your number of databases and the
     * total number of concurrent background workers you want running at any given point in time.
     */
    maxBackgroundWorkers?: string;
}

export interface GetPgServiceIntegration {
    integrationType: string;
    sourceServiceName: string;
}

export interface GetRedisComponent {
    component: string;
    /**
     * (Required) Hostname or IP address of the server where to migrate data from
     */
    host: string;
    kafkaAuthenticationMethod: string;
    /**
     * (Required) Port number of the server where to migrate data from
     */
    port: number;
    route: string;
    /**
     * The server where to migrate data from is secured with SSL
     */
    ssl: boolean;
    usage: string;
}

export interface GetRedisRedis {
}

export interface GetRedisRedisUserConfig {
    /**
     * Allow incoming connections from CIDR address block, e.g. `10.20.0.0/16`
     */
    ipFilters?: string[];
    /**
     * Migrate data from existing server
     */
    migration?: outputs.GetRedisRedisUserConfigMigration;
    /**
     * Allow access to selected service ports from private networks
     */
    privateAccess?: outputs.GetRedisRedisUserConfigPrivateAccess;
    /**
     * Allow access to selected service components through Privatelink
     */
    privatelinkAccess?: outputs.GetRedisRedisUserConfigPrivatelinkAccess;
    /**
     * Name of another project to fork a service from. This has
     * effect only when a new service is being created.
     */
    projectToForkFrom?: string;
    /**
     * Allow access to selected service ports from the public Internet
     */
    publicAccess?: outputs.GetRedisRedisUserConfigPublicAccess;
    /**
     * Name of the basebackup to restore in forked service
     */
    recoveryBasebackupName?: string;
    redisAclChannelsDefault?: string;
    /**
     * Redis IO thread count
     * * `redisLfuDecayTime"` - LFU maxmemory-policy counter decay time in minutes
     */
    redisIoThreads?: string;
    redisLfuDecayTime?: string;
    /**
     * Counter logarithm factor for volatile-lfu and allkeys-lfu 
     * maxmemory-policies
     */
    redisLfuLogFactor?: string;
    /**
     * Redis maxmemory-policy
     */
    redisMaxmemoryPolicy?: string;
    /**
     * Set notify-keyspace-events option
     */
    redisNotifyKeyspaceEvents?: string;
    redisNumberOfDatabases?: string;
    redisPersistence?: string;
    redisPubsubClientOutputBufferLimit?: string;
    /**
     * Require SSL to access Redis
     */
    redisSsl?: string;
    /**
     * Redis idle connection timeout
     * * `serviceToForkFrom"` - Name of another service to fork from. This has effect only
     * when a new service is being created.
     */
    redisTimeout?: string;
    serviceToForkFrom?: string;
    staticIps?: string;
}

export interface GetRedisRedisUserConfigMigration {
    /**
     * Database name for bootstrapping the initial connection
     */
    dbname?: string;
    /**
     * (Required) Hostname or IP address of the server where to migrate data from
     */
    host?: string;
    /**
     * Comma-separated list of databases, which should be ignored during
     * migration (supported by MySQL only at the moment)
     */
    ignoreDbs?: string;
    /**
     * Password for authentication with the server where to migrate data from
     */
    password?: string;
    /**
     * (Required) Port number of the server where to migrate data from
     */
    port?: string;
    /**
     * The server where to migrate data from is secured with SSL
     */
    ssl?: string;
    /**
     * User name for authentication with the server where to migrate data from
     */
    username?: string;
}

export interface GetRedisRedisUserConfigPrivateAccess {
    /**
     * Allow clients to connect to prometheus from the public internet 
     * for service nodes that are in a project VPC or another type of private network
     */
    prometheus?: string;
    /**
     * Redis specific server provided values.
     */
    redis?: string;
}

export interface GetRedisRedisUserConfigPrivatelinkAccess {
    /**
     * Redis specific server provided values.
     */
    redis?: string;
}

export interface GetRedisRedisUserConfigPublicAccess {
    /**
     * Allow clients to connect to prometheus from the public internet 
     * for service nodes that are in a project VPC or another type of private network
     */
    prometheus?: string;
    /**
     * Redis specific server provided values.
     */
    redis?: string;
}

export interface GetRedisServiceIntegration {
    integrationType: string;
    sourceServiceName: string;
}

export interface GetServiceCassandra {
}

export interface GetServiceCassandraUserConfig {
    cassandra?: outputs.GetServiceCassandraUserConfigCassandra;
    cassandraVersion?: string;
    ipFilters?: string[];
    migrateSstableloader?: string;
    privateAccess?: outputs.GetServiceCassandraUserConfigPrivateAccess;
    projectToForkFrom?: string;
    publicAccess?: outputs.GetServiceCassandraUserConfigPublicAccess;
    serviceToForkFrom?: string;
    staticIps?: string;
}

export interface GetServiceCassandraUserConfigCassandra {
    batchSizeFailThresholdInKb?: string;
    batchSizeWarnThresholdInKb?: string;
}

export interface GetServiceCassandraUserConfigPrivateAccess {
    prometheus?: string;
}

export interface GetServiceCassandraUserConfigPublicAccess {
    prometheus?: string;
}

export interface GetServiceComponent {
    component: string;
    host: string;
    kafkaAuthenticationMethod: string;
    port: number;
    route: string;
    ssl: boolean;
    usage: string;
}

export interface GetServiceElasticsearch {
    kibanaUri: string;
}

export interface GetServiceElasticsearchUserConfig {
    customDomain?: string;
    disableReplicationFactorAdjustment?: string;
    elasticsearch?: outputs.GetServiceElasticsearchUserConfigElasticsearch;
    elasticsearchVersion?: string;
    indexPatterns?: outputs.GetServiceElasticsearchUserConfigIndexPattern[];
    indexTemplate?: outputs.GetServiceElasticsearchUserConfigIndexTemplate;
    ipFilters?: string[];
    keepIndexRefreshInterval?: string;
    kibana?: outputs.GetServiceElasticsearchUserConfigKibana;
    maxIndexCount?: string;
    opensearchVersion?: string;
    privateAccess?: outputs.GetServiceElasticsearchUserConfigPrivateAccess;
    privatelinkAccess?: outputs.GetServiceElasticsearchUserConfigPrivatelinkAccess;
    projectToForkFrom?: string;
    publicAccess?: outputs.GetServiceElasticsearchUserConfigPublicAccess;
    recoveryBasebackupName?: string;
    serviceToForkFrom?: string;
    staticIps?: string;
}

export interface GetServiceElasticsearchUserConfigElasticsearch {
    actionAutoCreateIndexEnabled?: string;
    actionDestructiveRequiresName?: string;
    clusterMaxShardsPerNode?: string;
    httpMaxContentLength?: string;
    httpMaxHeaderSize?: string;
    httpMaxInitialLineLength?: string;
    indicesFielddataCacheSize?: string;
    indicesMemoryIndexBufferSize?: string;
    indicesQueriesCacheSize?: string;
    indicesQueryBoolMaxClauseCount?: string;
    reindexRemoteWhitelists?: string[];
    searchMaxBuckets?: string;
    threadPoolAnalyzeQueueSize?: string;
    threadPoolAnalyzeSize?: string;
    threadPoolForceMergeSize?: string;
    threadPoolGetQueueSize?: string;
    threadPoolGetSize?: string;
    threadPoolIndexQueueSize?: string;
    threadPoolIndexSize?: string;
    threadPoolSearchQueueSize?: string;
    threadPoolSearchSize?: string;
    threadPoolSearchThrottledQueueSize?: string;
    threadPoolSearchThrottledSize?: string;
    threadPoolWriteQueueSize?: string;
    threadPoolWriteSize?: string;
}

export interface GetServiceElasticsearchUserConfigIndexPattern {
    maxIndexCount?: string;
    pattern?: string;
    sortingAlgorithm?: string;
}

export interface GetServiceElasticsearchUserConfigIndexTemplate {
    mappingNestedObjectsLimit?: string;
    numberOfReplicas?: string;
    numberOfShards?: string;
}

export interface GetServiceElasticsearchUserConfigKibana {
    elasticsearchRequestTimeout?: string;
    enabled?: string;
    maxOldSpaceSize?: string;
}

export interface GetServiceElasticsearchUserConfigPrivateAccess {
    elasticsearch?: string;
    kibana?: string;
    prometheus?: string;
}

export interface GetServiceElasticsearchUserConfigPrivatelinkAccess {
    elasticsearch?: string;
    kibana?: string;
}

export interface GetServiceElasticsearchUserConfigPublicAccess {
    elasticsearch?: string;
    kibana?: string;
    prometheus?: string;
}

export interface GetServiceGrafana {
}

export interface GetServiceGrafanaUserConfig {
    alertingEnabled?: string;
    alertingErrorOrTimeout?: string;
    alertingMaxAnnotationsToKeep?: string;
    alertingNodataOrNullvalues?: string;
    allowEmbedding?: string;
    authAzuread?: outputs.GetServiceGrafanaUserConfigAuthAzuread;
    authBasicEnabled?: string;
    authGenericOauth?: outputs.GetServiceGrafanaUserConfigAuthGenericOauth;
    authGithub?: outputs.GetServiceGrafanaUserConfigAuthGithub;
    authGitlab?: outputs.GetServiceGrafanaUserConfigAuthGitlab;
    authGoogle?: outputs.GetServiceGrafanaUserConfigAuthGoogle;
    cookieSamesite?: string;
    customDomain?: string;
    dashboardsMinRefreshInterval?: string;
    dashboardsVersionsToKeep?: string;
    dataproxySendUserHeader?: string;
    dataproxyTimeout?: string;
    dateFormats?: outputs.GetServiceGrafanaUserConfigDateFormats;
    disableGravatar?: string;
    editorsCanAdmin?: string;
    externalImageStorage?: outputs.GetServiceGrafanaUserConfigExternalImageStorage;
    googleAnalyticsUaId?: string;
    ipFilters?: string[];
    metricsEnabled?: string;
    privateAccess?: outputs.GetServiceGrafanaUserConfigPrivateAccess;
    privatelinkAccess?: outputs.GetServiceGrafanaUserConfigPrivatelinkAccess;
    projectToForkFrom?: string;
    publicAccess?: outputs.GetServiceGrafanaUserConfigPublicAccess;
    recoveryBasebackupName?: string;
    serviceToForkFrom?: string;
    smtpServer?: outputs.GetServiceGrafanaUserConfigSmtpServer;
    staticIps?: string;
    userAutoAssignOrg?: string;
    userAutoAssignOrgRole?: string;
    viewersCanEdit?: string;
}

export interface GetServiceGrafanaUserConfigAuthAzuread {
    allowSignUp?: string;
    allowedDomains?: string[];
    allowedGroups?: string[];
    authUrl?: string;
    clientId?: string;
    clientSecret?: string;
    tokenUrl?: string;
}

export interface GetServiceGrafanaUserConfigAuthGenericOauth {
    allowSignUp?: string;
    allowedDomains?: string[];
    allowedOrganizations?: string[];
    apiUrl?: string;
    authUrl?: string;
    clientId?: string;
    clientSecret?: string;
    name?: string;
    scopes?: string[];
    tokenUrl?: string;
}

export interface GetServiceGrafanaUserConfigAuthGithub {
    allowSignUp?: string;
    allowedOrganizations?: string[];
    clientId?: string;
    clientSecret?: string;
    teamIds?: string[];
}

export interface GetServiceGrafanaUserConfigAuthGitlab {
    allowSignUp?: string;
    allowedGroups?: string[];
    apiUrl?: string;
    authUrl?: string;
    clientId?: string;
    clientSecret?: string;
    tokenUrl?: string;
}

export interface GetServiceGrafanaUserConfigAuthGoogle {
    allowSignUp?: string;
    allowedDomains?: string[];
    clientId?: string;
    clientSecret?: string;
}

export interface GetServiceGrafanaUserConfigDateFormats {
    defaultTimezone?: string;
    fullDate?: string;
    intervalDay?: string;
    intervalHour?: string;
    intervalMinute?: string;
    intervalMonth?: string;
    intervalSecond?: string;
    intervalYear?: string;
}

export interface GetServiceGrafanaUserConfigExternalImageStorage {
    accessKey?: string;
    bucketUrl?: string;
    provider?: string;
    secretKey?: string;
}

export interface GetServiceGrafanaUserConfigPrivateAccess {
    grafana?: string;
}

export interface GetServiceGrafanaUserConfigPrivatelinkAccess {
    grafana?: string;
}

export interface GetServiceGrafanaUserConfigPublicAccess {
    grafana?: string;
}

export interface GetServiceGrafanaUserConfigSmtpServer {
    fromAddress?: string;
    fromName?: string;
    host?: string;
    password?: string;
    port?: string;
    skipVerify?: string;
    starttlsPolicy?: string;
    username?: string;
}

export interface GetServiceInfluxdb {
    databaseName: string;
}

export interface GetServiceInfluxdbUserConfig {
    customDomain?: string;
    influxdb?: outputs.GetServiceInfluxdbUserConfigInfluxdb;
    ipFilters?: string[];
    privateAccess?: outputs.GetServiceInfluxdbUserConfigPrivateAccess;
    privatelinkAccess?: outputs.GetServiceInfluxdbUserConfigPrivatelinkAccess;
    projectToForkFrom?: string;
    publicAccess?: outputs.GetServiceInfluxdbUserConfigPublicAccess;
    recoveryBasebackupName?: string;
    serviceToForkFrom?: string;
    staticIps?: string;
}

export interface GetServiceInfluxdbUserConfigInfluxdb {
    logQueriesAfter?: string;
    maxConnectionLimit?: string;
    maxRowLimit?: string;
    maxSelectBuckets?: string;
    maxSelectPoint?: string;
    queryTimeout?: string;
}

export interface GetServiceInfluxdbUserConfigPrivateAccess {
    influxdb?: string;
}

export interface GetServiceInfluxdbUserConfigPrivatelinkAccess {
    influxdb?: string;
}

export interface GetServiceInfluxdbUserConfigPublicAccess {
    influxdb?: string;
}

export interface GetServiceIntegrationDashboardUserConfig {
}

export interface GetServiceIntegrationDatadogUserConfig {
    datadogTags?: outputs.GetServiceIntegrationDatadogUserConfigDatadogTag[];
    excludeConsumerGroups?: string[];
    excludeTopics?: string[];
    includeConsumerGroups?: string[];
    includeTopics?: string[];
    kafkaCustomMetrics?: string[];
    maxJmxMetrics?: string;
}

export interface GetServiceIntegrationDatadogUserConfigDatadogTag {
    comment?: string;
    tag?: string;
}

export interface GetServiceIntegrationEndpointDatadogUserConfig {
    datadogApiKey?: string;
    datadogTags?: outputs.GetServiceIntegrationEndpointDatadogUserConfigDatadogTag[];
    disableConsumerStats?: string;
    maxPartitionContexts?: string;
    site?: string;
}

export interface GetServiceIntegrationEndpointDatadogUserConfigDatadogTag {
    comment?: string;
    tag?: string;
}

export interface GetServiceIntegrationEndpointExternalAwsCloudwatchLogsUserConfig {
    accessKey?: string;
    logGroupName?: string;
    region?: string;
    secretKey?: string;
}

export interface GetServiceIntegrationEndpointExternalAwsCloudwatchMetricsUserConfig {
    accessKey?: string;
    namespace?: string;
    region?: string;
    secretKey?: string;
}

export interface GetServiceIntegrationEndpointExternalElasticsearchLogsUserConfig {
    ca?: string;
    indexDaysMax?: string;
    indexPrefix?: string;
    timeout?: string;
    url?: string;
}

export interface GetServiceIntegrationEndpointExternalGoogleCloudLoggingUserConfig {
    logId?: string;
    projectId?: string;
    serviceAccountCredentials?: string;
}

export interface GetServiceIntegrationEndpointExternalKafkaUserConfig {
    bootstrapServers?: string;
    saslMechanism?: string;
    saslPlainPassword?: string;
    saslPlainUsername?: string;
    securityProtocol?: string;
    sslCaCert?: string;
    sslClientCert?: string;
    sslClientKey?: string;
    sslEndpointIdentificationAlgorithm?: string;
}

export interface GetServiceIntegrationEndpointExternalSchemaRegistryUserConfig {
    authentication?: string;
    basicAuthPassword?: string;
    basicAuthUsername?: string;
    url?: string;
}

export interface GetServiceIntegrationEndpointJolokiaUserConfig {
    basicAuthPassword?: string;
    basicAuthUsername?: string;
}

export interface GetServiceIntegrationEndpointPrometheusUserConfig {
    basicAuthPassword?: string;
    basicAuthUsername?: string;
}

export interface GetServiceIntegrationEndpointRsyslogUserConfig {
    ca?: string;
    cert?: string;
    format?: string;
    key?: string;
    logline?: string;
    port?: string;
    sd?: string;
    server?: string;
    tls?: string;
}

export interface GetServiceIntegrationEndpointSignalfxUserConfig {
    enabledMetrics?: string[];
    signalfxApiKey?: string;
    signalfxRealm?: string;
}

export interface GetServiceIntegrationExternalAwsCloudwatchLogsUserConfig {
}

export interface GetServiceIntegrationExternalAwsCloudwatchMetricsUserConfig {
    droppedMetrics?: outputs.GetServiceIntegrationExternalAwsCloudwatchMetricsUserConfigDroppedMetric[];
    extraMetrics?: outputs.GetServiceIntegrationExternalAwsCloudwatchMetricsUserConfigExtraMetric[];
}

export interface GetServiceIntegrationExternalAwsCloudwatchMetricsUserConfigDroppedMetric {
    field?: string;
    metric?: string;
}

export interface GetServiceIntegrationExternalAwsCloudwatchMetricsUserConfigExtraMetric {
    field?: string;
    metric?: string;
}

export interface GetServiceIntegrationExternalElasticsearchLogsUserConfig {
}

export interface GetServiceIntegrationExternalGoogleCloudLoggingUserConfig {
}

export interface GetServiceIntegrationKafkaConnectUserConfig {
    kafkaConnect?: outputs.GetServiceIntegrationKafkaConnectUserConfigKafkaConnect;
}

export interface GetServiceIntegrationKafkaConnectUserConfigKafkaConnect {
    configStorageTopic?: string;
    groupId?: string;
    offsetStorageTopic?: string;
    statusStorageTopic?: string;
}

export interface GetServiceIntegrationKafkaLogsUserConfig {
    kafkaTopic?: string;
}

export interface GetServiceIntegrationKafkaMirrormakerUserConfig {
    clusterAlias?: string;
}

export interface GetServiceIntegrationLogsUserConfig {
    elasticsearchIndexDaysMax?: string;
    elasticsearchIndexPrefix?: string;
}

export interface GetServiceIntegrationM3aggregatorUserConfig {
}

export interface GetServiceIntegrationM3coordinatorUserConfig {
}

export interface GetServiceIntegrationMetricsUserConfig {
    database?: string;
    retentionDays?: string;
    roUsername?: string;
    sourceMysql?: outputs.GetServiceIntegrationMetricsUserConfigSourceMysql;
    username?: string;
}

export interface GetServiceIntegrationMetricsUserConfigSourceMysql {
    telegraf?: outputs.GetServiceIntegrationMetricsUserConfigSourceMysqlTelegraf;
}

export interface GetServiceIntegrationMetricsUserConfigSourceMysqlTelegraf {
    gatherEventWaits?: string;
    gatherFileEventsStats?: string;
    gatherIndexIoWaits?: string;
    gatherInfoSchemaAutoInc?: string;
    gatherInnodbMetrics?: string;
    gatherPerfEventsStatements?: string;
    gatherProcessList?: string;
    gatherSlaveStatus?: string;
    gatherTableIoWaits?: string;
    gatherTableLockWaits?: string;
    gatherTableSchema?: string;
    perfEventsStatementsDigestTextLimit?: string;
    perfEventsStatementsLimit?: string;
    perfEventsStatementsTimeLimit?: string;
}

export interface GetServiceIntegrationMirrormakerUserConfig {
    mirrormakerWhitelist?: string;
}

export interface GetServiceIntegrationPrometheusUserConfig {
    sourceMysql?: outputs.GetServiceIntegrationPrometheusUserConfigSourceMysql;
}

export interface GetServiceIntegrationPrometheusUserConfigSourceMysql {
    telegraf?: outputs.GetServiceIntegrationPrometheusUserConfigSourceMysqlTelegraf;
}

export interface GetServiceIntegrationPrometheusUserConfigSourceMysqlTelegraf {
    gatherEventWaits?: string;
    gatherFileEventsStats?: string;
    gatherIndexIoWaits?: string;
    gatherInfoSchemaAutoInc?: string;
    gatherInnodbMetrics?: string;
    gatherPerfEventsStatements?: string;
    gatherProcessList?: string;
    gatherSlaveStatus?: string;
    gatherTableIoWaits?: string;
    gatherTableLockWaits?: string;
    gatherTableSchema?: string;
    perfEventsStatementsDigestTextLimit?: string;
    perfEventsStatementsLimit?: string;
    perfEventsStatementsTimeLimit?: string;
}

export interface GetServiceIntegrationReadReplicaUserConfig {
}

export interface GetServiceIntegrationRsyslogUserConfig {
}

export interface GetServiceIntegrationSchemaRegistryProxyUserConfig {
}

export interface GetServiceIntegrationSignalfxUserConfig {
}

export interface GetServiceKafka {
    accessCert: string;
    accessKey: string;
    connectUri: string;
    restUri: string;
    schemaRegistryUri: string;
}

export interface GetServiceKafkaConnect {
}

export interface GetServiceKafkaConnectUserConfig {
    ipFilters?: string[];
    kafkaConnect?: outputs.GetServiceKafkaConnectUserConfigKafkaConnect;
    privateAccess?: outputs.GetServiceKafkaConnectUserConfigPrivateAccess;
    privatelinkAccess?: outputs.GetServiceKafkaConnectUserConfigPrivatelinkAccess;
    publicAccess?: outputs.GetServiceKafkaConnectUserConfigPublicAccess;
    staticIps?: string;
}

export interface GetServiceKafkaConnectUserConfigKafkaConnect {
    connectorClientConfigOverridePolicy?: string;
    consumerAutoOffsetReset?: string;
    consumerFetchMaxBytes?: string;
    consumerIsolationLevel?: string;
    consumerMaxPartitionFetchBytes?: string;
    consumerMaxPollIntervalMs?: string;
    consumerMaxPollRecords?: string;
    offsetFlushIntervalMs?: string;
    offsetFlushTimeoutMs?: string;
    producerMaxRequestSize?: string;
    sessionTimeoutMs?: string;
}

export interface GetServiceKafkaConnectUserConfigPrivateAccess {
    kafkaConnect?: string;
    prometheus?: string;
}

export interface GetServiceKafkaConnectUserConfigPrivatelinkAccess {
    kafkaConnect?: string;
}

export interface GetServiceKafkaConnectUserConfigPublicAccess {
    kafkaConnect?: string;
    prometheus?: string;
}

export interface GetServiceKafkaMirrormaker {
}

export interface GetServiceKafkaMirrormakerUserConfig {
    ipFilters?: string[];
    kafkaMirrormaker?: outputs.GetServiceKafkaMirrormakerUserConfigKafkaMirrormaker;
    staticIps?: string;
}

export interface GetServiceKafkaMirrormakerUserConfigKafkaMirrormaker {
    emitCheckpointsEnabled?: string;
    emitCheckpointsIntervalSeconds?: string;
    refreshGroupsEnabled?: string;
    refreshGroupsIntervalSeconds?: string;
    refreshTopicsEnabled?: string;
    refreshTopicsIntervalSeconds?: string;
    syncGroupOffsetsEnabled?: string;
    syncGroupOffsetsIntervalSeconds?: string;
    syncTopicConfigsEnabled?: string;
    tasksMaxPerCpu?: string;
}

export interface GetServiceKafkaUserConfig {
    customDomain?: string;
    ipFilters?: string[];
    kafka?: outputs.GetServiceKafkaUserConfigKafka;
    kafkaAuthenticationMethods?: outputs.GetServiceKafkaUserConfigKafkaAuthenticationMethods;
    kafkaConnect?: string;
    kafkaConnectConfig?: outputs.GetServiceKafkaUserConfigKafkaConnectConfig;
    kafkaRest?: string;
    kafkaRestConfig?: outputs.GetServiceKafkaUserConfigKafkaRestConfig;
    kafkaVersion?: string;
    privateAccess?: outputs.GetServiceKafkaUserConfigPrivateAccess;
    privatelinkAccess?: outputs.GetServiceKafkaUserConfigPrivatelinkAccess;
    publicAccess?: outputs.GetServiceKafkaUserConfigPublicAccess;
    schemaRegistry?: string;
    schemaRegistryConfig?: outputs.GetServiceKafkaUserConfigSchemaRegistryConfig;
    staticIps?: string;
}

export interface GetServiceKafkaUserConfigKafka {
    autoCreateTopicsEnable?: string;
    compressionType?: string;
    connectionsMaxIdleMs?: string;
    defaultReplicationFactor?: string;
    groupInitialRebalanceDelayMs?: string;
    groupMaxSessionTimeoutMs?: string;
    groupMinSessionTimeoutMs?: string;
    logCleanerDeleteRetentionMs?: string;
    logCleanerMaxCompactionLagMs?: string;
    logCleanerMinCleanableRatio?: string;
    logCleanerMinCompactionLagMs?: string;
    logCleanupPolicy?: string;
    logFlushIntervalMessages?: string;
    logFlushIntervalMs?: string;
    logIndexIntervalBytes?: string;
    logIndexSizeMaxBytes?: string;
    logMessageDownconversionEnable?: string;
    logMessageTimestampDifferenceMaxMs?: string;
    logMessageTimestampType?: string;
    logPreallocate?: string;
    logRetentionBytes?: string;
    logRetentionHours?: string;
    logRetentionMs?: string;
    logRollJitterMs?: string;
    logRollMs?: string;
    logSegmentBytes?: string;
    logSegmentDeleteDelayMs?: string;
    maxConnectionsPerIp?: string;
    maxIncrementalFetchSessionCacheSlots?: string;
    messageMaxBytes?: string;
    minInsyncReplicas?: string;
    numPartitions?: string;
    offsetsRetentionMinutes?: string;
    producerPurgatoryPurgeIntervalRequests?: string;
    replicaFetchMaxBytes?: string;
    replicaFetchResponseMaxBytes?: string;
    socketRequestMaxBytes?: string;
    transactionRemoveExpiredTransactionCleanupIntervalMs?: string;
    transactionStateLogSegmentBytes?: string;
}

export interface GetServiceKafkaUserConfigKafkaAuthenticationMethods {
    certificate?: string;
    sasl?: string;
}

export interface GetServiceKafkaUserConfigKafkaConnectConfig {
    connectorClientConfigOverridePolicy?: string;
    consumerAutoOffsetReset?: string;
    consumerFetchMaxBytes?: string;
    consumerIsolationLevel?: string;
    consumerMaxPartitionFetchBytes?: string;
    consumerMaxPollIntervalMs?: string;
    consumerMaxPollRecords?: string;
    offsetFlushIntervalMs?: string;
    offsetFlushTimeoutMs?: string;
    producerMaxRequestSize?: string;
    sessionTimeoutMs?: string;
}

export interface GetServiceKafkaUserConfigKafkaRestConfig {
    consumerEnableAutoCommit?: string;
    consumerRequestMaxBytes?: string;
    consumerRequestTimeoutMs?: string;
    producerAcks?: string;
    producerLingerMs?: string;
    simpleconsumerPoolSizeMax?: string;
}

export interface GetServiceKafkaUserConfigPrivateAccess {
    prometheus?: string;
}

export interface GetServiceKafkaUserConfigPrivatelinkAccess {
    kafka?: string;
    kafkaConnect?: string;
    kafkaRest?: string;
    schemaRegistry?: string;
}

export interface GetServiceKafkaUserConfigPublicAccess {
    kafka?: string;
    kafkaConnect?: string;
    kafkaRest?: string;
    prometheus?: string;
    schemaRegistry?: string;
}

export interface GetServiceKafkaUserConfigSchemaRegistryConfig {
    leaderEligibility?: string;
    topicName?: string;
}

export interface GetServiceMysql {
}

export interface GetServiceMysqlUserConfig {
    adminPassword?: string;
    adminUsername?: string;
    backupHour?: string;
    backupMinute?: string;
    binlogRetentionPeriod?: string;
    ipFilters?: string[];
    migration?: outputs.GetServiceMysqlUserConfigMigration;
    mysql?: outputs.GetServiceMysqlUserConfigMysql;
    mysqlVersion?: string;
    privateAccess?: outputs.GetServiceMysqlUserConfigPrivateAccess;
    privatelinkAccess?: outputs.GetServiceMysqlUserConfigPrivatelinkAccess;
    projectToForkFrom?: string;
    publicAccess?: outputs.GetServiceMysqlUserConfigPublicAccess;
    recoveryTargetTime?: string;
    serviceToForkFrom?: string;
    staticIps?: string;
}

export interface GetServiceMysqlUserConfigMigration {
    dbname?: string;
    host?: string;
    ignoreDbs?: string;
    password?: string;
    port?: string;
    ssl?: string;
    username?: string;
}

export interface GetServiceMysqlUserConfigMysql {
    connectTimeout?: string;
    defaultTimeZone?: string;
    groupConcatMaxLen?: string;
    informationSchemaStatsExpiry?: string;
    innodbFtMinTokenSize?: string;
    innodbFtServerStopwordTable?: string;
    innodbLockWaitTimeout?: string;
    innodbLogBufferSize?: string;
    innodbOnlineAlterLogMaxSize?: string;
    innodbPrintAllDeadlocks?: string;
    innodbRollbackOnTimeout?: string;
    interactiveTimeout?: string;
    internalTmpMemStorageEngine?: string;
    longQueryTime?: string;
    maxAllowedPacket?: string;
    maxHeapTableSize?: string;
    netReadTimeout?: string;
    netWriteTimeout?: string;
    slowQueryLog?: string;
    sortBufferSize?: string;
    sqlMode?: string;
    sqlRequirePrimaryKey?: string;
    tmpTableSize?: string;
    waitTimeout?: string;
}

export interface GetServiceMysqlUserConfigPrivateAccess {
    mysql?: string;
    mysqlx?: string;
    prometheus?: string;
}

export interface GetServiceMysqlUserConfigPrivatelinkAccess {
    mysql?: string;
    mysqlx?: string;
}

export interface GetServiceMysqlUserConfigPublicAccess {
    mysql?: string;
    mysqlx?: string;
    prometheus?: string;
}

export interface GetServiceOpensearch {
    opensearchDashboardsUri: string;
}

export interface GetServiceOpensearchUserConfig {
    customDomain?: string;
    disableReplicationFactorAdjustment?: string;
    indexPatterns?: outputs.GetServiceOpensearchUserConfigIndexPattern[];
    indexTemplate?: outputs.GetServiceOpensearchUserConfigIndexTemplate;
    ipFilters?: string[];
    keepIndexRefreshInterval?: string;
    maxIndexCount?: string;
    opensearch?: outputs.GetServiceOpensearchUserConfigOpensearch;
    opensearchDashboards?: outputs.GetServiceOpensearchUserConfigOpensearchDashboards;
    opensearchVersion?: string;
    privateAccess?: outputs.GetServiceOpensearchUserConfigPrivateAccess;
    privatelinkAccess?: outputs.GetServiceOpensearchUserConfigPrivatelinkAccess;
    projectToForkFrom?: string;
    publicAccess?: outputs.GetServiceOpensearchUserConfigPublicAccess;
    recoveryBasebackupName?: string;
    serviceToForkFrom?: string;
    staticIps?: string;
}

export interface GetServiceOpensearchUserConfigIndexPattern {
    maxIndexCount?: string;
    pattern?: string;
    sortingAlgorithm?: string;
}

export interface GetServiceOpensearchUserConfigIndexTemplate {
    mappingNestedObjectsLimit?: string;
    numberOfReplicas?: string;
    numberOfShards?: string;
}

export interface GetServiceOpensearchUserConfigOpensearch {
    actionAutoCreateIndexEnabled?: string;
    actionDestructiveRequiresName?: string;
    clusterMaxShardsPerNode?: string;
    httpMaxContentLength?: string;
    httpMaxHeaderSize?: string;
    httpMaxInitialLineLength?: string;
    indicesFielddataCacheSize?: string;
    indicesMemoryIndexBufferSize?: string;
    indicesQueriesCacheSize?: string;
    indicesQueryBoolMaxClauseCount?: string;
    reindexRemoteWhitelists?: string[];
    searchMaxBuckets?: string;
    threadPoolAnalyzeQueueSize?: string;
    threadPoolAnalyzeSize?: string;
    threadPoolForceMergeSize?: string;
    threadPoolGetQueueSize?: string;
    threadPoolGetSize?: string;
    threadPoolIndexSize?: string;
    threadPoolSearchQueueSize?: string;
    threadPoolSearchSize?: string;
    threadPoolSearchThrottledQueueSize?: string;
    threadPoolSearchThrottledSize?: string;
    threadPoolWriteQueueSize?: string;
    threadPoolWriteSize?: string;
}

export interface GetServiceOpensearchUserConfigOpensearchDashboards {
    enabled?: string;
    maxOldSpaceSize?: string;
    opensearchRequestTimeout?: string;
}

export interface GetServiceOpensearchUserConfigPrivateAccess {
    opensearch?: string;
    opensearchDashboards?: string;
    prometheus?: string;
}

export interface GetServiceOpensearchUserConfigPrivatelinkAccess {
    opensearch?: string;
    opensearchDashboards?: string;
}

export interface GetServiceOpensearchUserConfigPublicAccess {
    opensearch?: string;
    opensearchDashboards?: string;
    prometheus?: string;
}

export interface GetServicePg {
    dbname: string;
    host: string;
    password: string;
    port: number;
    replicaUri: string;
    sslmode: string;
    uri: string;
    user: string;
}

export interface GetServicePgUserConfig {
    adminPassword?: string;
    adminUsername?: string;
    backupHour?: string;
    backupMinute?: string;
    ipFilters?: string[];
    migration?: outputs.GetServicePgUserConfigMigration;
    pg?: outputs.GetServicePgUserConfigPg;
    pgReadReplica?: string;
    pgServiceToForkFrom?: string;
    pgVersion?: string;
    pgbouncer?: outputs.GetServicePgUserConfigPgbouncer;
    pglookout?: outputs.GetServicePgUserConfigPglookout;
    privateAccess?: outputs.GetServicePgUserConfigPrivateAccess;
    privatelinkAccess?: outputs.GetServicePgUserConfigPrivatelinkAccess;
    projectToForkFrom?: string;
    publicAccess?: outputs.GetServicePgUserConfigPublicAccess;
    recoveryTargetTime?: string;
    serviceToForkFrom?: string;
    sharedBuffersPercentage?: string;
    staticIps?: string;
    synchronousReplication?: string;
    timescaledb?: outputs.GetServicePgUserConfigTimescaledb;
    variant?: string;
    workMem?: string;
}

export interface GetServicePgUserConfigMigration {
    dbname?: string;
    host?: string;
    ignoreDbs?: string;
    password?: string;
    port?: string;
    ssl?: string;
    username?: string;
}

export interface GetServicePgUserConfigPg {
    autovacuumAnalyzeScaleFactor?: string;
    autovacuumAnalyzeThreshold?: string;
    autovacuumFreezeMaxAge?: string;
    autovacuumMaxWorkers?: string;
    autovacuumNaptime?: string;
    autovacuumVacuumCostDelay?: string;
    autovacuumVacuumCostLimit?: string;
    autovacuumVacuumScaleFactor?: string;
    autovacuumVacuumThreshold?: string;
    bgwriterDelay?: string;
    bgwriterFlushAfter?: string;
    bgwriterLruMaxpages?: string;
    bgwriterLruMultiplier?: string;
    deadlockTimeout?: string;
    idleInTransactionSessionTimeout?: string;
    jit?: string;
    logAutovacuumMinDuration?: string;
    logErrorVerbosity?: string;
    logLinePrefix?: string;
    logMinDurationStatement?: string;
    maxFilesPerProcess?: string;
    maxLocksPerTransaction?: string;
    maxLogicalReplicationWorkers?: string;
    maxParallelWorkers?: string;
    maxParallelWorkersPerGather?: string;
    maxPredLocksPerTransaction?: string;
    maxPreparedTransactions?: string;
    maxReplicationSlots?: string;
    maxStackDepth?: string;
    maxStandbyArchiveDelay?: string;
    maxStandbyStreamingDelay?: string;
    maxWalSenders?: string;
    maxWorkerProcesses?: string;
    pgPartmanBgwInterval?: string;
    pgPartmanBgwRole?: string;
    pgStatStatementsTrack?: string;
    tempFileLimit?: string;
    timezone?: string;
    trackActivityQuerySize?: string;
    trackCommitTimestamp?: string;
    trackFunctions?: string;
    trackIoTiming?: string;
    walSenderTimeout?: string;
    walWriterDelay?: string;
}

export interface GetServicePgUserConfigPgbouncer {
    autodbIdleTimeout?: string;
    autodbMaxDbConnections?: string;
    autodbPoolMode?: string;
    autodbPoolSize?: string;
    ignoreStartupParameters?: string[];
    minPoolSize?: string;
    serverIdleTimeout?: string;
    serverLifetime?: string;
    serverResetQueryAlways?: string;
}

export interface GetServicePgUserConfigPglookout {
    maxFailoverReplicationTimeLag?: string;
}

export interface GetServicePgUserConfigPrivateAccess {
    pg?: string;
    pgbouncer?: string;
    prometheus?: string;
}

export interface GetServicePgUserConfigPrivatelinkAccess {
    pg?: string;
    pgbouncer?: string;
}

export interface GetServicePgUserConfigPublicAccess {
    pg?: string;
    pgbouncer?: string;
    prometheus?: string;
}

export interface GetServicePgUserConfigTimescaledb {
    maxBackgroundWorkers?: string;
}

export interface GetServiceRedis {
}

export interface GetServiceRedisUserConfig {
    ipFilters?: string[];
    migration?: outputs.GetServiceRedisUserConfigMigration;
    privateAccess?: outputs.GetServiceRedisUserConfigPrivateAccess;
    privatelinkAccess?: outputs.GetServiceRedisUserConfigPrivatelinkAccess;
    projectToForkFrom?: string;
    publicAccess?: outputs.GetServiceRedisUserConfigPublicAccess;
    recoveryBasebackupName?: string;
    redisAclChannelsDefault?: string;
    redisIoThreads?: string;
    redisLfuDecayTime?: string;
    redisLfuLogFactor?: string;
    redisMaxmemoryPolicy?: string;
    redisNotifyKeyspaceEvents?: string;
    redisNumberOfDatabases?: string;
    redisPersistence?: string;
    redisPubsubClientOutputBufferLimit?: string;
    redisSsl?: string;
    redisTimeout?: string;
    serviceToForkFrom?: string;
    staticIps?: string;
}

export interface GetServiceRedisUserConfigMigration {
    dbname?: string;
    host?: string;
    ignoreDbs?: string;
    password?: string;
    port?: string;
    ssl?: string;
    username?: string;
}

export interface GetServiceRedisUserConfigPrivateAccess {
    prometheus?: string;
    redis?: string;
}

export interface GetServiceRedisUserConfigPrivatelinkAccess {
    redis?: string;
}

export interface GetServiceRedisUserConfigPublicAccess {
    prometheus?: string;
    redis?: string;
}

export interface GetServiceServiceIntegration {
    integrationType: string;
    sourceServiceName: string;
}

export interface GrafanaComponent {
    component: string;
    /**
     * Server hostname or IP
     */
    host: string;
    kafkaAuthenticationMethod: string;
    /**
     * SMTP server port
     */
    port: number;
    route: string;
    ssl: boolean;
    usage: string;
}

export interface GrafanaGrafana {
}

export interface GrafanaGrafanaUserConfig {
    /**
     * Enable or disable Grafana alerting functionality
     */
    alertingEnabled?: string;
    /**
     * Default error or timeout setting for new alerting rules
     */
    alertingErrorOrTimeout?: string;
    alertingMaxAnnotationsToKeep?: string;
    /**
     * Default value for 'no data or null values' for
     * new alerting rules
     */
    alertingNodataOrNullvalues?: string;
    /**
     * Allow embedding Grafana dashboards with iframe/frame/object/embed 
     * tags. Disabled by default to limit impact of clickjacking
     */
    allowEmbedding?: string;
    authAzuread?: outputs.GrafanaGrafanaUserConfigAuthAzuread;
    /**
     * Enable or disable basic authentication form, used by Grafana 
     * built-in login.
     */
    authBasicEnabled?: string;
    /**
     * Generic OAuth integration.
     */
    authGenericOauth?: outputs.GrafanaGrafanaUserConfigAuthGenericOauth;
    /**
     * Automatically sign-up users on successful sign-in
     */
    authGithub?: outputs.GrafanaGrafanaUserConfigAuthGithub;
    /**
     * GitLab Auth integration.
     */
    authGitlab?: outputs.GrafanaGrafanaUserConfigAuthGitlab;
    /**
     * Google Auth integration
     */
    authGoogle?: outputs.GrafanaGrafanaUserConfigAuthGoogle;
    /**
     * Cookie SameSite attribute: 'strict' prevents sending cookie for 
     * cross-site requests, effectively disabling direct linking from other sites to Grafana. 'lax' is the default value.
     */
    cookieSamesite?: string;
    /**
     * Serve the web frontend using a custom CNAME pointing to the Aiven DNS name.
     */
    customDomain?: string;
    /**
     * Signed sequence of decimal numbers, followed 
     * by a unit suffix (ms, s, m, h, d), e.g. 30s, 1h.
     */
    dashboardsMinRefreshInterval?: string;
    /**
     * Dashboard versions to keep per dashboard.
     */
    dashboardsVersionsToKeep?: string;
    /**
     * Send 'X-Grafana-User' header to data source.
     */
    dataproxySendUserHeader?: string;
    /**
     * Timeout for data proxy requests in seconds.
     */
    dataproxyTimeout?: string;
    dateFormats?: outputs.GrafanaGrafanaUserConfigDateFormats;
    /**
     * Set to true to disable gravatar. Defaults to false 
     * (gravatar is enabled).
     */
    disableGravatar?: string;
    /**
     * Editors can manage folders, teams and dashboards created by them.
     */
    editorsCanAdmin?: string;
    /**
     * External image store settings
     */
    externalImageStorage?: outputs.GrafanaGrafanaUserConfigExternalImageStorage;
    /**
     * Google Analytics Universal Analytics ID for tracking Grafana usage
     */
    googleAnalyticsUaId?: string;
    /**
     * Allow incoming connections from CIDR address block, e.g. '10.20.0.0/16'
     */
    ipFilters?: string[];
    /**
     * Enable Grafana /metrics endpoint
     */
    metricsEnabled?: string;
    privateAccess?: outputs.GrafanaGrafanaUserConfigPrivateAccess;
    /**
     * Allow access to selected service components through Privatelink
     */
    privatelinkAccess?: outputs.GrafanaGrafanaUserConfigPrivatelinkAccess;
    /**
     * Name of another project to fork a service from. This has 
     * effect only when a new service is being created.
     */
    projectToForkFrom?: string;
    /**
     * Allow access to selected service ports from the public Internet.
     */
    publicAccess?: outputs.GrafanaGrafanaUserConfigPublicAccess;
    /**
     * Name of the basebackup to restore in forked service.
     */
    recoveryBasebackupName?: string;
    /**
     * Name of another service to fork from. This has effect only 
     * when a new service is being created.
     */
    serviceToForkFrom?: string;
    /**
     * SMTP server settings.
     */
    smtpServer?: outputs.GrafanaGrafanaUserConfigSmtpServer;
    staticIps?: string;
    /**
     * Auto-assign new users on signup to main organization. 
     * Defaults to false.
     */
    userAutoAssignOrg?: string;
    /**
     * Set role for new signups. Defaults to Viewer.
     */
    userAutoAssignOrgRole?: string;
    /**
     * Users with view-only permission can edit but not save dashboards.
     */
    viewersCanEdit?: string;
}

export interface GrafanaGrafanaUserConfigAuthAzuread {
    /**
     * Automatically sign-up users on successful sign-in
     */
    allowSignUp?: string;
    /**
     * Allowed domain
     */
    allowedDomains?: string[];
    /**
     * Require users to belong to one of given groups
     */
    allowedGroups?: string[];
    /**
     * Authorization URL. This only needs to be set when using self hosted GitLab
     */
    authUrl?: string;
    /**
     * Client ID from provider
     */
    clientId?: string;
    /**
     * Client secret from provider
     */
    clientSecret?: string;
    /**
     * Token URL. This only needs to be set when using self hosted GitLab
     */
    tokenUrl?: string;
}

export interface GrafanaGrafanaUserConfigAuthGenericOauth {
    /**
     * Automatically sign-up users on successful sign-in
     */
    allowSignUp?: string;
    /**
     * Allowed domain
     */
    allowedDomains?: string[];
    /**
     * Must consist of alpha-numeric characters and dashes"
     */
    allowedOrganizations?: string[];
    /**
     * API URL. This only needs to be set when using self hosted GitLab
     */
    apiUrl?: string;
    /**
     * Authorization URL. This only needs to be set when using self hosted GitLab
     */
    authUrl?: string;
    /**
     * Client ID from provider
     */
    clientId?: string;
    /**
     * Client secret from provider
     */
    clientSecret?: string;
    /**
     * Name of the OAuth integration
     */
    name?: string;
    /**
     * Scope must be non-empty string without whitespace
     */
    scopes?: string[];
    /**
     * Token URL. This only needs to be set when using self hosted GitLab
     */
    tokenUrl?: string;
}

export interface GrafanaGrafanaUserConfigAuthGithub {
    /**
     * Automatically sign-up users on successful sign-in
     */
    allowSignUp?: string;
    /**
     * Must consist of alpha-numeric characters and dashes"
     */
    allowedOrganizations?: string[];
    /**
     * Client ID from provider
     */
    clientId?: string;
    /**
     * Client secret from provider
     */
    clientSecret?: string;
    /**
     * Require users to belong to one of given team IDs
     */
    teamIds?: string[];
}

export interface GrafanaGrafanaUserConfigAuthGitlab {
    /**
     * Automatically sign-up users on successful sign-in
     */
    allowSignUp?: string;
    /**
     * Require users to belong to one of given groups
     */
    allowedGroups?: string[];
    /**
     * API URL. This only needs to be set when using self hosted GitLab
     */
    apiUrl?: string;
    /**
     * Authorization URL. This only needs to be set when using self hosted GitLab
     */
    authUrl?: string;
    /**
     * Client ID from provider
     */
    clientId?: string;
    /**
     * Client secret from provider
     */
    clientSecret?: string;
    /**
     * Token URL. This only needs to be set when using self hosted GitLab
     */
    tokenUrl?: string;
}

export interface GrafanaGrafanaUserConfigAuthGoogle {
    /**
     * Automatically sign-up users on successful sign-in
     */
    allowSignUp?: string;
    /**
     * Allowed domain
     */
    allowedDomains?: string[];
    /**
     * Client ID from provider
     */
    clientId?: string;
    /**
     * Client secret from provider
     */
    clientSecret?: string;
}

export interface GrafanaGrafanaUserConfigDateFormats {
    defaultTimezone?: string;
    fullDate?: string;
    intervalDay?: string;
    intervalHour?: string;
    intervalMinute?: string;
    intervalMonth?: string;
    intervalSecond?: string;
    intervalYear?: string;
}

export interface GrafanaGrafanaUserConfigExternalImageStorage {
    /**
     * S3 access key. Requires permissions to the S3 bucket for the 
     * s3:PutObject and s3:PutObjectAcl actions
     */
    accessKey?: string;
    /**
     * Bucket URL for S3
     */
    bucketUrl?: string;
    /**
     * Provider type
     */
    provider?: string;
    /**
     * S3 secret key
     */
    secretKey?: string;
}

export interface GrafanaGrafanaUserConfigPrivateAccess {
    /**
     * Allow clients to connect to grafana from the public internet for service nodes that 
     * are in a project VPC or another type of private network.
     */
    grafana?: string;
}

export interface GrafanaGrafanaUserConfigPrivatelinkAccess {
    /**
     * Allow clients to connect to grafana from the public internet for service nodes that 
     * are in a project VPC or another type of private network.
     */
    grafana?: string;
}

export interface GrafanaGrafanaUserConfigPublicAccess {
    /**
     * Allow clients to connect to grafana from the public internet for service nodes that 
     * are in a project VPC or another type of private network.
     */
    grafana?: string;
}

export interface GrafanaGrafanaUserConfigSmtpServer {
    /**
     * Address used for sending emails
     */
    fromAddress?: string;
    /**
     * Name used in outgoing emails, defaults to Grafana
     */
    fromName?: string;
    /**
     * Server hostname or IP
     */
    host?: string;
    /**
     * Password for SMTP authentication
     */
    password?: string;
    /**
     * SMTP server port
     */
    port?: string;
    /**
     * Skip verifying server certificate. Defaults to false
     */
    skipVerify?: string;
    /**
     * Either OpportunisticStartTLS, MandatoryStartTLS or NoStartTLS. 
     * Default is OpportunisticStartTLS.
     */
    starttlsPolicy?: string;
    /**
     * Username for SMTP authentication
     */
    username?: string;
}

export interface GrafanaServiceIntegration {
    integrationType: string;
    sourceServiceName: string;
}

export interface InfluxDbComponent {
    component: string;
    host: string;
    kafkaAuthenticationMethod: string;
    port: number;
    route: string;
    ssl: boolean;
    usage: string;
}

export interface InfluxDbInfluxdb {
    databaseName: string;
}

export interface InfluxDbInfluxdbUserConfig {
    /**
     * Serve the web frontend using a custom CNAME pointing to the Aiven DNS name
     */
    customDomain?: string;
    /**
     * influxdb.conf configuration values
     */
    influxdb?: outputs.InfluxDbInfluxdbUserConfigInfluxdb;
    /**
     * allow incoming connections from CIDR address block, e.g. `10.20.0.0/16`
     */
    ipFilters?: string[];
    /**
     * Allow access to selected service ports from private networks
     */
    privateAccess?: outputs.InfluxDbInfluxdbUserConfigPrivateAccess;
    /**
     * Allow access to selected service components through Privatelink
     */
    privatelinkAccess?: outputs.InfluxDbInfluxdbUserConfigPrivatelinkAccess;
    /**
     * Name of another project to fork a service from. This has 
     * effect only when a new service is being created.
     */
    projectToForkFrom?: string;
    /**
     * Allow access to selected service ports from the public Internet
     */
    publicAccess?: outputs.InfluxDbInfluxdbUserConfigPublicAccess;
    /**
     * Name of the basebackup to restore in forked service
     */
    recoveryBasebackupName?: string;
    /**
     * Name of another service to fork from. This has effect 
     * only when a new service is being created.
     */
    serviceToForkFrom?: string;
    staticIps?: string;
}

export interface InfluxDbInfluxdbUserConfigInfluxdb {
    /**
     * The maximum duration in seconds before a query is 
     * logged as a slow query. Setting this to 0 (the default) will never log slow queries.
     */
    logQueriesAfter?: string;
    maxConnectionLimit?: string;
    /**
     * The maximum number of rows returned in a non-chunked query. 
     * Setting this to 0 (the default) allows an unlimited number to be returned.
     */
    maxRowLimit?: string;
    /**
     * The maximum number of `GROUP BY time()` buckets that 
     * can be processed in a query. Setting this to 0 (the default) allows an unlimited number to
     * be processed.
     */
    maxSelectBuckets?: string;
    /**
     * The maximum number of points that can be processed in a 
     * SELECT statement. Setting this to 0 (the default) allows an unlimited number to be processed.
     */
    maxSelectPoint?: string;
    /**
     * The maximum duration in seconds before a query is killed. 
     * Setting this to 0 (the default) will never kill slow queries.
     */
    queryTimeout?: string;
}

export interface InfluxDbInfluxdbUserConfigPrivateAccess {
    /**
     * influxdb.conf configuration values
     */
    influxdb?: string;
}

export interface InfluxDbInfluxdbUserConfigPrivatelinkAccess {
    /**
     * influxdb.conf configuration values
     */
    influxdb?: string;
}

export interface InfluxDbInfluxdbUserConfigPublicAccess {
    /**
     * influxdb.conf configuration values
     */
    influxdb?: string;
}

export interface InfluxDbServiceIntegration {
    integrationType: string;
    sourceServiceName: string;
}

export interface KafkaComponent {
    component: string;
    host: string;
    kafkaAuthenticationMethod: string;
    port: number;
    route: string;
    ssl: boolean;
    usage: string;
}

export interface KafkaConnectComponent {
    component: string;
    host: string;
    kafkaAuthenticationMethod: string;
    port: number;
    route: string;
    ssl: boolean;
    usage: string;
}

export interface KafkaConnectKafkaConnect {
}

export interface KafkaConnectKafkaConnectUserConfig {
    /**
     * allow incoming connections from CIDR address block, e.g. `10.20.0.0/16`
     */
    ipFilters?: string[];
    /**
     * Allow clients to connect to kafkaConnect from the public internet for 
     * service nodes that are in a project VPC or another type of private network.
     */
    kafkaConnect?: outputs.KafkaConnectKafkaConnectUserConfigKafkaConnect;
    /**
     * Allow access to selected service ports from private networks.
     */
    privateAccess?: outputs.KafkaConnectKafkaConnectUserConfigPrivateAccess;
    /**
     * Allow access to selected service components through Privatelink
     */
    privatelinkAccess?: outputs.KafkaConnectKafkaConnectUserConfigPrivatelinkAccess;
    /**
     * Allow access to selected service ports from the public Internet.
     */
    publicAccess?: outputs.KafkaConnectKafkaConnectUserConfigPublicAccess;
    staticIps?: string;
}

export interface KafkaConnectKafkaConnectUserConfigKafkaConnect {
    /**
     * Defines what client configurations can be 
     * overridden by the connector. Default is None.
     */
    connectorClientConfigOverridePolicy?: string;
    /**
     * What to do when there is no initial offset in Kafka or 
     * if the current offset does not exist any more on the server. Default is earliest.
     */
    consumerAutoOffsetReset?: string;
    /**
     * Records are fetched in batches by the consumer, and if 
     * the first record batch in the first non-empty partition of the fetch is larger than this value,
     * the record batch will still be returned to ensure that the consumer can make progress. As such,
     * this is not a absolute maximum.
     */
    consumerFetchMaxBytes?: string;
    /**
     * Transaction read isolation level. readUncommitted is 
     * the default, but readCommitted can be used if consume-exactly-once behavior is desired.
     */
    consumerIsolationLevel?: string;
    /**
     * Records are fetched in batches by the consumer.If 
     * the first record batch in the first non-empty partition of the fetch is larger than this limit,
     * the batch will still be returned to ensure that the consumer can make progress.
     */
    consumerMaxPartitionFetchBytes?: string;
    /**
     * The maximum delay in milliseconds between invocations 
     * of poll() when using consumer group management (defaults to 300000).
     */
    consumerMaxPollIntervalMs?: string;
    /**
     * The maximum number of records returned by a single poll.
     */
    consumerMaxPollRecords?: string;
    /**
     * The interval at which to try committing offsets for tasks 
     * (defaults to 60000).
     */
    offsetFlushIntervalMs?: string;
    /**
     * Maximum number of milliseconds to wait for records to flush 
     * and partition offset data to be committed to offset storage before cancelling the process and restoring
     * the offset data to be committed in a future attempt (defaults to 5000).
     */
    offsetFlushTimeoutMs?: string;
    /**
     * This setting will limit the number of record batches the 
     * producer will send in a single request to avoid sending huge requests.
     */
    producerMaxRequestSize?: string;
    /**
     * The timeout in milliseconds used to detect failures when using Kafkaâ€™s 
     * group management facilities (defaults to 10000).
     */
    sessionTimeoutMs?: string;
}

export interface KafkaConnectKafkaConnectUserConfigPrivateAccess {
    /**
     * Allow clients to connect to kafkaConnect from the public internet for 
     * service nodes that are in a project VPC or another type of private network.
     */
    kafkaConnect?: string;
    /**
     * Allow clients to connect to prometheus from the public internet for service 
     * nodes that are in a project VPC or another type of private network.
     */
    prometheus?: string;
}

export interface KafkaConnectKafkaConnectUserConfigPrivatelinkAccess {
    /**
     * Allow clients to connect to kafkaConnect from the public internet for 
     * service nodes that are in a project VPC or another type of private network.
     */
    kafkaConnect?: string;
}

export interface KafkaConnectKafkaConnectUserConfigPublicAccess {
    /**
     * Allow clients to connect to kafkaConnect from the public internet for 
     * service nodes that are in a project VPC or another type of private network.
     */
    kafkaConnect?: string;
    /**
     * Allow clients to connect to prometheus from the public internet for service 
     * nodes that are in a project VPC or another type of private network.
     */
    prometheus?: string;
}

export interface KafkaConnectServiceIntegration {
    integrationType: string;
    sourceServiceName: string;
}

export interface KafkaConnectorTask {
    connector: string;
    /**
     * List of tasks of a connector, each element contains `connector` 
     * (Related connector name) and `task` (Task id / number).
     */
    task: number;
}

export interface KafkaKafka {
    /**
     * The Kafka client certificate
     */
    accessCert: string;
    /**
     * The Kafka client certificate key
     */
    accessKey: string;
    /**
     * The Kafka Connect URI, if any
     */
    connectUri: string;
    /**
     * The Kafka REST URI, if any
     */
    restUri: string;
    /**
     * The Schema Registry URI, if any
     */
    schemaRegistryUri: string;
}

export interface KafkaKafkaUserConfig {
    /**
     * Serve the web frontend using a custom CNAME pointing to the Aiven DNS name.
     */
    customDomain?: string;
    /**
     * Allow incoming connections from CIDR address block, e.g. '10.20.0.0/16'.
     */
    ipFilters?: string[];
    /**
     * Enable kafka
     */
    kafka?: outputs.KafkaKafkaUserConfigKafka;
    /**
     * Kafka authentication methods
     */
    kafkaAuthenticationMethods?: outputs.KafkaKafkaUserConfigKafkaAuthenticationMethods;
    /**
     * Enable kafka_connect
     */
    kafkaConnect?: string;
    /**
     * Kafka Connect configuration values
     */
    kafkaConnectConfig?: outputs.KafkaKafkaUserConfigKafkaConnectConfig;
    /**
     * Enable kafka_rest
     */
    kafkaRest?: string;
    /**
     * Kafka-REST configuration
     */
    kafkaRestConfig?: outputs.KafkaKafkaUserConfigKafkaRestConfig;
    /**
     * Kafka major version
     */
    kafkaVersion?: string;
    /**
     * Allow access to selected service ports from private networks
     */
    privateAccess?: outputs.KafkaKafkaUserConfigPrivateAccess;
    /**
     * Allow access to selected service components through Privatelink
     */
    privatelinkAccess?: outputs.KafkaKafkaUserConfigPrivatelinkAccess;
    /**
     * Allow access to selected service ports from the public Internet
     */
    publicAccess?: outputs.KafkaKafkaUserConfigPublicAccess;
    /**
     * Enable Schema-Registry service
     */
    schemaRegistry?: string;
    /**
     * Schema Registry configuration
     */
    schemaRegistryConfig?: outputs.KafkaKafkaUserConfigSchemaRegistryConfig;
    staticIps?: string;
}

export interface KafkaKafkaUserConfigKafka {
    /**
     * Enable auto creation of topics
     */
    autoCreateTopicsEnable?: string;
    /**
     * Specify the final compression type for a given topic. This 
     * configuration accepts the standard compression codecs ('gzip', 'snappy', 'lz4', 'zstd').
     * It additionally accepts 'uncompressed' which is equivalent to no compression; and 'producer'
     * which means retain the original compression codec set by the producer.
     */
    compressionType?: string;
    /**
     * Idle connections timeout: the server socket processor 
     * threads close the connections that idle for longer than this.
     */
    connectionsMaxIdleMs?: string;
    /**
     * Replication factor for autocreated topics
     */
    defaultReplicationFactor?: string;
    /**
     * The amount of time, in milliseconds, the group 
     * coordinator will wait for more consumers to join a new group before performing the first rebalance.
     * A longer delay means potentially fewer rebalances, but increases the time until processing begins.
     * The default value for this is 3 seconds. During development and testing it might be desirable to set
     * this to 0 in order to not delay test execution time.
     */
    groupInitialRebalanceDelayMs?: string;
    /**
     * The maximum allowed session timeout for registered 
     * consumers. Longer timeouts give consumers more time to process messages in between heartbeats
     * at the cost of a longer time to detect failures.
     */
    groupMaxSessionTimeoutMs?: string;
    /**
     * The minimum allowed session timeout for registered 
     * consumers. Longer timeouts give consumers more time to process messages in between heartbeats
     * at the cost of a longer time to detect failures.
     */
    groupMinSessionTimeoutMs?: string;
    logCleanerDeleteRetentionMs?: string;
    /**
     * The maximum amount of time message will 
     * remain uncompacted. Only applicable for logs that are being compacted
     */
    logCleanerMaxCompactionLagMs?: string;
    /**
     * Controls log compactor frequency. Larger 
     * value means more frequent compactions but also more space wasted for logs. Consider setting
     * log.cleaner.max.compaction.lag.ms to enforce compactions sooner, instead of setting a very
     * high value for this option.
     */
    logCleanerMinCleanableRatio?: string;
    /**
     * The minimum time a message will remain 
     * uncompacted in the log. Only applicable for logs that are being compacted.
     */
    logCleanerMinCompactionLagMs?: string;
    /**
     * The default cleanup policy for segments beyond the retention window.
     */
    logCleanupPolicy?: string;
    /**
     * The number of messages accumulated on a log partition 
     * before messages are flushed to disk.
     */
    logFlushIntervalMessages?: string;
    /**
     * The maximum time in ms that a message in any topic is kept 
     * in memory before flushed to disk. If not set, the value in log.flush.scheduler.interval.ms is used.
     */
    logFlushIntervalMs?: string;
    /**
     * The interval with which Kafka adds an entry to the offset index.
     */
    logIndexIntervalBytes?: string;
    /**
     * The maximum size in bytes of the offset index.
     */
    logIndexSizeMaxBytes?: string;
    /**
     * This configuration controls whether down-conversion 
     * of message formats is enabled to satisfy consume requests.
     */
    logMessageDownconversionEnable?: string;
    /**
     * The maximum difference allowed between 
     * the timestamp when a broker receives a message and the timestamp specified in the message
     */
    logMessageTimestampDifferenceMaxMs?: string;
    /**
     * Define whether the timestamp in the message is 
     * message create time or log append time.
     */
    logMessageTimestampType?: string;
    /**
     * Should pre allocate file when create new segment?
     */
    logPreallocate?: string;
    /**
     * The maximum size of the log before deleting messages
     */
    logRetentionBytes?: string;
    /**
     * The number of hours to keep a log file before deleting it.
     */
    logRetentionHours?: string;
    /**
     * The number of milliseconds to keep a log file before deleting it 
     * (in milliseconds), If not set, the value in log.retention.minutes is used. If set to -1, no
     * time limit is applied.
     */
    logRetentionMs?: string;
    /**
     * The maximum jitter to subtract from logRollTimeMillis 
     * (in milliseconds). If not set, the value in log.roll.jitter.hours is used.
     */
    logRollJitterMs?: string;
    /**
     * The maximum time before a new log segment is rolled out (in milliseconds).
     */
    logRollMs?: string;
    /**
     * The maximum size of a single log file
     */
    logSegmentBytes?: string;
    /**
     * The amount of time to wait before deleting a file 
     * from the filesystem.
     */
    logSegmentDeleteDelayMs?: string;
    /**
     * The maximum number of connections allowed from each ip 
     * address (defaults to 2147483647).
     */
    maxConnectionsPerIp?: string;
    /**
     * The maximum number of incremental fetch 
     * sessions that the broker will maintain.
     */
    maxIncrementalFetchSessionCacheSlots?: string;
    /**
     * The maximum size of message that the server can receive.
     */
    messageMaxBytes?: string;
    /**
     * When a producer sets acks to 'all' (or '-1'), 
     * min.insync.replicas specifies the minimum number of replicas that must acknowledge a write for
     * the write to be considered successful.
     */
    minInsyncReplicas?: string;
    /**
     * Number of partitions for autocreated topics
     */
    numPartitions?: string;
    /**
     * Log retention window in minutes for offsets topic.
     */
    offsetsRetentionMinutes?: string;
    /**
     * The purge interval (in number of 
     * requests) of the producer request purgatory(defaults to 1000).
     */
    producerPurgatoryPurgeIntervalRequests?: string;
    /**
     * The number of bytes of messages to attempt to fetch 
     * for each partition (defaults to 1048576). This is not an absolute maximum, if the first record
     * batch in the first non-empty partition of the fetch is larger than this value, the record batch
     * will still be returned to ensure that progress can be made.
     */
    replicaFetchMaxBytes?: string;
    /**
     * Maximum bytes expected for the entire fetch 
     * response (defaults to 10485760). Records are fetched in batches, and if the first record batch
     * in the first non-empty partition of the fetch is larger than this value, the record batch will
     * still be returned to ensure that progress can be made. As such, this is not an absolute maximum.
     */
    replicaFetchResponseMaxBytes?: string;
    /**
     * The maximum number of bytes in a socket request 
     * (defaults to 104857600).
     */
    socketRequestMaxBytes?: string;
    /**
     * The interval at which 
     * to remove transactions that have expired due to transactional.id.expiration.ms passing (defaults
     * to 3600000 (1 hour)).
     */
    transactionRemoveExpiredTransactionCleanupIntervalMs?: string;
    /**
     * The transaction topic segment bytes should 
     * be kept relatively small in order to facilitate faster log compaction and cache loads (defaults
     * to 104857600 (100 mebibytes)).
     */
    transactionStateLogSegmentBytes?: string;
}

export interface KafkaKafkaUserConfigKafkaAuthenticationMethods {
    /**
     * Enable certificate/SSL authentication
     */
    certificate?: string;
    /**
     * Enable SASL authentication
     */
    sasl?: string;
}

export interface KafkaKafkaUserConfigKafkaConnectConfig {
    /**
     * Defines what client configurations can 
     * be overridden by the connector. Default is None
     */
    connectorClientConfigOverridePolicy?: string;
    /**
     * What to do when there is no initial offset in Kafka or 
     * if the current offset does not exist any more on the server. Default is earliest.
     */
    consumerAutoOffsetReset?: string;
    /**
     * Records are fetched in batches by the consumer, and 
     * if the first record batch in the first non-empty partition of the fetch is larger than this value,
     * the record batch will still be returned to ensure that the consumer can make progress. As such,
     * this is not a absolute maximum.
     */
    consumerFetchMaxBytes?: string;
    /**
     * Transaction read isolation level. readUncommitted is 
     * the default, but readCommitted can be used if consume-exactly-once behavior is desired.
     */
    consumerIsolationLevel?: string;
    /**
     * Records are fetched in batches by the consumer.If 
     * the first record batch in the first non-empty partition of the fetch is larger than this limit,
     * the batch will still be returned to ensure that the consumer can make progress.
     */
    consumerMaxPartitionFetchBytes?: string;
    /**
     * The maximum delay in milliseconds between invocations 
     * of poll() when using consumer group management (defaults to 300000).
     */
    consumerMaxPollIntervalMs?: string;
    /**
     * The maximum number of records returned in a single call 
     * to poll() (defaults to 500).
     */
    consumerMaxPollRecords?: string;
    /**
     * The interval at which to try committing offsets for 
     * tasks (defaults to 60000).
     */
    offsetFlushIntervalMs?: string;
    /**
     * Maximum number of milliseconds to wait for records to 
     * flush and partition offset data to be committed to offset storage before cancelling the process
     * and restoring the offset data to be committed in a future attempt (defaults to 5000).
     */
    offsetFlushTimeoutMs?: string;
    /**
     * This setting will limit the number of record batches 
     * the producer will send in a single request to avoid sending huge requests.
     */
    producerMaxRequestSize?: string;
    /**
     * The timeout in milliseconds used to detect failures when 
     * using Kafkaâ€™s group management facilities (defaults to 10000).
     */
    sessionTimeoutMs?: string;
}

export interface KafkaKafkaUserConfigKafkaRestConfig {
    /**
     * If true the consumer's offset will be periodically 
     * committed to Kafka in the background
     */
    consumerEnableAutoCommit?: string;
    /**
     * Maximum number of bytes in unencoded message keys and 
     * values by a single request
     */
    consumerRequestMaxBytes?: string;
    /**
     * The maximum total time to wait for messages for a 
     * request if the maximum number of messages has not yet been reached
     */
    consumerRequestTimeoutMs?: string;
    /**
     * The number of acknowledgments the producer requires the leader to 
     * have received before considering a request complete. If set to 'all' or '-1', the leader will wait
     * for the full set of in-sync replicas to acknowledge the record.
     */
    producerAcks?: string;
    /**
     * Wait for up to the given delay to allow batching records together
     */
    producerLingerMs?: string;
    /**
     * Maximum number of SimpleConsumers that can be 
     * instantiated per broker.
     */
    simpleconsumerPoolSizeMax?: string;
}

export interface KafkaKafkaUserConfigPrivateAccess {
    /**
     * Allow clients to connect to prometheus from the public internet for 
     * service nodes that are in a project VPC or another type of private network
     */
    prometheus?: string;
}

export interface KafkaKafkaUserConfigPrivatelinkAccess {
    /**
     * Enable kafka
     */
    kafka?: string;
    /**
     * Enable kafka_connect
     */
    kafkaConnect?: string;
    /**
     * Enable kafka_rest
     */
    kafkaRest?: string;
    /**
     * Enable Schema-Registry service
     */
    schemaRegistry?: string;
}

export interface KafkaKafkaUserConfigPublicAccess {
    /**
     * Enable kafka
     */
    kafka?: string;
    /**
     * Enable kafka_connect
     */
    kafkaConnect?: string;
    /**
     * Enable kafka_rest
     */
    kafkaRest?: string;
    /**
     * Allow clients to connect to prometheus from the public internet for 
     * service nodes that are in a project VPC or another type of private network
     */
    prometheus?: string;
    /**
     * Enable Schema-Registry service
     */
    schemaRegistry?: string;
}

export interface KafkaKafkaUserConfigSchemaRegistryConfig {
    /**
     * If true, Karapace / Schema Registry on the service nodes can 
     * participate in leader election. It might be needed to disable this when the schemas topic is replicated
     * to a secondary cluster and Karapace / Schema Registry there must not participate in leader election.
     * Defaults to 'true'.
     */
    leaderEligibility?: string;
    /**
     * The durable single partition topic that acts as the durable log for the 
     * data. This topic must be compacted to avoid losing data due to retention policy. Please note that
     * changing this configuration in an existing Schema Registry / Karapace setup leads to previous
     * schemas being inaccessible, data encoded with them potentially unreadable and schema ID sequence
     * put out of order. It's only possible to do the switch while Schema Registry / Karapace is disabled.
     * Defaults to '_schemas'.
     */
    topicName?: string;
}

export interface KafkaMirrorMakerComponent {
    component: string;
    host: string;
    kafkaAuthenticationMethod: string;
    port: number;
    route: string;
    ssl: boolean;
    usage: string;
}

export interface KafkaMirrorMakerKafkaMirrormaker {
}

export interface KafkaMirrorMakerKafkaMirrormakerUserConfig {
    /**
     * allow incoming connections from CIDR address block, e.g. `10.20.0.0/16`
     */
    ipFilters?: string[];
    /**
     * Kafka MirrorMaker configuration values
     */
    kafkaMirrormaker?: outputs.KafkaMirrorMakerKafkaMirrormakerUserConfigKafkaMirrormaker;
    staticIps?: string;
}

export interface KafkaMirrorMakerKafkaMirrormakerUserConfigKafkaMirrormaker {
    /**
     * Whether to periodically write the translated offsets
     * of replicated consumer groups (in the source cluster) to __consumer_offsets topic in target cluster,
     * as long as no active consumers in that group are connected to the target cluster.
     */
    emitCheckpointsEnabled?: string;
    emitCheckpointsIntervalSeconds?: string;
    /**
     * Whether to periodically check for new consumer groups.
     * Defaults to 'true'.
     */
    refreshGroupsEnabled?: string;
    /**
     * Frequency of consumer group refresh in seconds.
     * Defaults to 600 seconds (10 minutes).
     */
    refreshGroupsIntervalSeconds?: string;
    /**
     * Whether to periodically check for new topics and
     * partitions. Defaults to 'true'.
     */
    refreshTopicsEnabled?: string;
    /**
     * Frequency of topic and partitions refresh in
     * seconds. Defaults to 600 seconds (10 minutes).
     */
    refreshTopicsIntervalSeconds?: string;
    /**
     * Whether to periodically write the translated offsets of replicated consumer groups (in the source cluster) to __consumer_offsets topic in target cluster, as long as no active consumers in that group are connected to the target cluster. Defaults to 'false'.
     */
    syncGroupOffsetsEnabled?: string;
    /**
     * Frequency at which consumer group offsets
     * are synced (default: 60, every minute).
     */
    syncGroupOffsetsIntervalSeconds?: string;
    syncTopicConfigsEnabled?: string;
    tasksMaxPerCpu?: string;
}

export interface KafkaMirrorMakerServiceIntegration {
    integrationType: string;
    sourceServiceName: string;
}

export interface KafkaServiceIntegration {
    integrationType: string;
    sourceServiceName: string;
}

export interface KafkaTopicConfig {
    /**
     * cleanup.policy value, can be `create`, `delete` or `compact,delete`
     */
    cleanupPolicy?: string;
    /**
     * compression.type value
     */
    compressionType?: string;
    /**
     * delete.retention.ms value
     */
    deleteRetentionMs?: string;
    /**
     * file.delete.delay.ms value
     */
    fileDeleteDelayMs?: string;
    /**
     * flush.messages value
     */
    flushMessages?: string;
    /**
     * flush.ms value
     */
    flushMs?: string;
    /**
     * index.interval.bytes value
     */
    indexIntervalBytes?: string;
    /**
     * max.compaction.lag.ms value
     */
    maxCompactionLagMs?: string;
    /**
     * max.message.bytes value
     */
    maxMessageBytes?: string;
    /**
     * message.downconversion.enable value
     */
    messageDownconversionEnable?: string;
    /**
     * message.format.version value
     */
    messageFormatVersion?: string;
    /**
     * message.timestamp.difference.max.ms value
     */
    messageTimestampDifferenceMaxMs?: string;
    /**
     * message.timestamp.type value
     */
    messageTimestampType?: string;
    /**
     * min.cleanable.dirty.ratio value
     */
    minCleanableDirtyRatio?: string;
    /**
     * min.compaction.lag.ms value
     */
    minCompactionLagMs?: string;
    /**
     * min.insync.replicas value
     */
    minInsyncReplicas?: string;
    /**
     * preallocate value
     */
    preallocate?: string;
    /**
     * retention.bytes value
     */
    retentionBytes?: string;
    /**
     * retention.ms value
     */
    retentionMs?: string;
    /**
     * segment.bytes value
     */
    segmentBytes?: string;
    /**
     * segment.index.bytes value
     */
    segmentIndexBytes?: string;
    /**
     * segment.jitter.ms value
     */
    segmentJitterMs?: string;
    /**
     * segment.ms value
     */
    segmentMs?: string;
    /**
     * unclean.leader.election.enable value
     */
    uncleanLeaderElectionEnable?: string;
}

export interface KafkaTopicTag {
    key: string;
    value?: string;
}

export interface M3AggregatorComponent {
    component: string;
    host: string;
    kafkaAuthenticationMethod: string;
    port: number;
    route: string;
    ssl: boolean;
    usage: string;
}

export interface M3AggregatorM3aggregator {
}

export interface M3AggregatorM3aggregatorUserConfig {
    /**
     * Serve the web frontend using a custom CNAME pointing to the Aiven DNS name.
     */
    customDomain?: string;
    /**
     * Allow incoming connections from CIDR address block, e.g. '10.20.0.0/16'
     */
    ipFilters?: string[];
    m3Version?: string;
    /**
     * M3 major version
     */
    m3aggregatorVersion?: string;
    staticIps?: string;
}

export interface M3AggregatorServiceIntegration {
    integrationType: string;
    sourceServiceName: string;
}

export interface M3DbComponent {
    component: string;
    host: string;
    kafkaAuthenticationMethod: string;
    port: number;
    route: string;
    ssl: boolean;
    usage: string;
}

export interface M3DbM3db {
}

export interface M3DbM3dbUserConfig {
    /**
     * Serve the web frontend using a custom CNAME pointing to the Aiven DNS name.
     */
    customDomain?: string;
    /**
     * Allow incoming connections from CIDR address block, e.g. '10.20.0.0/16'
     */
    ipFilters?: string[];
    /**
     * M3 limits
     */
    limits?: outputs.M3DbM3dbUserConfigLimits;
    m3Version?: string;
    /**
     * Enables access to Graphite Carbon 
     * plaintext metrics ingestion. It can be enabled only for services inside VPCs. The
     * metrics are written to aggregated namespaces only.
     */
    m3coordinatorEnableGraphiteCarbonIngest?: string;
    /**
     * M3 major version
     */
    m3dbVersion?: string;
    /**
     * List of M3 namespaces
     */
    namespaces?: outputs.M3DbM3dbUserConfigNamespace[];
    /**
     * Allow access to selected service ports from private networks.
     */
    privateAccess?: outputs.M3DbM3dbUserConfigPrivateAccess;
    /**
     * Name of another project to fork a service from. This has
     * effect only when a new service is being created.
     */
    projectToForkFrom?: string;
    /**
     * Allow access to selected service ports from the public Internet.
     */
    publicAccess?: outputs.M3DbM3dbUserConfigPublicAccess;
    /**
     * Mapping rules allow more granular use of aggregation, not simply sending 
     * everything to a namespace. If mapping rules exist that target a namespace, only data matching mapping
     * rules will be sent to it and nothing else.
     */
    rules?: outputs.M3DbM3dbUserConfigRules;
    /**
     * Name of another service to fork from. This has effect only 
     * when a new service is being created.
     */
    serviceToForkFrom?: string;
    staticIps?: string;
}

export interface M3DbM3dbUserConfigLimits {
    /**
     * The maximum number of data points fetched during request
     */
    globalDatapoints?: string;
    /**
     * The maximum number of data points fetched in single query
     */
    queryDatapoints?: string;
    /**
     * When query limits are exceeded, whether to return error 
     * (if True) or return partial results (False)
     */
    queryRequireExhaustive?: string;
    /**
     * The maximum number of series fetched in single query
     */
    querySeries?: string;
}

export interface M3DbM3dbUserConfigNamespace {
    /**
     * The name of the namespace
     */
    name?: string;
    /**
     * Namespace options
     */
    options?: outputs.M3DbM3dbUserConfigNamespaceOptions;
    /**
     * The resolution for an aggregated namespace
     */
    resolution?: string;
    /**
     * The type of aggregation (aggregated/unaggregated)
     */
    type?: string;
}

export interface M3DbM3dbUserConfigNamespaceOptions {
    /**
     * Retention options
     */
    retentionOptions?: outputs.M3DbM3dbUserConfigNamespaceOptionsRetentionOptions;
    /**
     * Controls whether M3DB will create snapshot files for 
     * this namespace
     */
    snapshotEnabled?: string;
    /**
     * Controls whether M3DB will include writes to this 
     * namespace in the commitlog.
     */
    writesToCommitlog?: string;
}

export interface M3DbM3dbUserConfigNamespaceOptionsRetentionOptions {
    /**
     * Controls how long we wait before expiring stale data
     */
    blockDataExpiryDuration?: string;
    /**
     * Controls how long to keep a block in memory before 
     * flushing to a fileset on disk
     */
    blocksizeDuration?: string;
    /**
     * Controls how far into the future writes to 
     * the namespace will be accepted
     */
    bufferFutureDuration?: string;
    /**
     * Controls how far into the past writes to the 
     * namespace will be accepted
     */
    bufferPastDuration?: string;
    /**
     * Controls the duration of time that M3DB will 
     * retain data for the namespace
     */
    retentionPeriodDuration?: string;
}

export interface M3DbM3dbUserConfigPrivateAccess {
    /**
     * Allow clients to connect to m3coordinator from the public internet 
     * for service nodes that are in a project VPC or another type of private network.
     */
    m3coordinator?: string;
}

export interface M3DbM3dbUserConfigPublicAccess {
    /**
     * Allow clients to connect to m3coordinator from the public internet 
     * for service nodes that are in a project VPC or another type of private network.
     */
    m3coordinator?: string;
}

export interface M3DbM3dbUserConfigRules {
    mappings?: outputs.M3DbM3dbUserConfigRulesMapping[];
}

export interface M3DbM3dbUserConfigRulesMapping {
    /**
     * List of aggregations to be applied
     */
    aggregations?: string[];
    /**
     * Drop the matching metric; Only store the derived metric (as specified in the roll-up rules), if any.
     */
    drop?: string;
    /**
     * The metrics to be used with this particular rule; Matching metric names with wildcards (using
     * __name__:wildcard) or matching tags and their (optionally wildcarded) values. For value, !
     * can be used at start of value for negation, and multiple filters can be supplied using space as separator.
     */
    filter?: string;
    /**
     * The name of the namespace
     */
    name?: string;
    /**
     * List of tags to be appended to matching metrics.
     */
    tags?: outputs.M3DbM3dbUserConfigRulesMappingTag[];
}

export interface M3DbM3dbUserConfigRulesMappingTag {
    /**
     * The name of the namespace
     */
    name?: string;
    /**
     * Value of the tag.
     */
    value?: string;
}

export interface M3DbServiceIntegration {
    integrationType: string;
    sourceServiceName: string;
}

export interface MySqlComponent {
    component: string;
    /**
     * Hostname or IP address of the server where to migrate data from
     */
    host: string;
    kafkaAuthenticationMethod: string;
    /**
     * Port number of the server where to migrate data from
     */
    port: number;
    route: string;
    /**
     * The server where to migrate data from is secured with SSL
     */
    ssl: boolean;
    usage: string;
}

export interface MySqlMysql {
}

export interface MySqlMysqlUserConfig {
    /**
     * Custom password for admin user. Defaults to random string. 
     * This must be set only when a new service is being created.
     */
    adminPassword?: string;
    /**
     * Custom username for admin user. This must be set only when a 
     * new service is being created.
     */
    adminUsername?: string;
    /**
     * The hour of day (in UTC) when backup for the service is started. 
     * New backup is only started if previous backup has already completed.
     */
    backupHour?: string;
    /**
     * The minute of an hour when backup for the service is started. 
     * New backup is only started if previous backup has already completed.
     */
    backupMinute?: string;
    /**
     * The minimum amount of time in seconds to keep binlog entries 
     * before deletion. This may be extended for services that require binlog entries for longer than the
     * default for example if using the MySQL Debezium Kafka connector.
     */
    binlogRetentionPeriod?: string;
    /**
     * Allow incoming connections from CIDR address block, e.g. '10.20.0.0/16'
     */
    ipFilters?: string[];
    /**
     * Migrate data from existing server
     */
    migration?: outputs.MySqlMysqlUserConfigMigration;
    /**
     * Allow clients to connect to mysql from the public internet for service 
     * nodes that are in a project VPC or another type of private network
     */
    mysql?: outputs.MySqlMysqlUserConfigMysql;
    /**
     * MySQL major version
     */
    mysqlVersion?: string;
    /**
     * Allow access to selected service ports from private networks
     */
    privateAccess?: outputs.MySqlMysqlUserConfigPrivateAccess;
    /**
     * Allow access to selected service components through Privatelink
     */
    privatelinkAccess?: outputs.MySqlMysqlUserConfigPrivatelinkAccess;
    /**
     * Name of another project to fork a service from. This has
     * effect only when a new service is being created.
     */
    projectToForkFrom?: string;
    /**
     * Allow access to selected service ports from the public Internet
     */
    publicAccess?: outputs.MySqlMysqlUserConfigPublicAccess;
    /**
     * Recovery target time when forking a service. This has effect 
     * only when a new service is being created.
     */
    recoveryTargetTime?: string;
    /**
     * Name of another service to fork from. This has effect only when 
     * a new service is being created.
     */
    serviceToForkFrom?: string;
    staticIps?: string;
}

export interface MySqlMysqlUserConfigMigration {
    /**
     * Database name for bootstrapping the initial connection
     */
    dbname?: string;
    /**
     * Hostname or IP address of the server where to migrate data from
     */
    host?: string;
    /**
     * Comma-separated list of databases, which should be ignored 
     * during migration (supported by MySQL only at the moment)
     */
    ignoreDbs?: string;
    /**
     * Password for authentication with the server where to migrate data from
     */
    password?: string;
    /**
     * Port number of the server where to migrate data from
     */
    port?: string;
    /**
     * The server where to migrate data from is secured with SSL
     */
    ssl?: string;
    /**
     * User name for authentication with the server where to migrate data from
     */
    username?: string;
}

export interface MySqlMysqlUserConfigMysql {
    /**
     * The number of seconds that the mysqld server waits for a 
     * connect packet before responding with Bad handshake
     */
    connectTimeout?: string;
    /**
     * Default server time zone as an offset from UTC 
     * (from -12:00 to +12:00), a time zone name, or 'SYSTEM' to use the MySQL server default.
     */
    defaultTimeZone?: string;
    /**
     * The maximum permitted result length in bytes for 
     * the GROUP_CONCAT() function.
     */
    groupConcatMaxLen?: string;
    /**
     * The time, in seconds, before cached 
     * statistics expire
     */
    informationSchemaStatsExpiry?: string;
    /**
     * Minimum length of words that are stored in 
     * an InnoDB FULLTEXT index.
     */
    innodbFtMinTokenSize?: string;
    /**
     * This option is used to specify your 
     * own InnoDB FULLTEXT index stopword list for all InnoDB tables.
     */
    innodbFtServerStopwordTable?: string;
    /**
     * The length of time in seconds an InnoDB 
     * transaction waits for a row lock before giving up.
     */
    innodbLockWaitTimeout?: string;
    /**
     * The size in bytes of the buffer that InnoDB 
     * uses to write to the log files on disk.
     */
    innodbLogBufferSize?: string;
    /**
     * The upper limit in bytes on the 
     * size of the temporary log files used during online DDL operations for InnoDB tables.
     */
    innodbOnlineAlterLogMaxSize?: string;
    /**
     * When enabled, information about all 
     * deadlocks in InnoDB user transactions is recorded in the error log. Disabled by default.
     */
    innodbPrintAllDeadlocks?: string;
    /**
     * When enabled a transaction timeout 
     * causes InnoDB to abort and roll back the entire transaction.
     */
    innodbRollbackOnTimeout?: string;
    /**
     * The number of seconds the server waits for 
     * activity on an interactive connection before closing it.
     */
    interactiveTimeout?: string;
    internalTmpMemStorageEngine?: string;
    /**
     * The slowQueryLogs work as SQL statements that take 
     * more than longQueryTime seconds to execute. Default is 10s
     */
    longQueryTime?: string;
    /**
     * Size of the largest message in bytes that can 
     * be received by the server. Default is 67108864 (64M)
     */
    maxAllowedPacket?: string;
    /**
     * Limits the size of internal in-memory tables. 
     * Also set tmp_table_size. Default is 16777216 (16M)
     */
    maxHeapTableSize?: string;
    /**
     * The number of seconds to wait for more data from 
     * a connection before aborting the read.
     */
    netReadTimeout?: string;
    /**
     * The number of seconds to wait for a block to be 
     * written to a connection before aborting the write.
     */
    netWriteTimeout?: string;
    /**
     * Slow query log enables capturing of slow queries. 
     * Setting slowQueryLog to false also truncates the mysql.slow_log table. Default is off
     */
    slowQueryLog?: string;
    /**
     * Sort buffer size in bytes for ORDER BY optimization. 
     * Default is 262144 (256K)
     */
    sortBufferSize?: string;
    /**
     * Global SQL mode. Set to empty to use MySQL server defaults. 
     * When creating a new service and not setting this field Aiven default SQL mode (strict,
     * SQL standard compliant) will be assigned.
     */
    sqlMode?: string;
    /**
     * Require primary key to be defined for new 
     * tables or old tables modified with ALTER TABLE and fail if missing. It is recommended
     * to always have primary keys because various functionality may break if any large table
     * is missing them.
     */
    sqlRequirePrimaryKey?: string;
    /**
     * Limits the size of internal in-memory tables. Also set 
     * max_heap_table_size. Default is 16777216 (16M)
     */
    tmpTableSize?: string;
    /**
     * The number of seconds the server waits for activity on 
     * a noninteractive connection before closing it.
     */
    waitTimeout?: string;
}

export interface MySqlMysqlUserConfigPrivateAccess {
    /**
     * Allow clients to connect to mysql from the public internet for service 
     * nodes that are in a project VPC or another type of private network
     */
    mysql?: string;
    /**
     * Allow clients to connect to mysqlx from the public internet for service 
     * nodes that are in a project VPC or another type of private network
     */
    mysqlx?: string;
    /**
     * Allow clients to connect to prometheus from the public internet 
     * for service nodes that are in a project VPC or another type of private network
     */
    prometheus?: string;
}

export interface MySqlMysqlUserConfigPrivatelinkAccess {
    /**
     * Allow clients to connect to mysql from the public internet for service 
     * nodes that are in a project VPC or another type of private network
     */
    mysql?: string;
    /**
     * Allow clients to connect to mysqlx from the public internet for service 
     * nodes that are in a project VPC or another type of private network
     */
    mysqlx?: string;
}

export interface MySqlMysqlUserConfigPublicAccess {
    /**
     * Allow clients to connect to mysql from the public internet for service 
     * nodes that are in a project VPC or another type of private network
     */
    mysql?: string;
    /**
     * Allow clients to connect to mysqlx from the public internet for service 
     * nodes that are in a project VPC or another type of private network
     */
    mysqlx?: string;
    /**
     * Allow clients to connect to prometheus from the public internet 
     * for service nodes that are in a project VPC or another type of private network
     */
    prometheus?: string;
}

export interface MySqlServiceIntegration {
    integrationType: string;
    sourceServiceName: string;
}

export interface OpenSearchComponent {
    component: string;
    host: string;
    kafkaAuthenticationMethod: string;
    port: number;
    route: string;
    ssl: boolean;
    usage: string;
}

export interface OpenSearchOpensearch {
    /**
     * URI for Opensearch dashboards frontend.
     */
    opensearchDashboardsUri: string;
}

export interface OpenSearchOpensearchUserConfig {
    /**
     * Serve the web frontend using a custom CNAME pointing to the Aiven DNS name.
     */
    customDomain?: string;
    /**
     * Disable automatic replication factor adjustment for
     * multi-node services. By default, Aiven ensures all indexes are replicated at least to two nodes. Note: setting
     * this to true increases a risk of data loss in case of virtual machine failure.
     */
    disableReplicationFactorAdjustment?: string;
    /**
     * Glob pattern and number of indexes matching that pattern to be kept.
     */
    indexPatterns?: outputs.OpenSearchOpensearchUserConfigIndexPattern[];
    /**
     * Template settings for all new indexe.
     */
    indexTemplate?: outputs.OpenSearchOpensearchUserConfigIndexTemplate;
    /**
     * allow incoming connections from CIDR address block, e.g. `10.20.0.0/16`
     */
    ipFilters?: string[];
    keepIndexRefreshInterval?: string;
    /**
     * Maximum number of indexes to keep before deleting the oldest one.
     */
    maxIndexCount?: string;
    /**
     * Allow clients to connect to opensearch from the public internet for service nodes
     * that are in a project VPC or another type of private network.
     */
    opensearch?: outputs.OpenSearchOpensearchUserConfigOpensearch;
    /**
     * Allow clients to connect to opensearchDashboards from the public
     * internet for service nodes that are in a project VPC or another type of private network.
     */
    opensearchDashboards?: outputs.OpenSearchOpensearchUserConfigOpensearchDashboards;
    /**
     * Opensearch major version.
     */
    opensearchVersion?: string;
    /**
     * Allow access to selected service ports from private networks.
     */
    privateAccess?: outputs.OpenSearchOpensearchUserConfigPrivateAccess;
    /**
     * Allow access to selected service components through Privatelink
     */
    privatelinkAccess?: outputs.OpenSearchOpensearchUserConfigPrivatelinkAccess;
    /**
     * Name of another project to fork a service from. This has effect only when a
     * new service is being created.
     */
    projectToForkFrom?: string;
    /**
     * Allow access to selected service ports from the public Internet.
     */
    publicAccess?: outputs.OpenSearchOpensearchUserConfigPublicAccess;
    /**
     * Name of the basebackup to restore in forked service.
     */
    recoveryBasebackupName?: string;
    /**
     * Name of another service to fork from. This has effect only when a new service
     * is being created.
     */
    serviceToForkFrom?: string;
    staticIps?: string;
}

export interface OpenSearchOpensearchUserConfigIndexPattern {
    /**
     * Maximum number of indexes to keep before deleting the oldest one.
     */
    maxIndexCount?: string;
    /**
     * Must consist of alpha-numeric characters, dashes, underscores, dots and glob
     * characters (* and ?)
     */
    pattern?: string;
    /**
     * Deletion sorting algorithm
     */
    sortingAlgorithm?: string;
}

export interface OpenSearchOpensearchUserConfigIndexTemplate {
    /**
     * The maximum number of nested JSON objects that a single document
     * can contain across all nested types. This limit helps to prevent out of memory errors when a document contains
     * too many nested objects. Default is 10000.
     */
    mappingNestedObjectsLimit?: string;
    /**
     * The number of replicas each primary shard has.
     */
    numberOfReplicas?: string;
    /**
     * The number of primary shards that an index should have.
     */
    numberOfShards?: string;
}

export interface OpenSearchOpensearchUserConfigOpensearch {
    /**
     * Explicitly allow or block automatic creation of indices.
     * Defaults to true
     */
    actionAutoCreateIndexEnabled?: string;
    /**
     * Require explicit index names when deleting
     */
    actionDestructiveRequiresName?: string;
    /**
     * Controls the number of shards allowed in the cluster per data node.
     */
    clusterMaxShardsPerNode?: string;
    /**
     * Maximum content length for HTTP requests to the Opensearch HTTP API, in
     * bytes.
     */
    httpMaxContentLength?: string;
    /**
     * The max size of allowed headers, in bytes.
     */
    httpMaxHeaderSize?: string;
    /**
     * The max length of an HTTP URL, in bytes.
     */
    httpMaxInitialLineLength?: string;
    /**
     * Relative amount. Maximum amount of heap memory used for field data
     * cache. This is an expert setting; decreasing the value too much will increase overhead of loading field data;
     * too much memory used for field data cache will decrease amount of heap available for other operations.
     */
    indicesFielddataCacheSize?: string;
    /**
     * Percentage value. Default is 10%. Total amount of heap used
     * for indexing buffer, before writing segments to disk. This is an expert setting. Too low value will slow down
     * indexing; too high value will increase indexing performance but causes performance issues for query
     * performance.
     */
    indicesMemoryIndexBufferSize?: string;
    /**
     * Percentage value. Default is 10%. Maximum amount of heap used for
     * query cache. This is an expert setting. Too low value will decrease query performance and increase performance
     * for other operations; too high value will cause issues with other Opensearch functionality.
     */
    indicesQueriesCacheSize?: string;
    /**
     * Maximum number of clauses Lucene BooleanQuery can have. The
     * default value (1024) is relatively high, and increasing it may cause performance issues. Investigate other
     * approaches first before increasing this value.
     */
    indicesQueryBoolMaxClauseCount?: string;
    /**
     * Whitelisted addresses for reindexing. Changing this value will cause
     * all Opensearch instances to restart.
     */
    reindexRemoteWhitelists?: string[];
    /**
     * Maximum number of aggregation buckets allowed in a single response.
     * Opensearch default value is used when this is not defined.
     */
    searchMaxBuckets?: string;
    /**
     * Size for the thread pool queue. See documentation for exact
     * details.
     */
    threadPoolAnalyzeQueueSize?: string;
    /**
     * Size for the thread pool. See documentation for exact details. Do note
     * this may have maximum value depending on CPU count - value is automatically lowered if set to higher than
     * maximum value.
     */
    threadPoolAnalyzeSize?: string;
    /**
     * Size for the thread pool. See documentation for exact details. Do
     * note this may have maximum value depending on CPU count - value is automatically lowered if set to higher than
     * maximum value.
     */
    threadPoolForceMergeSize?: string;
    /**
     * Size for the thread pool queue. See documentation for exact details.
     */
    threadPoolGetQueueSize?: string;
    /**
     * Size for the thread pool. See documentation for exact details. Do note
     * this may have maximum value depending on CPU count - value is automatically lowered if set to higher than
     * maximum value.
     */
    threadPoolGetSize?: string;
    /**
     * Size for the thread pool. See documentation for exact details. Do note
     * this may have maximum value depending on CPU count - value is automatically lowered if set to higher than
     * maximum value.
     */
    threadPoolIndexSize?: string;
    /**
     * Size for the thread pool queue. See documentation for exact
     * details.
     */
    threadPoolSearchQueueSize?: string;
    /**
     * Size for the thread pool. See documentation for exact details. Do note
     * this may have maximum value depending on CPU count - value is automatically lowered if set to higher than
     * maximum value.
     */
    threadPoolSearchSize?: string;
    /**
     * Size for the thread pool queue. See documentation for
     * exact details.
     */
    threadPoolSearchThrottledQueueSize?: string;
    /**
     * Size for the thread pool. See documentation for exact
     * details. Do note this may have maximum value depending on CPU count - value is automatically lowered if set to
     * higher than maximum value.
     */
    threadPoolSearchThrottledSize?: string;
    /**
     * Size for the thread pool queue. See documentation for exact
     * details.
     */
    threadPoolWriteQueueSize?: string;
    /**
     * Size for the thread pool. See documentation for exact details. Do note
     * this may have maximum value depending on CPU count - value is automatically lowered if set to higher than
     * maximum value.
     */
    threadPoolWriteSize?: string;
}

export interface OpenSearchOpensearchUserConfigOpensearchDashboards {
    /**
     * Enable or disable opensearch_dashboards.
     */
    enabled?: string;
    /**
     * Limits the maximum amount of memory (in MiB) the Opensearch dashboards
     * process can use. This sets the maxOldSpaceSize option of the nodejs running the Opensearch dashboards.
     * Note: the memory reserved by Opensearch dashboards is not available for Opensearch.
     */
    maxOldSpaceSize?: string;
    /**
     * Timeout in milliseconds for requests made by opensearchDashboards
     * towards Opensearch.
     */
    opensearchRequestTimeout?: string;
}

export interface OpenSearchOpensearchUserConfigPrivateAccess {
    /**
     * Allow clients to connect to opensearch from the public internet for service nodes
     * that are in a project VPC or another type of private network.
     */
    opensearch?: string;
    /**
     * Allow clients to connect to opensearchDashboards from the public
     * internet for service nodes that are in a project VPC or another type of private network.
     */
    opensearchDashboards?: string;
    /**
     * Allow clients to connect to prometheus from the public internet for service nodes
     * that are in a project VPC or another type of private network.
     */
    prometheus?: string;
}

export interface OpenSearchOpensearchUserConfigPrivatelinkAccess {
    /**
     * Allow clients to connect to opensearch from the public internet for service nodes
     * that are in a project VPC or another type of private network.
     */
    opensearch?: string;
    /**
     * Allow clients to connect to opensearchDashboards from the public
     * internet for service nodes that are in a project VPC or another type of private network.
     */
    opensearchDashboards?: string;
}

export interface OpenSearchOpensearchUserConfigPublicAccess {
    /**
     * Allow clients to connect to opensearch from the public internet for service nodes
     * that are in a project VPC or another type of private network.
     */
    opensearch?: string;
    /**
     * Allow clients to connect to opensearchDashboards from the public
     * internet for service nodes that are in a project VPC or another type of private network.
     */
    opensearchDashboards?: string;
    /**
     * Allow clients to connect to prometheus from the public internet for service nodes
     * that are in a project VPC or another type of private network.
     */
    prometheus?: string;
}

export interface OpenSearchServiceIntegration {
    integrationType: string;
    sourceServiceName: string;
}

export interface PgComponent {
    component: string;
    /**
     * hostname or IP address of the server where to migrate data from.
     */
    host: string;
    kafkaAuthenticationMethod: string;
    /**
     * port number of the server where to migrate data from.
     */
    port: number;
    route: string;
    /**
     * the server where to migrate data from is secured with SSL.
     */
    ssl: boolean;
    usage: string;
}

export interface PgPg {
    /**
     * database name for bootstrapping the initial connection.
     */
    dbname: string;
    /**
     * hostname or IP address of the server where to migrate data from.
     */
    host: string;
    /**
     * password for authentication with the server where to migrate data from.
     */
    password: string;
    /**
     * port number of the server where to migrate data from.
     */
    port: number;
    /**
     * PostgreSQL replica URI for services with a replica
     */
    replicaUri: string;
    /**
     * PostgreSQL sslmode setting (currently always `require`)
     */
    sslmode: string;
    /**
     * PostgreSQL master connection URI
     */
    uri: string;
    /**
     * PostgreSQL admin user name
     */
    user: string;
}

export interface PgPgUserConfig {
    /**
     * custom password for admin user. Defaults to random string. *This must
     * be set only when a new service is being created.*
     */
    adminPassword?: string;
    /**
     * custom username for admin user. *This must be set only when a new service
     * is being created.*
     */
    adminUsername?: string;
    /**
     * the hour of day (in UTC) when backup for the service is started. New backup 
     * is only started if previous backup has already completed.
     */
    backupHour?: string;
    /**
     * the minute of an hour when backup for the service is started. New backup 
     * is only started if previous backup has already completed.
     */
    backupMinute?: string;
    /**
     * allow incoming connections from CIDR address block, e.g. `10.20.0.0/16`
     */
    ipFilters?: string[];
    /**
     * migrate data from existing server, has the following options:
     */
    migration?: outputs.PgPgUserConfigMigration;
    /**
     * Enable pg.
     */
    pg?: outputs.PgPgUserConfigPg;
    /**
     * This setting is deprecated. Use read-replica service integration instead.
     */
    pgReadReplica?: string;
    /**
     * Name of the PG Service from which to fork (deprecated, use service_to_fork_from). 
     * This has effect only when a new service is being created.
     */
    pgServiceToForkFrom?: string;
    /**
     * PostgreSQL major version.
     */
    pgVersion?: string;
    /**
     * Enable pgbouncer.
     */
    pgbouncer?: outputs.PgPgUserConfigPgbouncer;
    /**
     * PGLookout settings.
     */
    pglookout?: outputs.PgPgUserConfigPglookout;
    /**
     * Allow access to selected service ports from private networks.
     */
    privateAccess?: outputs.PgPgUserConfigPrivateAccess;
    /**
     * Allow access to selected service components through Privatelink.
     */
    privatelinkAccess?: outputs.PgPgUserConfigPrivatelinkAccess;
    /**
     * Name of another project to fork a service from. This has
     * effect only when a new service is being created.
     */
    projectToForkFrom?: string;
    /**
     * Allow access to selected service ports from the public Internet
     */
    publicAccess?: outputs.PgPgUserConfigPublicAccess;
    /**
     * Recovery target time when forking a service. This has effect 
     * only when a new service is being created.
     */
    recoveryTargetTime?: string;
    /**
     * Name of another service to fork from. This has effect only 
     * when a new service is being created.
     */
    serviceToForkFrom?: string;
    /**
     * Percentage of total RAM that the database server uses for 
     * memory buffers. Valid range is 20-60 (float), which corresponds to 20% - 60%. This setting adjusts
     * the sharedBuffers configuration value. The absolute maximum is 12 GB.
     */
    sharedBuffersPercentage?: string;
    staticIps?: string;
    /**
     * Synchronous replication type. Note that the service plan 
     * also needs to support synchronous replication.
     */
    synchronousReplication?: string;
    /**
     * TimescaleDB extension configuration values.
     */
    timescaledb?: outputs.PgPgUserConfigTimescaledb;
    /**
     * Variant of the PostgreSQL service, may affect the features that are 
     * exposed by default. Options: `aiven` or `timescale`.
     */
    variant?: string;
    /**
     * Sets the maximum amount of memory to be used by a query operation (such 
     * as a sort or hash table) before writing to temporary disk files, in MB. Default is 1MB + 0.075% of
     * total RAM (up to 32MB).
     */
    workMem?: string;
}

export interface PgPgUserConfigMigration {
    /**
     * database name for bootstrapping the initial connection.
     */
    dbname?: string;
    /**
     * hostname or IP address of the server where to migrate data from.
     */
    host?: string;
    /**
     * Comma-separated list of databases, which should be ignored during 
     * migration (supported by MySQL only at the moment)
     */
    ignoreDbs?: string;
    /**
     * password for authentication with the server where to migrate data from.
     */
    password?: string;
    /**
     * port number of the server where to migrate data from.
     */
    port?: string;
    /**
     * the server where to migrate data from is secured with SSL.
     */
    ssl?: string;
    /**
     * user name for authentication with the server where to migrate data from.
     */
    username?: string;
}

export interface PgPgUserConfigPg {
    /**
     * Specifies a fraction of the table size to add to 
     * autovacuumAnalyzeThreshold when deciding whether to trigger an ANALYZE. The default is 0.2
     * (20% of table size).
     */
    autovacuumAnalyzeScaleFactor?: string;
    /**
     * specifies the minimum number of inserted, updated 
     * or deleted tuples needed to trigger an ANALYZE in any one table. The default is 50 tuples.
     */
    autovacuumAnalyzeThreshold?: string;
    /**
     * specifies the maximum age (in transactions) that a table's 
     * pg_class.relfrozenxid field can attain before a VACUUM operation is forced to prevent transaction ID
     * wraparound within the table. Note that the system will launch autovacuum processes to prevent wraparound
     * even when autovacuum is otherwise disabled. This parameter will cause the server to be restarted.
     */
    autovacuumFreezeMaxAge?: string;
    /**
     * specifies the maximum number of autovacuum processes (other 
     * than the autovacuum launcher) that may be running at any one time. The default is three. This parameter
     * can only be set at server start.
     */
    autovacuumMaxWorkers?: string;
    /**
     * specifies the minimum delay between autovacuum runs on any 
     * given database. The delay is measured in seconds, and the default is one minute.
     */
    autovacuumNaptime?: string;
    /**
     * specifies the cost delay value that will be used 
     * in automatic VACUUM operations. If -1 is specified, the regular vacuumCostDelay value will be
     * used. The default value is 20 milliseconds.
     */
    autovacuumVacuumCostDelay?: string;
    /**
     * specifies the cost limit value that will be used in 
     * automatic VACUUM operations. If -1 is specified (which is the default), the regular vacuumCostLimit
     * value will be used.
     */
    autovacuumVacuumCostLimit?: string;
    /**
     * specifies a fraction of the table size to add to 
     * autovacuumVacuumThreshold when deciding whether to trigger a VACUUM. The default is 0.2 (20% of table size).
     */
    autovacuumVacuumScaleFactor?: string;
    /**
     * specifies the minimum number of updated or deleted tuples 
     * needed to trigger a VACUUM in any one table. The default is 50 tuples
     */
    autovacuumVacuumThreshold?: string;
    bgwriterDelay?: string;
    bgwriterFlushAfter?: string;
    bgwriterLruMaxpages?: string;
    bgwriterLruMultiplier?: string;
    /**
     * this is the amount of time, in milliseconds, to wait on a lock before 
     * checking to see if there is a deadlock condition.
     */
    deadlockTimeout?: string;
    /**
     * Time out sessions with open transactions after 
     * this number of milliseconds.
     */
    idleInTransactionSessionTimeout?: string;
    /**
     * Controls system-wide use of Just-in-Time Compilation (JIT).
     */
    jit?: string;
    /**
     * Causes each action executed by autovacuum to be logged 
     * if it ran for at least the specified number of milliseconds. Setting this to zero logs all autovacuum
     * actions. Minus-one (the default) disables logging autovacuum actions.
     */
    logAutovacuumMinDuration?: string;
    /**
     * Controls the amount of detail written in the server log for 
     * each message that is logged. Possible values: `TERSE`, `DEFAULT` and `VERBOSE`.
     */
    logErrorVerbosity?: string;
    /**
     * Choose from one of the available log-formats. These can support 
     * popular log analyzers like pgbadger, pganalyze etc.
     * milliseconds to run, -1 disables
     */
    logLinePrefix?: string;
    /**
     * Log statements that take more than this number of
     */
    logMinDurationStatement?: string;
    /**
     * PostgreSQL maximum number of files that can be open per process
     */
    maxFilesPerProcess?: string;
    /**
     * PostgreSQL maximum locks per transaction
     */
    maxLocksPerTransaction?: string;
    /**
     * PostgreSQL maximum logical replication workers 
     * (taken from the pool of max_parallel_workers)
     */
    maxLogicalReplicationWorkers?: string;
    /**
     * Sets the maximum number of workers that the system can 
     * support for parallel queries.
     */
    maxParallelWorkers?: string;
    /**
     * Sets the maximum number of workers that can be 
     * started by a single Gather or Gather Merge node.
     */
    maxParallelWorkersPerGather?: string;
    /**
     * PostgreSQL maximum predicate locks per transaction
     */
    maxPredLocksPerTransaction?: string;
    /**
     * PostgreSQL maximum prepared transactions
     */
    maxPreparedTransactions?: string;
    /**
     * PostgreSQL maximum replication slots
     */
    maxReplicationSlots?: string;
    /**
     * Maximum depth of the stack in bytes
     */
    maxStackDepth?: string;
    /**
     * Max standby archive delay in milliseconds
     */
    maxStandbyArchiveDelay?: string;
    /**
     * Max standby streaming delay in milliseconds
     */
    maxStandbyStreamingDelay?: string;
    /**
     * PostgreSQL maximum WAL senders
     */
    maxWalSenders?: string;
    /**
     * Sets the maximum number of background processes that the system
     * can support
     * * `pg_partman_bgw.interval` - (Optional) Sets the time interval to run pg_partman's scheduled tasks
     * * `pg_partman_bgw.role` - (Optional) Controls which role to use for pg_partman's scheduled
     * background tasks.
     * * `pg_stat_statements.track` - (Optional) Controls which statements are counted. Specify top
     * to track top-level statements (those issued directly by clients), all to also track nested
     * statements (such as statements invoked within functions), or none to disable statement statistics
     * collection. The default value is top.
     */
    maxWorkerProcesses?: string;
    pgPartmanBgwDotInterval?: string;
    pgPartmanBgwDotRole?: string;
    pgStatStatementsDotTrack?: string;
    /**
     * PostgreSQL temporary file limit in KiB, -1 for unlimited
     */
    tempFileLimit?: string;
    /**
     * PostgreSQL service timezone
     */
    timezone?: string;
    /**
     * Specifies the number of bytes reserved to track the currently 
     * executing command for each active session.
     */
    trackActivityQuerySize?: string;
    /**
     * Record commit time of transactions
     */
    trackCommitTimestamp?: string;
    /**
     * Enables tracking of function call counts and time used.
     */
    trackFunctions?: string;
    /**
     * Enables timing of database I/O calls. This parameter is off by default, 
     * because it will repeatedly query the operating system for the current time, which may cause significant
     * overhead on some platforms.
     */
    trackIoTiming?: string;
    /**
     * Terminate replication connections that are inactive for longer than 
     * this amount of time, in milliseconds.
     */
    walSenderTimeout?: string;
    /**
     * WAL flush interval in milliseconds. Note that setting this value 
     * to lower than the default 200ms may negatively impact performance
     */
    walWriterDelay?: string;
}

export interface PgPgUserConfigPgbouncer {
    /**
     * If the automatically created database pools have been unused this 
     * many seconds, they are freed. If 0 then timeout is disabled.
     */
    autodbIdleTimeout?: string;
    /**
     * Do not allow more than this many server connections per database 
     * (regardless of user). Setting it to 0 means unlimited.
     */
    autodbMaxDbConnections?: string;
    /**
     * PGBouncer pool mode
     */
    autodbPoolMode?: string;
    /**
     * If non-zero then create automatically a pool of that size per user 
     * when a pool doesn't exist.
     */
    autodbPoolSize?: string;
    /**
     * Enum of parameters to ignore when given in startup packet.
     */
    ignoreStartupParameters?: string[];
    /**
     * Add more server connections to pool if below this number. Improves 
     * behavior when usual load comes suddenly back after period of total inactivity. The value is
     * effectively capped at the pool size.
     */
    minPoolSize?: string;
    /**
     * If a server connection has been idle more than this many seconds 
     * it will be dropped. If 0 then timeout is disabled.
     */
    serverIdleTimeout?: string;
    /**
     * The pooler will close an unused server connection that has been connected 
     * longer than this.
     */
    serverLifetime?: string;
    /**
     * Run serverResetQuery (DISCARD ALL) in all pooling modes.
     */
    serverResetQueryAlways?: string;
}

export interface PgPgUserConfigPglookout {
    /**
     * Number of seconds of master unavailability before 
     * triggering database failover to standby
     */
    maxFailoverReplicationTimeLag?: string;
}

export interface PgPgUserConfigPrivateAccess {
    /**
     * Enable pg.
     */
    pg?: string;
    /**
     * Enable pgbouncer.
     */
    pgbouncer?: string;
    /**
     * Allow clients to connect to prometheus from the public internet for 
     * service nodes that are in a project VPC or another type of private network
     */
    prometheus?: string;
}

export interface PgPgUserConfigPrivatelinkAccess {
    /**
     * Enable pg.
     */
    pg?: string;
    /**
     * Enable pgbouncer.
     */
    pgbouncer?: string;
}

export interface PgPgUserConfigPublicAccess {
    /**
     * Enable pg.
     */
    pg?: string;
    /**
     * Enable pgbouncer.
     */
    pgbouncer?: string;
    /**
     * Allow clients to connect to prometheus from the public internet for 
     * service nodes that are in a project VPC or another type of private network
     */
    prometheus?: string;
}

export interface PgPgUserConfigTimescaledb {
    /**
     * The number of background workers for timescaledb 
     * operations. You should configure this setting to the sum of your number of databases and the
     * total number of concurrent background workers you want running at any given point in time.
     */
    maxBackgroundWorkers?: string;
}

export interface PgServiceIntegration {
    integrationType: string;
    sourceServiceName: string;
}

export interface RedisComponent {
    component: string;
    /**
     * Hostname or IP address of the server where to migrate data from
     */
    host: string;
    kafkaAuthenticationMethod: string;
    /**
     * Port number of the server where to migrate data from
     */
    port: number;
    route: string;
    /**
     * The server where to migrate data from is secured with SSL
     */
    ssl: boolean;
    usage: string;
}

export interface RedisRedis {
}

export interface RedisRedisUserConfig {
    /**
     * Allow incoming connections from CIDR address block, e.g. `10.20.0.0/16`
     */
    ipFilters?: string[];
    /**
     * Migrate data from existing server
     */
    migration?: outputs.RedisRedisUserConfigMigration;
    /**
     * Allow access to selected service ports from private networks
     */
    privateAccess?: outputs.RedisRedisUserConfigPrivateAccess;
    /**
     * Allow access to selected service components through Privatelink
     */
    privatelinkAccess?: outputs.RedisRedisUserConfigPrivatelinkAccess;
    /**
     * Name of another project to fork a service from. This has
     * effect only when a new service is being created.
     */
    projectToForkFrom?: string;
    /**
     * Allow access to selected service ports from the public Internet
     */
    publicAccess?: outputs.RedisRedisUserConfigPublicAccess;
    /**
     * Name of the basebackup to restore in forked service
     */
    recoveryBasebackupName?: string;
    redisAclChannelsDefault?: string;
    /**
     * Redis IO thread count
     * * `redisLfuDecayTime"` - (Optional) LFU maxmemory-policy counter decay time in minutes
     */
    redisIoThreads?: string;
    redisLfuDecayTime?: string;
    /**
     * Counter logarithm factor for volatile-lfu and allkeys-lfu 
     * maxmemory-policies
     */
    redisLfuLogFactor?: string;
    /**
     * Redis maxmemory-policy
     */
    redisMaxmemoryPolicy?: string;
    /**
     * Set notify-keyspace-events option
     */
    redisNotifyKeyspaceEvents?: string;
    redisNumberOfDatabases?: string;
    redisPersistence?: string;
    redisPubsubClientOutputBufferLimit?: string;
    /**
     * Require SSL to access Redis
     */
    redisSsl?: string;
    /**
     * Redis idle connection timeout
     * * `serviceToForkFrom"` - (Optional) Name of another service to fork from. This has effect only
     * when a new service is being created.
     */
    redisTimeout?: string;
    serviceToForkFrom?: string;
    staticIps?: string;
}

export interface RedisRedisUserConfigMigration {
    /**
     * Database name for bootstrapping the initial connection
     */
    dbname?: string;
    /**
     * Hostname or IP address of the server where to migrate data from
     */
    host?: string;
    /**
     * Comma-separated list of databases, which should be ignored during 
     * migration (supported by MySQL only at the moment)
     */
    ignoreDbs?: string;
    /**
     * Password for authentication with the server where to migrate data from
     */
    password?: string;
    /**
     * Port number of the server where to migrate data from
     */
    port?: string;
    /**
     * The server where to migrate data from is secured with SSL
     */
    ssl?: string;
    /**
     * User name for authentication with the server where to migrate data from
     */
    username?: string;
}

export interface RedisRedisUserConfigPrivateAccess {
    /**
     * Allow clients to connect to prometheus from the public internet 
     * for service nodes that are in a project VPC or another type of private network
     */
    prometheus?: string;
    /**
     * Allow clients to connect to redis from the public internet for service 
     * nodes that are in a project VPC or another type of private network
     */
    redis?: string;
}

export interface RedisRedisUserConfigPrivatelinkAccess {
    /**
     * Allow clients to connect to redis from the public internet for service 
     * nodes that are in a project VPC or another type of private network
     */
    redis?: string;
}

export interface RedisRedisUserConfigPublicAccess {
    /**
     * Allow clients to connect to prometheus from the public internet 
     * for service nodes that are in a project VPC or another type of private network
     */
    prometheus?: string;
    /**
     * Allow clients to connect to redis from the public internet for service 
     * nodes that are in a project VPC or another type of private network
     */
    redis?: string;
}

export interface RedisServiceIntegration {
    integrationType: string;
    sourceServiceName: string;
}

export interface ServiceCassandra {
}

export interface ServiceCassandraUserConfig {
    cassandra?: outputs.ServiceCassandraUserConfigCassandra;
    cassandraVersion?: string;
    ipFilters?: string[];
    migrateSstableloader?: string;
    privateAccess?: outputs.ServiceCassandraUserConfigPrivateAccess;
    projectToForkFrom?: string;
    publicAccess?: outputs.ServiceCassandraUserConfigPublicAccess;
    serviceToForkFrom?: string;
    staticIps?: string;
}

export interface ServiceCassandraUserConfigCassandra {
    batchSizeFailThresholdInKb?: string;
    batchSizeWarnThresholdInKb?: string;
}

export interface ServiceCassandraUserConfigPrivateAccess {
    prometheus?: string;
}

export interface ServiceCassandraUserConfigPublicAccess {
    prometheus?: string;
}

export interface ServiceComponent {
    component: string;
    host: string;
    kafkaAuthenticationMethod: string;
    port: number;
    route: string;
    ssl: boolean;
    usage: string;
}

export interface ServiceElasticsearch {
    kibanaUri: string;
}

export interface ServiceElasticsearchUserConfig {
    customDomain?: string;
    disableReplicationFactorAdjustment?: string;
    elasticsearch?: outputs.ServiceElasticsearchUserConfigElasticsearch;
    elasticsearchVersion?: string;
    indexPatterns?: outputs.ServiceElasticsearchUserConfigIndexPattern[];
    indexTemplate?: outputs.ServiceElasticsearchUserConfigIndexTemplate;
    ipFilters?: string[];
    keepIndexRefreshInterval?: string;
    kibana?: outputs.ServiceElasticsearchUserConfigKibana;
    maxIndexCount?: string;
    opensearchVersion?: string;
    privateAccess?: outputs.ServiceElasticsearchUserConfigPrivateAccess;
    privatelinkAccess?: outputs.ServiceElasticsearchUserConfigPrivatelinkAccess;
    projectToForkFrom?: string;
    publicAccess?: outputs.ServiceElasticsearchUserConfigPublicAccess;
    recoveryBasebackupName?: string;
    serviceToForkFrom?: string;
    staticIps?: string;
}

export interface ServiceElasticsearchUserConfigElasticsearch {
    actionAutoCreateIndexEnabled?: string;
    actionDestructiveRequiresName?: string;
    clusterMaxShardsPerNode?: string;
    httpMaxContentLength?: string;
    httpMaxHeaderSize?: string;
    httpMaxInitialLineLength?: string;
    indicesFielddataCacheSize?: string;
    indicesMemoryIndexBufferSize?: string;
    indicesQueriesCacheSize?: string;
    indicesQueryBoolMaxClauseCount?: string;
    reindexRemoteWhitelists?: string[];
    searchMaxBuckets?: string;
    threadPoolAnalyzeQueueSize?: string;
    threadPoolAnalyzeSize?: string;
    threadPoolForceMergeSize?: string;
    threadPoolGetQueueSize?: string;
    threadPoolGetSize?: string;
    threadPoolIndexQueueSize?: string;
    threadPoolIndexSize?: string;
    threadPoolSearchQueueSize?: string;
    threadPoolSearchSize?: string;
    threadPoolSearchThrottledQueueSize?: string;
    threadPoolSearchThrottledSize?: string;
    threadPoolWriteQueueSize?: string;
    threadPoolWriteSize?: string;
}

export interface ServiceElasticsearchUserConfigIndexPattern {
    maxIndexCount?: string;
    pattern?: string;
    sortingAlgorithm?: string;
}

export interface ServiceElasticsearchUserConfigIndexTemplate {
    mappingNestedObjectsLimit?: string;
    numberOfReplicas?: string;
    numberOfShards?: string;
}

export interface ServiceElasticsearchUserConfigKibana {
    elasticsearchRequestTimeout?: string;
    enabled?: string;
    maxOldSpaceSize?: string;
}

export interface ServiceElasticsearchUserConfigPrivateAccess {
    elasticsearch?: string;
    kibana?: string;
    prometheus?: string;
}

export interface ServiceElasticsearchUserConfigPrivatelinkAccess {
    elasticsearch?: string;
    kibana?: string;
}

export interface ServiceElasticsearchUserConfigPublicAccess {
    elasticsearch?: string;
    kibana?: string;
    prometheus?: string;
}

export interface ServiceGrafana {
}

export interface ServiceGrafanaUserConfig {
    alertingEnabled?: string;
    alertingErrorOrTimeout?: string;
    alertingMaxAnnotationsToKeep?: string;
    alertingNodataOrNullvalues?: string;
    allowEmbedding?: string;
    authAzuread?: outputs.ServiceGrafanaUserConfigAuthAzuread;
    authBasicEnabled?: string;
    authGenericOauth?: outputs.ServiceGrafanaUserConfigAuthGenericOauth;
    authGithub?: outputs.ServiceGrafanaUserConfigAuthGithub;
    authGitlab?: outputs.ServiceGrafanaUserConfigAuthGitlab;
    authGoogle?: outputs.ServiceGrafanaUserConfigAuthGoogle;
    cookieSamesite?: string;
    customDomain?: string;
    dashboardsMinRefreshInterval?: string;
    dashboardsVersionsToKeep?: string;
    dataproxySendUserHeader?: string;
    dataproxyTimeout?: string;
    dateFormats?: outputs.ServiceGrafanaUserConfigDateFormats;
    disableGravatar?: string;
    editorsCanAdmin?: string;
    externalImageStorage?: outputs.ServiceGrafanaUserConfigExternalImageStorage;
    googleAnalyticsUaId?: string;
    ipFilters?: string[];
    metricsEnabled?: string;
    privateAccess?: outputs.ServiceGrafanaUserConfigPrivateAccess;
    privatelinkAccess?: outputs.ServiceGrafanaUserConfigPrivatelinkAccess;
    projectToForkFrom?: string;
    publicAccess?: outputs.ServiceGrafanaUserConfigPublicAccess;
    recoveryBasebackupName?: string;
    serviceToForkFrom?: string;
    smtpServer?: outputs.ServiceGrafanaUserConfigSmtpServer;
    staticIps?: string;
    userAutoAssignOrg?: string;
    userAutoAssignOrgRole?: string;
    viewersCanEdit?: string;
}

export interface ServiceGrafanaUserConfigAuthAzuread {
    allowSignUp?: string;
    allowedDomains?: string[];
    allowedGroups?: string[];
    authUrl?: string;
    clientId?: string;
    clientSecret?: string;
    tokenUrl?: string;
}

export interface ServiceGrafanaUserConfigAuthGenericOauth {
    allowSignUp?: string;
    allowedDomains?: string[];
    allowedOrganizations?: string[];
    apiUrl?: string;
    authUrl?: string;
    clientId?: string;
    clientSecret?: string;
    name?: string;
    scopes?: string[];
    tokenUrl?: string;
}

export interface ServiceGrafanaUserConfigAuthGithub {
    allowSignUp?: string;
    allowedOrganizations?: string[];
    clientId?: string;
    clientSecret?: string;
    teamIds?: string[];
}

export interface ServiceGrafanaUserConfigAuthGitlab {
    allowSignUp?: string;
    allowedGroups?: string[];
    apiUrl?: string;
    authUrl?: string;
    clientId?: string;
    clientSecret?: string;
    tokenUrl?: string;
}

export interface ServiceGrafanaUserConfigAuthGoogle {
    allowSignUp?: string;
    allowedDomains?: string[];
    clientId?: string;
    clientSecret?: string;
}

export interface ServiceGrafanaUserConfigDateFormats {
    defaultTimezone?: string;
    fullDate?: string;
    intervalDay?: string;
    intervalHour?: string;
    intervalMinute?: string;
    intervalMonth?: string;
    intervalSecond?: string;
    intervalYear?: string;
}

export interface ServiceGrafanaUserConfigExternalImageStorage {
    accessKey?: string;
    bucketUrl?: string;
    provider?: string;
    secretKey?: string;
}

export interface ServiceGrafanaUserConfigPrivateAccess {
    grafana?: string;
}

export interface ServiceGrafanaUserConfigPrivatelinkAccess {
    grafana?: string;
}

export interface ServiceGrafanaUserConfigPublicAccess {
    grafana?: string;
}

export interface ServiceGrafanaUserConfigSmtpServer {
    fromAddress?: string;
    fromName?: string;
    host?: string;
    password?: string;
    port?: string;
    skipVerify?: string;
    starttlsPolicy?: string;
    username?: string;
}

export interface ServiceInfluxdb {
    databaseName: string;
}

export interface ServiceInfluxdbUserConfig {
    customDomain?: string;
    influxdb?: outputs.ServiceInfluxdbUserConfigInfluxdb;
    ipFilters?: string[];
    privateAccess?: outputs.ServiceInfluxdbUserConfigPrivateAccess;
    privatelinkAccess?: outputs.ServiceInfluxdbUserConfigPrivatelinkAccess;
    projectToForkFrom?: string;
    publicAccess?: outputs.ServiceInfluxdbUserConfigPublicAccess;
    recoveryBasebackupName?: string;
    serviceToForkFrom?: string;
    staticIps?: string;
}

export interface ServiceInfluxdbUserConfigInfluxdb {
    logQueriesAfter?: string;
    maxConnectionLimit?: string;
    maxRowLimit?: string;
    maxSelectBuckets?: string;
    maxSelectPoint?: string;
    queryTimeout?: string;
}

export interface ServiceInfluxdbUserConfigPrivateAccess {
    influxdb?: string;
}

export interface ServiceInfluxdbUserConfigPrivatelinkAccess {
    influxdb?: string;
}

export interface ServiceInfluxdbUserConfigPublicAccess {
    influxdb?: string;
}

export interface ServiceIntegrationDashboardUserConfig {
}

export interface ServiceIntegrationDatadogUserConfig {
    datadogTags?: outputs.ServiceIntegrationDatadogUserConfigDatadogTag[];
    excludeConsumerGroups?: string[];
    excludeTopics?: string[];
    includeConsumerGroups?: string[];
    includeTopics?: string[];
    kafkaCustomMetrics?: string[];
    maxJmxMetrics?: string;
}

export interface ServiceIntegrationDatadogUserConfigDatadogTag {
    comment?: string;
    tag?: string;
}

export interface ServiceIntegrationEndpointDatadogUserConfig {
    datadogApiKey?: string;
    datadogTags?: outputs.ServiceIntegrationEndpointDatadogUserConfigDatadogTag[];
    disableConsumerStats?: string;
    maxPartitionContexts?: string;
    site?: string;
}

export interface ServiceIntegrationEndpointDatadogUserConfigDatadogTag {
    comment?: string;
    tag?: string;
}

export interface ServiceIntegrationEndpointExternalAwsCloudwatchLogsUserConfig {
    accessKey?: string;
    logGroupName?: string;
    region?: string;
    secretKey?: string;
}

export interface ServiceIntegrationEndpointExternalAwsCloudwatchMetricsUserConfig {
    accessKey?: string;
    namespace?: string;
    region?: string;
    secretKey?: string;
}

export interface ServiceIntegrationEndpointExternalElasticsearchLogsUserConfig {
    ca?: string;
    indexDaysMax?: string;
    indexPrefix?: string;
    timeout?: string;
    url?: string;
}

export interface ServiceIntegrationEndpointExternalGoogleCloudLoggingUserConfig {
    logId?: string;
    projectId?: string;
    serviceAccountCredentials?: string;
}

export interface ServiceIntegrationEndpointExternalKafkaUserConfig {
    bootstrapServers?: string;
    saslMechanism?: string;
    saslPlainPassword?: string;
    saslPlainUsername?: string;
    securityProtocol?: string;
    sslCaCert?: string;
    sslClientCert?: string;
    sslClientKey?: string;
    sslEndpointIdentificationAlgorithm?: string;
}

export interface ServiceIntegrationEndpointExternalSchemaRegistryUserConfig {
    authentication?: string;
    basicAuthPassword?: string;
    basicAuthUsername?: string;
    url?: string;
}

export interface ServiceIntegrationEndpointJolokiaUserConfig {
    basicAuthPassword?: string;
    basicAuthUsername?: string;
}

export interface ServiceIntegrationEndpointPrometheusUserConfig {
    basicAuthPassword?: string;
    basicAuthUsername?: string;
}

export interface ServiceIntegrationEndpointRsyslogUserConfig {
    ca?: string;
    cert?: string;
    format?: string;
    key?: string;
    logline?: string;
    port?: string;
    sd?: string;
    server?: string;
    tls?: string;
}

export interface ServiceIntegrationEndpointSignalfxUserConfig {
    enabledMetrics?: string[];
    signalfxApiKey?: string;
    signalfxRealm?: string;
}

export interface ServiceIntegrationExternalAwsCloudwatchLogsUserConfig {
}

export interface ServiceIntegrationExternalAwsCloudwatchMetricsUserConfig {
    droppedMetrics?: outputs.ServiceIntegrationExternalAwsCloudwatchMetricsUserConfigDroppedMetric[];
    extraMetrics?: outputs.ServiceIntegrationExternalAwsCloudwatchMetricsUserConfigExtraMetric[];
}

export interface ServiceIntegrationExternalAwsCloudwatchMetricsUserConfigDroppedMetric {
    field?: string;
    metric?: string;
}

export interface ServiceIntegrationExternalAwsCloudwatchMetricsUserConfigExtraMetric {
    field?: string;
    metric?: string;
}

export interface ServiceIntegrationExternalElasticsearchLogsUserConfig {
}

export interface ServiceIntegrationExternalGoogleCloudLoggingUserConfig {
}

export interface ServiceIntegrationKafkaConnectUserConfig {
    kafkaConnect?: outputs.ServiceIntegrationKafkaConnectUserConfigKafkaConnect;
}

export interface ServiceIntegrationKafkaConnectUserConfigKafkaConnect {
    configStorageTopic?: string;
    groupId?: string;
    offsetStorageTopic?: string;
    statusStorageTopic?: string;
}

export interface ServiceIntegrationKafkaLogsUserConfig {
    kafkaTopic?: string;
}

export interface ServiceIntegrationKafkaMirrormakerUserConfig {
    clusterAlias?: string;
}

export interface ServiceIntegrationLogsUserConfig {
    elasticsearchIndexDaysMax?: string;
    elasticsearchIndexPrefix?: string;
}

export interface ServiceIntegrationM3aggregatorUserConfig {
}

export interface ServiceIntegrationM3coordinatorUserConfig {
}

export interface ServiceIntegrationMetricsUserConfig {
    database?: string;
    retentionDays?: string;
    roUsername?: string;
    sourceMysql?: outputs.ServiceIntegrationMetricsUserConfigSourceMysql;
    username?: string;
}

export interface ServiceIntegrationMetricsUserConfigSourceMysql {
    telegraf?: outputs.ServiceIntegrationMetricsUserConfigSourceMysqlTelegraf;
}

export interface ServiceIntegrationMetricsUserConfigSourceMysqlTelegraf {
    gatherEventWaits?: string;
    gatherFileEventsStats?: string;
    gatherIndexIoWaits?: string;
    gatherInfoSchemaAutoInc?: string;
    gatherInnodbMetrics?: string;
    gatherPerfEventsStatements?: string;
    gatherProcessList?: string;
    gatherSlaveStatus?: string;
    gatherTableIoWaits?: string;
    gatherTableLockWaits?: string;
    gatherTableSchema?: string;
    perfEventsStatementsDigestTextLimit?: string;
    perfEventsStatementsLimit?: string;
    perfEventsStatementsTimeLimit?: string;
}

export interface ServiceIntegrationMirrormakerUserConfig {
    mirrormakerWhitelist?: string;
}

export interface ServiceIntegrationPrometheusUserConfig {
    sourceMysql?: outputs.ServiceIntegrationPrometheusUserConfigSourceMysql;
}

export interface ServiceIntegrationPrometheusUserConfigSourceMysql {
    telegraf?: outputs.ServiceIntegrationPrometheusUserConfigSourceMysqlTelegraf;
}

export interface ServiceIntegrationPrometheusUserConfigSourceMysqlTelegraf {
    gatherEventWaits?: string;
    gatherFileEventsStats?: string;
    gatherIndexIoWaits?: string;
    gatherInfoSchemaAutoInc?: string;
    gatherInnodbMetrics?: string;
    gatherPerfEventsStatements?: string;
    gatherProcessList?: string;
    gatherSlaveStatus?: string;
    gatherTableIoWaits?: string;
    gatherTableLockWaits?: string;
    gatherTableSchema?: string;
    perfEventsStatementsDigestTextLimit?: string;
    perfEventsStatementsLimit?: string;
    perfEventsStatementsTimeLimit?: string;
}

export interface ServiceIntegrationReadReplicaUserConfig {
}

export interface ServiceIntegrationRsyslogUserConfig {
}

export interface ServiceIntegrationSchemaRegistryProxyUserConfig {
}

export interface ServiceIntegrationSignalfxUserConfig {
}

export interface ServiceKafka {
    accessCert: string;
    accessKey: string;
    connectUri: string;
    restUri: string;
    schemaRegistryUri: string;
}

export interface ServiceKafkaConnect {
}

export interface ServiceKafkaConnectUserConfig {
    ipFilters?: string[];
    kafkaConnect?: outputs.ServiceKafkaConnectUserConfigKafkaConnect;
    privateAccess?: outputs.ServiceKafkaConnectUserConfigPrivateAccess;
    privatelinkAccess?: outputs.ServiceKafkaConnectUserConfigPrivatelinkAccess;
    publicAccess?: outputs.ServiceKafkaConnectUserConfigPublicAccess;
    staticIps?: string;
}

export interface ServiceKafkaConnectUserConfigKafkaConnect {
    connectorClientConfigOverridePolicy?: string;
    consumerAutoOffsetReset?: string;
    consumerFetchMaxBytes?: string;
    consumerIsolationLevel?: string;
    consumerMaxPartitionFetchBytes?: string;
    consumerMaxPollIntervalMs?: string;
    consumerMaxPollRecords?: string;
    offsetFlushIntervalMs?: string;
    offsetFlushTimeoutMs?: string;
    producerMaxRequestSize?: string;
    sessionTimeoutMs?: string;
}

export interface ServiceKafkaConnectUserConfigPrivateAccess {
    kafkaConnect?: string;
    prometheus?: string;
}

export interface ServiceKafkaConnectUserConfigPrivatelinkAccess {
    kafkaConnect?: string;
}

export interface ServiceKafkaConnectUserConfigPublicAccess {
    kafkaConnect?: string;
    prometheus?: string;
}

export interface ServiceKafkaMirrormaker {
}

export interface ServiceKafkaMirrormakerUserConfig {
    ipFilters?: string[];
    kafkaMirrormaker?: outputs.ServiceKafkaMirrormakerUserConfigKafkaMirrormaker;
    staticIps?: string;
}

export interface ServiceKafkaMirrormakerUserConfigKafkaMirrormaker {
    emitCheckpointsEnabled?: string;
    emitCheckpointsIntervalSeconds?: string;
    refreshGroupsEnabled?: string;
    refreshGroupsIntervalSeconds?: string;
    refreshTopicsEnabled?: string;
    refreshTopicsIntervalSeconds?: string;
    syncGroupOffsetsEnabled?: string;
    syncGroupOffsetsIntervalSeconds?: string;
    syncTopicConfigsEnabled?: string;
    tasksMaxPerCpu?: string;
}

export interface ServiceKafkaUserConfig {
    customDomain?: string;
    ipFilters?: string[];
    kafka?: outputs.ServiceKafkaUserConfigKafka;
    kafkaAuthenticationMethods?: outputs.ServiceKafkaUserConfigKafkaAuthenticationMethods;
    kafkaConnect?: string;
    kafkaConnectConfig?: outputs.ServiceKafkaUserConfigKafkaConnectConfig;
    kafkaRest?: string;
    kafkaRestConfig?: outputs.ServiceKafkaUserConfigKafkaRestConfig;
    kafkaVersion?: string;
    privateAccess?: outputs.ServiceKafkaUserConfigPrivateAccess;
    privatelinkAccess?: outputs.ServiceKafkaUserConfigPrivatelinkAccess;
    publicAccess?: outputs.ServiceKafkaUserConfigPublicAccess;
    schemaRegistry?: string;
    schemaRegistryConfig?: outputs.ServiceKafkaUserConfigSchemaRegistryConfig;
    staticIps?: string;
}

export interface ServiceKafkaUserConfigKafka {
    autoCreateTopicsEnable?: string;
    compressionType?: string;
    connectionsMaxIdleMs?: string;
    defaultReplicationFactor?: string;
    groupInitialRebalanceDelayMs?: string;
    groupMaxSessionTimeoutMs?: string;
    groupMinSessionTimeoutMs?: string;
    logCleanerDeleteRetentionMs?: string;
    logCleanerMaxCompactionLagMs?: string;
    logCleanerMinCleanableRatio?: string;
    logCleanerMinCompactionLagMs?: string;
    logCleanupPolicy?: string;
    logFlushIntervalMessages?: string;
    logFlushIntervalMs?: string;
    logIndexIntervalBytes?: string;
    logIndexSizeMaxBytes?: string;
    logMessageDownconversionEnable?: string;
    logMessageTimestampDifferenceMaxMs?: string;
    logMessageTimestampType?: string;
    logPreallocate?: string;
    logRetentionBytes?: string;
    logRetentionHours?: string;
    logRetentionMs?: string;
    logRollJitterMs?: string;
    logRollMs?: string;
    logSegmentBytes?: string;
    logSegmentDeleteDelayMs?: string;
    maxConnectionsPerIp?: string;
    maxIncrementalFetchSessionCacheSlots?: string;
    messageMaxBytes?: string;
    minInsyncReplicas?: string;
    numPartitions?: string;
    offsetsRetentionMinutes?: string;
    producerPurgatoryPurgeIntervalRequests?: string;
    replicaFetchMaxBytes?: string;
    replicaFetchResponseMaxBytes?: string;
    socketRequestMaxBytes?: string;
    transactionRemoveExpiredTransactionCleanupIntervalMs?: string;
    transactionStateLogSegmentBytes?: string;
}

export interface ServiceKafkaUserConfigKafkaAuthenticationMethods {
    certificate?: string;
    sasl?: string;
}

export interface ServiceKafkaUserConfigKafkaConnectConfig {
    connectorClientConfigOverridePolicy?: string;
    consumerAutoOffsetReset?: string;
    consumerFetchMaxBytes?: string;
    consumerIsolationLevel?: string;
    consumerMaxPartitionFetchBytes?: string;
    consumerMaxPollIntervalMs?: string;
    consumerMaxPollRecords?: string;
    offsetFlushIntervalMs?: string;
    offsetFlushTimeoutMs?: string;
    producerMaxRequestSize?: string;
    sessionTimeoutMs?: string;
}

export interface ServiceKafkaUserConfigKafkaRestConfig {
    consumerEnableAutoCommit?: string;
    consumerRequestMaxBytes?: string;
    consumerRequestTimeoutMs?: string;
    producerAcks?: string;
    producerLingerMs?: string;
    simpleconsumerPoolSizeMax?: string;
}

export interface ServiceKafkaUserConfigPrivateAccess {
    prometheus?: string;
}

export interface ServiceKafkaUserConfigPrivatelinkAccess {
    kafka?: string;
    kafkaConnect?: string;
    kafkaRest?: string;
    schemaRegistry?: string;
}

export interface ServiceKafkaUserConfigPublicAccess {
    kafka?: string;
    kafkaConnect?: string;
    kafkaRest?: string;
    prometheus?: string;
    schemaRegistry?: string;
}

export interface ServiceKafkaUserConfigSchemaRegistryConfig {
    leaderEligibility?: string;
    topicName?: string;
}

export interface ServiceMysql {
}

export interface ServiceMysqlUserConfig {
    adminPassword?: string;
    adminUsername?: string;
    backupHour?: string;
    backupMinute?: string;
    binlogRetentionPeriod?: string;
    ipFilters?: string[];
    migration?: outputs.ServiceMysqlUserConfigMigration;
    mysql?: outputs.ServiceMysqlUserConfigMysql;
    mysqlVersion?: string;
    privateAccess?: outputs.ServiceMysqlUserConfigPrivateAccess;
    privatelinkAccess?: outputs.ServiceMysqlUserConfigPrivatelinkAccess;
    projectToForkFrom?: string;
    publicAccess?: outputs.ServiceMysqlUserConfigPublicAccess;
    recoveryTargetTime?: string;
    serviceToForkFrom?: string;
    staticIps?: string;
}

export interface ServiceMysqlUserConfigMigration {
    dbname?: string;
    host?: string;
    ignoreDbs?: string;
    password?: string;
    port?: string;
    ssl?: string;
    username?: string;
}

export interface ServiceMysqlUserConfigMysql {
    connectTimeout?: string;
    defaultTimeZone?: string;
    groupConcatMaxLen?: string;
    informationSchemaStatsExpiry?: string;
    innodbFtMinTokenSize?: string;
    innodbFtServerStopwordTable?: string;
    innodbLockWaitTimeout?: string;
    innodbLogBufferSize?: string;
    innodbOnlineAlterLogMaxSize?: string;
    innodbPrintAllDeadlocks?: string;
    innodbRollbackOnTimeout?: string;
    interactiveTimeout?: string;
    internalTmpMemStorageEngine?: string;
    longQueryTime?: string;
    maxAllowedPacket?: string;
    maxHeapTableSize?: string;
    netReadTimeout?: string;
    netWriteTimeout?: string;
    slowQueryLog?: string;
    sortBufferSize?: string;
    sqlMode?: string;
    sqlRequirePrimaryKey?: string;
    tmpTableSize?: string;
    waitTimeout?: string;
}

export interface ServiceMysqlUserConfigPrivateAccess {
    mysql?: string;
    mysqlx?: string;
    prometheus?: string;
}

export interface ServiceMysqlUserConfigPrivatelinkAccess {
    mysql?: string;
    mysqlx?: string;
}

export interface ServiceMysqlUserConfigPublicAccess {
    mysql?: string;
    mysqlx?: string;
    prometheus?: string;
}

export interface ServiceOpensearch {
    opensearchDashboardsUri: string;
}

export interface ServiceOpensearchUserConfig {
    customDomain?: string;
    disableReplicationFactorAdjustment?: string;
    indexPatterns?: outputs.ServiceOpensearchUserConfigIndexPattern[];
    indexTemplate?: outputs.ServiceOpensearchUserConfigIndexTemplate;
    ipFilters?: string[];
    keepIndexRefreshInterval?: string;
    maxIndexCount?: string;
    opensearch?: outputs.ServiceOpensearchUserConfigOpensearch;
    opensearchDashboards?: outputs.ServiceOpensearchUserConfigOpensearchDashboards;
    opensearchVersion?: string;
    privateAccess?: outputs.ServiceOpensearchUserConfigPrivateAccess;
    privatelinkAccess?: outputs.ServiceOpensearchUserConfigPrivatelinkAccess;
    projectToForkFrom?: string;
    publicAccess?: outputs.ServiceOpensearchUserConfigPublicAccess;
    recoveryBasebackupName?: string;
    serviceToForkFrom?: string;
    staticIps?: string;
}

export interface ServiceOpensearchUserConfigIndexPattern {
    maxIndexCount?: string;
    pattern?: string;
    sortingAlgorithm?: string;
}

export interface ServiceOpensearchUserConfigIndexTemplate {
    mappingNestedObjectsLimit?: string;
    numberOfReplicas?: string;
    numberOfShards?: string;
}

export interface ServiceOpensearchUserConfigOpensearch {
    actionAutoCreateIndexEnabled?: string;
    actionDestructiveRequiresName?: string;
    clusterMaxShardsPerNode?: string;
    httpMaxContentLength?: string;
    httpMaxHeaderSize?: string;
    httpMaxInitialLineLength?: string;
    indicesFielddataCacheSize?: string;
    indicesMemoryIndexBufferSize?: string;
    indicesQueriesCacheSize?: string;
    indicesQueryBoolMaxClauseCount?: string;
    reindexRemoteWhitelists?: string[];
    searchMaxBuckets?: string;
    threadPoolAnalyzeQueueSize?: string;
    threadPoolAnalyzeSize?: string;
    threadPoolForceMergeSize?: string;
    threadPoolGetQueueSize?: string;
    threadPoolGetSize?: string;
    threadPoolIndexSize?: string;
    threadPoolSearchQueueSize?: string;
    threadPoolSearchSize?: string;
    threadPoolSearchThrottledQueueSize?: string;
    threadPoolSearchThrottledSize?: string;
    threadPoolWriteQueueSize?: string;
    threadPoolWriteSize?: string;
}

export interface ServiceOpensearchUserConfigOpensearchDashboards {
    enabled?: string;
    maxOldSpaceSize?: string;
    opensearchRequestTimeout?: string;
}

export interface ServiceOpensearchUserConfigPrivateAccess {
    opensearch?: string;
    opensearchDashboards?: string;
    prometheus?: string;
}

export interface ServiceOpensearchUserConfigPrivatelinkAccess {
    opensearch?: string;
    opensearchDashboards?: string;
}

export interface ServiceOpensearchUserConfigPublicAccess {
    opensearch?: string;
    opensearchDashboards?: string;
    prometheus?: string;
}

export interface ServicePg {
    dbname: string;
    host: string;
    password: string;
    port: number;
    replicaUri: string;
    sslmode: string;
    uri: string;
    user: string;
}

export interface ServicePgUserConfig {
    adminPassword?: string;
    adminUsername?: string;
    backupHour?: string;
    backupMinute?: string;
    ipFilters?: string[];
    migration?: outputs.ServicePgUserConfigMigration;
    pg?: outputs.ServicePgUserConfigPg;
    pgReadReplica?: string;
    pgServiceToForkFrom?: string;
    pgVersion?: string;
    pgbouncer?: outputs.ServicePgUserConfigPgbouncer;
    pglookout?: outputs.ServicePgUserConfigPglookout;
    privateAccess?: outputs.ServicePgUserConfigPrivateAccess;
    privatelinkAccess?: outputs.ServicePgUserConfigPrivatelinkAccess;
    projectToForkFrom?: string;
    publicAccess?: outputs.ServicePgUserConfigPublicAccess;
    recoveryTargetTime?: string;
    serviceToForkFrom?: string;
    sharedBuffersPercentage?: string;
    staticIps?: string;
    synchronousReplication?: string;
    timescaledb?: outputs.ServicePgUserConfigTimescaledb;
    variant?: string;
    workMem?: string;
}

export interface ServicePgUserConfigMigration {
    dbname?: string;
    host?: string;
    ignoreDbs?: string;
    password?: string;
    port?: string;
    ssl?: string;
    username?: string;
}

export interface ServicePgUserConfigPg {
    autovacuumAnalyzeScaleFactor?: string;
    autovacuumAnalyzeThreshold?: string;
    autovacuumFreezeMaxAge?: string;
    autovacuumMaxWorkers?: string;
    autovacuumNaptime?: string;
    autovacuumVacuumCostDelay?: string;
    autovacuumVacuumCostLimit?: string;
    autovacuumVacuumScaleFactor?: string;
    autovacuumVacuumThreshold?: string;
    bgwriterDelay?: string;
    bgwriterFlushAfter?: string;
    bgwriterLruMaxpages?: string;
    bgwriterLruMultiplier?: string;
    deadlockTimeout?: string;
    idleInTransactionSessionTimeout?: string;
    jit?: string;
    logAutovacuumMinDuration?: string;
    logErrorVerbosity?: string;
    logLinePrefix?: string;
    logMinDurationStatement?: string;
    maxFilesPerProcess?: string;
    maxLocksPerTransaction?: string;
    maxLogicalReplicationWorkers?: string;
    maxParallelWorkers?: string;
    maxParallelWorkersPerGather?: string;
    maxPredLocksPerTransaction?: string;
    maxPreparedTransactions?: string;
    maxReplicationSlots?: string;
    maxStackDepth?: string;
    maxStandbyArchiveDelay?: string;
    maxStandbyStreamingDelay?: string;
    maxWalSenders?: string;
    maxWorkerProcesses?: string;
    pgPartmanBgwInterval?: string;
    pgPartmanBgwRole?: string;
    pgStatStatementsTrack?: string;
    tempFileLimit?: string;
    timezone?: string;
    trackActivityQuerySize?: string;
    trackCommitTimestamp?: string;
    trackFunctions?: string;
    trackIoTiming?: string;
    walSenderTimeout?: string;
    walWriterDelay?: string;
}

export interface ServicePgUserConfigPgbouncer {
    autodbIdleTimeout?: string;
    autodbMaxDbConnections?: string;
    autodbPoolMode?: string;
    autodbPoolSize?: string;
    ignoreStartupParameters?: string[];
    minPoolSize?: string;
    serverIdleTimeout?: string;
    serverLifetime?: string;
    serverResetQueryAlways?: string;
}

export interface ServicePgUserConfigPglookout {
    maxFailoverReplicationTimeLag?: string;
}

export interface ServicePgUserConfigPrivateAccess {
    pg?: string;
    pgbouncer?: string;
    prometheus?: string;
}

export interface ServicePgUserConfigPrivatelinkAccess {
    pg?: string;
    pgbouncer?: string;
}

export interface ServicePgUserConfigPublicAccess {
    pg?: string;
    pgbouncer?: string;
    prometheus?: string;
}

export interface ServicePgUserConfigTimescaledb {
    maxBackgroundWorkers?: string;
}

export interface ServiceRedis {
}

export interface ServiceRedisUserConfig {
    ipFilters?: string[];
    migration?: outputs.ServiceRedisUserConfigMigration;
    privateAccess?: outputs.ServiceRedisUserConfigPrivateAccess;
    privatelinkAccess?: outputs.ServiceRedisUserConfigPrivatelinkAccess;
    projectToForkFrom?: string;
    publicAccess?: outputs.ServiceRedisUserConfigPublicAccess;
    recoveryBasebackupName?: string;
    redisAclChannelsDefault?: string;
    redisIoThreads?: string;
    redisLfuDecayTime?: string;
    redisLfuLogFactor?: string;
    redisMaxmemoryPolicy?: string;
    redisNotifyKeyspaceEvents?: string;
    redisNumberOfDatabases?: string;
    redisPersistence?: string;
    redisPubsubClientOutputBufferLimit?: string;
    redisSsl?: string;
    redisTimeout?: string;
    serviceToForkFrom?: string;
    staticIps?: string;
}

export interface ServiceRedisUserConfigMigration {
    dbname?: string;
    host?: string;
    ignoreDbs?: string;
    password?: string;
    port?: string;
    ssl?: string;
    username?: string;
}

export interface ServiceRedisUserConfigPrivateAccess {
    prometheus?: string;
    redis?: string;
}

export interface ServiceRedisUserConfigPrivatelinkAccess {
    redis?: string;
}

export interface ServiceRedisUserConfigPublicAccess {
    prometheus?: string;
    redis?: string;
}

export interface ServiceServiceIntegration {
    integrationType: string;
    sourceServiceName: string;
}

