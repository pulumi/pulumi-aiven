// *** WARNING: this file was generated by pulumi-java-gen. ***
// *** Do not edit by hand unless you're certain you know what you are doing! ***

package com.pulumi.aiven;

import com.pulumi.aiven.FlinkJobTableArgs;
import com.pulumi.aiven.Utilities;
import com.pulumi.aiven.inputs.FlinkJobTableState;
import com.pulumi.aiven.outputs.FlinkJobTableUpsertKafka;
import com.pulumi.core.Output;
import com.pulumi.core.annotations.Export;
import com.pulumi.core.annotations.ResourceType;
import com.pulumi.core.internal.Codegen;
import java.lang.String;
import java.util.List;
import java.util.Optional;
import javax.annotation.Nullable;

/**
 * The Flink Table resource allows the creation and management of Aiven Tables.
 * 
 * ## Example Usage
 * ```java
 * package generated_program;
 * 
 * import com.pulumi.Context;
 * import com.pulumi.Pulumi;
 * import com.pulumi.core.Output;
 * import com.pulumi.aiven.FlinkJobTable;
 * import com.pulumi.aiven.FlinkJobTableArgs;
 * import java.util.List;
 * import java.util.ArrayList;
 * import java.util.Map;
 * import java.io.File;
 * import java.nio.file.Files;
 * import java.nio.file.Paths;
 * 
 * public class App {
 *     public static void main(String[] args) {
 *         Pulumi.run(App::stack);
 *     }
 * 
 *     public static void stack(Context ctx) {
 *         var table = new FlinkJobTable(&#34;table&#34;, FlinkJobTableArgs.builder()        
 *             .project(data.aiven_project().pr1().project())
 *             .serviceName(aiven_flink.flink().service_name())
 *             .tableName(&#34;&lt;TABLE_NAME&gt;&#34;)
 *             .integrationId(aiven_service_integration.flink_kafka().service_id())
 *             .jdbcTable(&#34;&lt;JDBC_TABLE_NAME&gt;&#34;)
 *             .kafkaTopic(aiven_kafka_topic.table_topic().topic_name())
 *             .schemaSql(&#34;&#34;&#34;
 *       `+&#34;`cpu`&#34;+` INT,
 *       `+&#34;`node`&#34;+` INT,
 *       `+&#34;`occurred_at`&#34;+` TIMESTAMP(3) METADATA FROM &#39;timestamp&#39;,
 *       WATERMARK FOR `+&#34;`occurred_at`&#34;+` AS `+&#34;`occurred_at`&#34;+` - INTERVAL &#39;5&#39; SECOND
 *             &#34;&#34;&#34;)
 *             .build());
 * 
 *     }
 * }
 * ```
 * 
 * ## Import
 * 
 * ```sh
 *  $ pulumi import aiven:index/flinkJobTable:FlinkJobTable table project/service_name/table_id
 * ```
 * 
 */
@ResourceType(type="aiven:index/flinkJobTable:FlinkJobTable")
public class FlinkJobTable extends com.pulumi.resources.CustomResource {
    /**
     * The id of the service integration that is used with this table. It must have the service integration type `flink`. To set up proper dependencies please refer to this variable as a reference. This property cannot be changed, doing so forces recreation of the resource.
     * 
     */
    @Export(name="integrationId", type=String.class, parameters={})
    private Output<String> integrationId;

    /**
     * @return The id of the service integration that is used with this table. It must have the service integration type `flink`. To set up proper dependencies please refer to this variable as a reference. This property cannot be changed, doing so forces recreation of the resource.
     * 
     */
    public Output<String> integrationId() {
        return this.integrationId;
    }
    /**
     * Name of the jdbc table that is to be connected to this table. Valid if the service integration id refers to a mysql or postgres service. This property cannot be changed, doing so forces recreation of the resource.
     * 
     */
    @Export(name="jdbcTable", type=String.class, parameters={})
    private Output</* @Nullable */ String> jdbcTable;

    /**
     * @return Name of the jdbc table that is to be connected to this table. Valid if the service integration id refers to a mysql or postgres service. This property cannot be changed, doing so forces recreation of the resource.
     * 
     */
    public Output<Optional<String>> jdbcTable() {
        return Codegen.optional(this.jdbcTable);
    }
    /**
     * When used as a source, upsert Kafka connectors update values that use an existing key and delete values that are null. For sinks, the connector correspondingly writes update or delete messages in a compacted topic. If no matching key is found, the values are added as new entries. For more information, see the Apache Flink documentation The possible values are `kafka` and `upsert-kafka`. This property cannot be changed, doing so forces recreation of the resource.
     * 
     */
    @Export(name="kafkaConnectorType", type=String.class, parameters={})
    private Output</* @Nullable */ String> kafkaConnectorType;

    /**
     * @return When used as a source, upsert Kafka connectors update values that use an existing key and delete values that are null. For sinks, the connector correspondingly writes update or delete messages in a compacted topic. If no matching key is found, the values are added as new entries. For more information, see the Apache Flink documentation The possible values are `kafka` and `upsert-kafka`. This property cannot be changed, doing so forces recreation of the resource.
     * 
     */
    public Output<Optional<String>> kafkaConnectorType() {
        return Codegen.optional(this.kafkaConnectorType);
    }
    /**
     * Defines an explicit list of physical columns from the table schema that configure the data type for the key format. This property cannot be changed, doing so forces recreation of the resource.
     * 
     */
    @Export(name="kafkaKeyFields", type=List.class, parameters={String.class})
    private Output</* @Nullable */ List<String>> kafkaKeyFields;

    /**
     * @return Defines an explicit list of physical columns from the table schema that configure the data type for the key format. This property cannot be changed, doing so forces recreation of the resource.
     * 
     */
    public Output<Optional<List<String>>> kafkaKeyFields() {
        return Codegen.optional(this.kafkaKeyFields);
    }
    /**
     * Kafka Key Format The possible values are `avro`, `avro-confluent`, `debezium-avro-confluent`, `debezium-json` and `json`. This property cannot be changed, doing so forces recreation of the resource.
     * 
     */
    @Export(name="kafkaKeyFormat", type=String.class, parameters={})
    private Output</* @Nullable */ String> kafkaKeyFormat;

    /**
     * @return Kafka Key Format The possible values are `avro`, `avro-confluent`, `debezium-avro-confluent`, `debezium-json` and `json`. This property cannot be changed, doing so forces recreation of the resource.
     * 
     */
    public Output<Optional<String>> kafkaKeyFormat() {
        return Codegen.optional(this.kafkaKeyFormat);
    }
    /**
     * Startup mode The possible values are `earliest-offset`, `latest-offset`, `group-offsets` and `timestamp`. This property cannot be changed, doing so forces recreation of the resource.
     * 
     */
    @Export(name="kafkaStartupMode", type=String.class, parameters={})
    private Output</* @Nullable */ String> kafkaStartupMode;

    /**
     * @return Startup mode The possible values are `earliest-offset`, `latest-offset`, `group-offsets` and `timestamp`. This property cannot be changed, doing so forces recreation of the resource.
     * 
     */
    public Output<Optional<String>> kafkaStartupMode() {
        return Codegen.optional(this.kafkaStartupMode);
    }
    /**
     * Name of the kafka topic that is to be connected to this table. Valid if the service integration id refers to a kafka service. This property cannot be changed, doing so forces recreation of the resource.
     * 
     */
    @Export(name="kafkaTopic", type=String.class, parameters={})
    private Output</* @Nullable */ String> kafkaTopic;

    /**
     * @return Name of the kafka topic that is to be connected to this table. Valid if the service integration id refers to a kafka service. This property cannot be changed, doing so forces recreation of the resource.
     * 
     */
    public Output<Optional<String>> kafkaTopic() {
        return Codegen.optional(this.kafkaTopic);
    }
    /**
     * Controls how key columns are handled in the message value. Select ALL to include the physical columns of the table schema in the message value. Select EXCEPT_KEY to exclude the physical columns of the table schema from the message value. This is the default for upsert Kafka connectors. The possible values are `[ALL EXCEPT_KEY]`. This property cannot be changed, doing so forces recreation of the resource.
     * 
     */
    @Export(name="kafkaValueFieldsInclude", type=String.class, parameters={})
    private Output</* @Nullable */ String> kafkaValueFieldsInclude;

    /**
     * @return Controls how key columns are handled in the message value. Select ALL to include the physical columns of the table schema in the message value. Select EXCEPT_KEY to exclude the physical columns of the table schema from the message value. This is the default for upsert Kafka connectors. The possible values are `[ALL EXCEPT_KEY]`. This property cannot be changed, doing so forces recreation of the resource.
     * 
     */
    public Output<Optional<String>> kafkaValueFieldsInclude() {
        return Codegen.optional(this.kafkaValueFieldsInclude);
    }
    /**
     * Kafka Value Format The possible values are `avro`, `avro-confluent`, `debezium-avro-confluent`, `debezium-json` and `json`. This property cannot be changed, doing so forces recreation of the resource.
     * 
     */
    @Export(name="kafkaValueFormat", type=String.class, parameters={})
    private Output</* @Nullable */ String> kafkaValueFormat;

    /**
     * @return Kafka Value Format The possible values are `avro`, `avro-confluent`, `debezium-avro-confluent`, `debezium-json` and `json`. This property cannot be changed, doing so forces recreation of the resource.
     * 
     */
    public Output<Optional<String>> kafkaValueFormat() {
        return Codegen.optional(this.kafkaValueFormat);
    }
    /**
     * [LIKE](https://nightlies.apache.org/flink/flink-docs-master/docs/dev/table/sql/create/#like) statement for table creation. This property cannot be changed, doing so forces recreation of the resource.
     * 
     */
    @Export(name="likeOptions", type=String.class, parameters={})
    private Output</* @Nullable */ String> likeOptions;

    /**
     * @return [LIKE](https://nightlies.apache.org/flink/flink-docs-master/docs/dev/table/sql/create/#like) statement for table creation. This property cannot be changed, doing so forces recreation of the resource.
     * 
     */
    public Output<Optional<String>> likeOptions() {
        return Codegen.optional(this.likeOptions);
    }
    /**
     * For an OpenSearch table, the OpenSearch index the table outputs to. This property cannot be changed, doing so forces recreation of the resource.
     * 
     */
    @Export(name="opensearchIndex", type=String.class, parameters={})
    private Output</* @Nullable */ String> opensearchIndex;

    /**
     * @return For an OpenSearch table, the OpenSearch index the table outputs to. This property cannot be changed, doing so forces recreation of the resource.
     * 
     */
    public Output<Optional<String>> opensearchIndex() {
        return Codegen.optional(this.opensearchIndex);
    }
    /**
     * Identifies the project this resource belongs to. To set up proper dependencies please refer to this variable as a reference. This property cannot be changed, doing so forces recreation of the resource.
     * 
     */
    @Export(name="project", type=String.class, parameters={})
    private Output<String> project;

    /**
     * @return Identifies the project this resource belongs to. To set up proper dependencies please refer to this variable as a reference. This property cannot be changed, doing so forces recreation of the resource.
     * 
     */
    public Output<String> project() {
        return this.project;
    }
    /**
     * The SQL statement to create the table. This property cannot be changed, doing so forces recreation of the resource.
     * 
     */
    @Export(name="schemaSql", type=String.class, parameters={})
    private Output<String> schemaSql;

    /**
     * @return The SQL statement to create the table. This property cannot be changed, doing so forces recreation of the resource.
     * 
     */
    public Output<String> schemaSql() {
        return this.schemaSql;
    }
    /**
     * Specifies the name of the service that this resource belongs to. To set up proper dependencies please refer to this variable as a reference. This property cannot be changed, doing so forces recreation of the resource.
     * 
     */
    @Export(name="serviceName", type=String.class, parameters={})
    private Output<String> serviceName;

    /**
     * @return Specifies the name of the service that this resource belongs to. To set up proper dependencies please refer to this variable as a reference. This property cannot be changed, doing so forces recreation of the resource.
     * 
     */
    public Output<String> serviceName() {
        return this.serviceName;
    }
    /**
     * The Table ID of the flink table in the flink service.
     * 
     */
    @Export(name="tableId", type=String.class, parameters={})
    private Output<String> tableId;

    /**
     * @return The Table ID of the flink table in the flink service.
     * 
     */
    public Output<String> tableId() {
        return this.tableId;
    }
    /**
     * Specifies the name of the table. This property cannot be changed, doing so forces recreation of the resource.
     * 
     */
    @Export(name="tableName", type=String.class, parameters={})
    private Output<String> tableName;

    /**
     * @return Specifies the name of the table. This property cannot be changed, doing so forces recreation of the resource.
     * 
     */
    public Output<String> tableName() {
        return this.tableName;
    }
    /**
     * Kafka upsert connector configuration.
     * 
     */
    @Export(name="upsertKafka", type=FlinkJobTableUpsertKafka.class, parameters={})
    private Output</* @Nullable */ FlinkJobTableUpsertKafka> upsertKafka;

    /**
     * @return Kafka upsert connector configuration.
     * 
     */
    public Output<Optional<FlinkJobTableUpsertKafka>> upsertKafka() {
        return Codegen.optional(this.upsertKafka);
    }

    /**
     *
     * @param name The _unique_ name of the resulting resource.
     */
    public FlinkJobTable(String name) {
        this(name, FlinkJobTableArgs.Empty);
    }
    /**
     *
     * @param name The _unique_ name of the resulting resource.
     * @param args The arguments to use to populate this resource's properties.
     */
    public FlinkJobTable(String name, FlinkJobTableArgs args) {
        this(name, args, null);
    }
    /**
     *
     * @param name The _unique_ name of the resulting resource.
     * @param args The arguments to use to populate this resource's properties.
     * @param options A bag of options that control this resource's behavior.
     */
    public FlinkJobTable(String name, FlinkJobTableArgs args, @Nullable com.pulumi.resources.CustomResourceOptions options) {
        super("aiven:index/flinkJobTable:FlinkJobTable", name, args == null ? FlinkJobTableArgs.Empty : args, makeResourceOptions(options, Codegen.empty()));
    }

    private FlinkJobTable(String name, Output<String> id, @Nullable FlinkJobTableState state, @Nullable com.pulumi.resources.CustomResourceOptions options) {
        super("aiven:index/flinkJobTable:FlinkJobTable", name, state, makeResourceOptions(options, id));
    }

    private static com.pulumi.resources.CustomResourceOptions makeResourceOptions(@Nullable com.pulumi.resources.CustomResourceOptions options, @Nullable Output<String> id) {
        var defaultOptions = com.pulumi.resources.CustomResourceOptions.builder()
            .version(Utilities.getVersion())
            .build();
        return com.pulumi.resources.CustomResourceOptions.merge(defaultOptions, options, id);
    }

    /**
     * Get an existing Host resource's state with the given name, ID, and optional extra
     * properties used to qualify the lookup.
     *
     * @param name The _unique_ name of the resulting resource.
     * @param id The _unique_ provider ID of the resource to lookup.
     * @param state
     * @param options Optional settings to control the behavior of the CustomResource.
     */
    public static FlinkJobTable get(String name, Output<String> id, @Nullable FlinkJobTableState state, @Nullable com.pulumi.resources.CustomResourceOptions options) {
        return new FlinkJobTable(name, id, state, options);
    }
}
