// *** WARNING: this file was generated by pulumi-language-java. ***
// *** Do not edit by hand unless you're certain you know what you are doing! ***

package com.pulumi.aiven.inputs;

import com.pulumi.aiven.inputs.ServiceIntegrationClickhouseKafkaUserConfigTableColumnArgs;
import com.pulumi.aiven.inputs.ServiceIntegrationClickhouseKafkaUserConfigTableTopicArgs;
import com.pulumi.core.Output;
import com.pulumi.core.annotations.Import;
import com.pulumi.exceptions.MissingRequiredPropertyException;
import java.lang.Boolean;
import java.lang.Integer;
import java.lang.String;
import java.util.List;
import java.util.Objects;
import java.util.Optional;
import javax.annotation.Nullable;


public final class ServiceIntegrationClickhouseKafkaUserConfigTableArgs extends com.pulumi.resources.ResourceArgs {

    public static final ServiceIntegrationClickhouseKafkaUserConfigTableArgs Empty = new ServiceIntegrationClickhouseKafkaUserConfigTableArgs();

    /**
     * Enum: `beginning`, `earliest`, `end`, `largest`, `latest`, `smallest`. Determines where to start reading from Kafka when no offset is stored or the stored offset is out of range. `earliest` starts from the beginning, `latest` starts from the end. Default: `earliest`.
     * 
     */
    @Import(name="autoOffsetReset")
    private @Nullable Output<String> autoOffsetReset;

    /**
     * @return Enum: `beginning`, `earliest`, `end`, `largest`, `latest`, `smallest`. Determines where to start reading from Kafka when no offset is stored or the stored offset is out of range. `earliest` starts from the beginning, `latest` starts from the end. Default: `earliest`.
     * 
     */
    public Optional<Output<String>> autoOffsetReset() {
        return Optional.ofNullable(this.autoOffsetReset);
    }

    /**
     * Array of column definitions that specify the structure of the ClickHouse table. Each column maps to a field in the Kafka messages
     * 
     */
    @Import(name="columns", required=true)
    private Output<List<ServiceIntegrationClickhouseKafkaUserConfigTableColumnArgs>> columns;

    /**
     * @return Array of column definitions that specify the structure of the ClickHouse table. Each column maps to a field in the Kafka messages
     * 
     */
    public Output<List<ServiceIntegrationClickhouseKafkaUserConfigTableColumnArgs>> columns() {
        return this.columns;
    }

    /**
     * Enum: `Avro`, `AvroConfluent`, `CSV`, `JSONAsString`, `JSONCompactEachRow`, `JSONCompactStringsEachRow`, `JSONEachRow`, `JSONStringsEachRow`, `MsgPack`, `Parquet`, `RawBLOB`, `TSKV`, `TSV`, `TabSeparated`. The format of the messages in the Kafka topics. Determines how ClickHouse parses and serializes the data (e.g., JSON, CSV, Avro). Default: `JSONEachRow`.
     * 
     */
    @Import(name="dataFormat", required=true)
    private Output<String> dataFormat;

    /**
     * @return Enum: `Avro`, `AvroConfluent`, `CSV`, `JSONAsString`, `JSONCompactEachRow`, `JSONCompactStringsEachRow`, `JSONEachRow`, `JSONStringsEachRow`, `MsgPack`, `Parquet`, `RawBLOB`, `TSKV`, `TSV`, `TabSeparated`. The format of the messages in the Kafka topics. Determines how ClickHouse parses and serializes the data (e.g., JSON, CSV, Avro). Default: `JSONEachRow`.
     * 
     */
    public Output<String> dataFormat() {
        return this.dataFormat;
    }

    /**
     * Enum: `basic`, `bestEffort`, `bestEffortUs`. Specifies how ClickHouse should parse DateTime values from text-based input formats. `basic` uses simple parsing, `bestEffort` attempts more flexible parsing. Default: `basic`.
     * 
     */
    @Import(name="dateTimeInputFormat")
    private @Nullable Output<String> dateTimeInputFormat;

    /**
     * @return Enum: `basic`, `bestEffort`, `bestEffortUs`. Specifies how ClickHouse should parse DateTime values from text-based input formats. `basic` uses simple parsing, `bestEffort` attempts more flexible parsing. Default: `basic`.
     * 
     */
    public Optional<Output<String>> dateTimeInputFormat() {
        return Optional.ofNullable(this.dateTimeInputFormat);
    }

    /**
     * The Kafka consumer group name. Multiple consumers with the same group name will share the workload and maintain offset positions. Default: `clickhouse`.
     * 
     */
    @Import(name="groupName", required=true)
    private Output<String> groupName;

    /**
     * @return The Kafka consumer group name. Multiple consumers with the same group name will share the workload and maintain offset positions. Default: `clickhouse`.
     * 
     */
    public Output<String> groupName() {
        return this.groupName;
    }

    /**
     * Enum: `default`, `stream`. Defines how ClickHouse should handle errors when processing Kafka messages. `default` stops on errors, `stream` continues processing and logs errors. Default: `default`.
     * 
     */
    @Import(name="handleErrorMode")
    private @Nullable Output<String> handleErrorMode;

    /**
     * @return Enum: `default`, `stream`. Defines how ClickHouse should handle errors when processing Kafka messages. `default` stops on errors, `stream` continues processing and logs errors. Default: `default`.
     * 
     */
    public Optional<Output<String>> handleErrorMode() {
        return Optional.ofNullable(this.handleErrorMode);
    }

    /**
     * Maximum number of rows to collect before flushing data between Kafka and ClickHouse. Default: `0`.
     * 
     */
    @Import(name="maxBlockSize")
    private @Nullable Output<Integer> maxBlockSize;

    /**
     * @return Maximum number of rows to collect before flushing data between Kafka and ClickHouse. Default: `0`.
     * 
     */
    public Optional<Output<Integer>> maxBlockSize() {
        return Optional.ofNullable(this.maxBlockSize);
    }

    /**
     * Maximum number of rows that can be processed from a single Kafka message for row-based formats. Useful for controlling memory usage. Default: `1`.
     * 
     */
    @Import(name="maxRowsPerMessage")
    private @Nullable Output<Integer> maxRowsPerMessage;

    /**
     * @return Maximum number of rows that can be processed from a single Kafka message for row-based formats. Useful for controlling memory usage. Default: `1`.
     * 
     */
    public Optional<Output<Integer>> maxRowsPerMessage() {
        return Optional.ofNullable(this.maxRowsPerMessage);
    }

    /**
     * The name of the ClickHouse table to be created. This table can consume data from and write data to the specified Kafka topics. Example: `events`.
     * 
     */
    @Import(name="name", required=true)
    private Output<String> name;

    /**
     * @return The name of the ClickHouse table to be created. This table can consume data from and write data to the specified Kafka topics. Example: `events`.
     * 
     */
    public Output<String> name() {
        return this.name;
    }

    /**
     * Number of Kafka consumers to run per table per replica. Increasing this can improve throughput but may increase resource usage. Default: `1`.
     * 
     */
    @Import(name="numConsumers")
    private @Nullable Output<Integer> numConsumers;

    /**
     * @return Number of Kafka consumers to run per table per replica. Increasing this can improve throughput but may increase resource usage. Default: `1`.
     * 
     */
    public Optional<Output<Integer>> numConsumers() {
        return Optional.ofNullable(this.numConsumers);
    }

    /**
     * Maximum number of messages to fetch in a single Kafka poll operation for reading. Default: `0`.
     * 
     */
    @Import(name="pollMaxBatchSize")
    private @Nullable Output<Integer> pollMaxBatchSize;

    /**
     * @return Maximum number of messages to fetch in a single Kafka poll operation for reading. Default: `0`.
     * 
     */
    public Optional<Output<Integer>> pollMaxBatchSize() {
        return Optional.ofNullable(this.pollMaxBatchSize);
    }

    /**
     * Timeout in milliseconds for a single poll from Kafka. Takes the value of the stream*flush*interval_ms server setting by default (500ms). Default: `0`.
     * 
     */
    @Import(name="pollMaxTimeoutMs")
    private @Nullable Output<Integer> pollMaxTimeoutMs;

    /**
     * @return Timeout in milliseconds for a single poll from Kafka. Takes the value of the stream*flush*interval_ms server setting by default (500ms). Default: `0`.
     * 
     */
    public Optional<Output<Integer>> pollMaxTimeoutMs() {
        return Optional.ofNullable(this.pollMaxTimeoutMs);
    }

    /**
     * The maximum number of messages in a batch sent to Kafka. If the number of messages exceeds this value, the batch is sent. Default: `10000`.
     * 
     */
    @Import(name="producerBatchNumMessages")
    private @Nullable Output<Integer> producerBatchNumMessages;

    /**
     * @return The maximum number of messages in a batch sent to Kafka. If the number of messages exceeds this value, the batch is sent. Default: `10000`.
     * 
     */
    public Optional<Output<Integer>> producerBatchNumMessages() {
        return Optional.ofNullable(this.producerBatchNumMessages);
    }

    /**
     * The maximum size in bytes of a batch of messages sent to Kafka. If the batch size is exceeded, the batch is sent.
     * 
     */
    @Import(name="producerBatchSize")
    private @Nullable Output<Integer> producerBatchSize;

    /**
     * @return The maximum size in bytes of a batch of messages sent to Kafka. If the batch size is exceeded, the batch is sent.
     * 
     */
    public Optional<Output<Integer>> producerBatchSize() {
        return Optional.ofNullable(this.producerBatchSize);
    }

    /**
     * Enum: `gzip`, `lz4`, `none`, `snappy`, `zstd`. The compression codec to use when sending a batch of messages to Kafka. Default: `none`.
     * 
     */
    @Import(name="producerCompressionCodec")
    private @Nullable Output<String> producerCompressionCodec;

    /**
     * @return Enum: `gzip`, `lz4`, `none`, `snappy`, `zstd`. The compression codec to use when sending a batch of messages to Kafka. Default: `none`.
     * 
     */
    public Optional<Output<String>> producerCompressionCodec() {
        return Optional.ofNullable(this.producerCompressionCodec);
    }

    /**
     * The compression level to use when sending a batch of messages to Kafka. Usable range is algorithm-dependent: [0-9] for gzip; [0-12] for lz4; only 0 for snappy; -1 = codec-dependent default compression level. Default: `-1`.
     * 
     */
    @Import(name="producerCompressionLevel")
    private @Nullable Output<Integer> producerCompressionLevel;

    /**
     * @return The compression level to use when sending a batch of messages to Kafka. Usable range is algorithm-dependent: [0-9] for gzip; [0-12] for lz4; only 0 for snappy; -1 = codec-dependent default compression level. Default: `-1`.
     * 
     */
    public Optional<Output<Integer>> producerCompressionLevel() {
        return Optional.ofNullable(this.producerCompressionLevel);
    }

    /**
     * The time in milliseconds to wait for additional messages before sending a batch. If the time is exceeded, the batch is sent. Default: `5`.
     * 
     */
    @Import(name="producerLingerMs")
    private @Nullable Output<Integer> producerLingerMs;

    /**
     * @return The time in milliseconds to wait for additional messages before sending a batch. If the time is exceeded, the batch is sent. Default: `5`.
     * 
     */
    public Optional<Output<Integer>> producerLingerMs() {
        return Optional.ofNullable(this.producerLingerMs);
    }

    /**
     * The maximum size of the buffer in kilobytes before sending.
     * 
     */
    @Import(name="producerQueueBufferingMaxKbytes")
    private @Nullable Output<Integer> producerQueueBufferingMaxKbytes;

    /**
     * @return The maximum size of the buffer in kilobytes before sending.
     * 
     */
    public Optional<Output<Integer>> producerQueueBufferingMaxKbytes() {
        return Optional.ofNullable(this.producerQueueBufferingMaxKbytes);
    }

    /**
     * The maximum number of messages to buffer before sending. Default: `100000`.
     * 
     */
    @Import(name="producerQueueBufferingMaxMessages")
    private @Nullable Output<Integer> producerQueueBufferingMaxMessages;

    /**
     * @return The maximum number of messages to buffer before sending. Default: `100000`.
     * 
     */
    public Optional<Output<Integer>> producerQueueBufferingMaxMessages() {
        return Optional.ofNullable(this.producerQueueBufferingMaxMessages);
    }

    /**
     * The number of acknowledgements the leader broker must receive from ISR brokers before responding to the request: 0=Broker does not send any response/ack to client, -1 will block until message is committed by all in sync replicas (ISRs). Default: `-1`.
     * 
     */
    @Import(name="producerRequestRequiredAcks")
    private @Nullable Output<Integer> producerRequestRequiredAcks;

    /**
     * @return The number of acknowledgements the leader broker must receive from ISR brokers before responding to the request: 0=Broker does not send any response/ack to client, -1 will block until message is committed by all in sync replicas (ISRs). Default: `-1`.
     * 
     */
    public Optional<Output<Integer>> producerRequestRequiredAcks() {
        return Optional.ofNullable(this.producerRequestRequiredAcks);
    }

    /**
     * Number of broken messages to skip before stopping processing when reading from Kafka. Useful for handling corrupted data without failing the entire integration. Default: `0`.
     * 
     */
    @Import(name="skipBrokenMessages")
    private @Nullable Output<Integer> skipBrokenMessages;

    /**
     * @return Number of broken messages to skip before stopping processing when reading from Kafka. Useful for handling corrupted data without failing the entire integration. Default: `0`.
     * 
     */
    public Optional<Output<Integer>> skipBrokenMessages() {
        return Optional.ofNullable(this.skipBrokenMessages);
    }

    /**
     * When enabled, each consumer runs in its own thread, providing better isolation and potentially better performance for high-throughput scenarios. Default: `false`.
     * 
     */
    @Import(name="threadPerConsumer")
    private @Nullable Output<Boolean> threadPerConsumer;

    /**
     * @return When enabled, each consumer runs in its own thread, providing better isolation and potentially better performance for high-throughput scenarios. Default: `false`.
     * 
     */
    public Optional<Output<Boolean>> threadPerConsumer() {
        return Optional.ofNullable(this.threadPerConsumer);
    }

    /**
     * Array of Kafka topics that this table will read data from or write data to. Messages from all specified topics will be inserted into this table, and data inserted into this table will be published to the topics
     * 
     */
    @Import(name="topics", required=true)
    private Output<List<ServiceIntegrationClickhouseKafkaUserConfigTableTopicArgs>> topics;

    /**
     * @return Array of Kafka topics that this table will read data from or write data to. Messages from all specified topics will be inserted into this table, and data inserted into this table will be published to the topics
     * 
     */
    public Output<List<ServiceIntegrationClickhouseKafkaUserConfigTableTopicArgs>> topics() {
        return this.topics;
    }

    private ServiceIntegrationClickhouseKafkaUserConfigTableArgs() {}

    private ServiceIntegrationClickhouseKafkaUserConfigTableArgs(ServiceIntegrationClickhouseKafkaUserConfigTableArgs $) {
        this.autoOffsetReset = $.autoOffsetReset;
        this.columns = $.columns;
        this.dataFormat = $.dataFormat;
        this.dateTimeInputFormat = $.dateTimeInputFormat;
        this.groupName = $.groupName;
        this.handleErrorMode = $.handleErrorMode;
        this.maxBlockSize = $.maxBlockSize;
        this.maxRowsPerMessage = $.maxRowsPerMessage;
        this.name = $.name;
        this.numConsumers = $.numConsumers;
        this.pollMaxBatchSize = $.pollMaxBatchSize;
        this.pollMaxTimeoutMs = $.pollMaxTimeoutMs;
        this.producerBatchNumMessages = $.producerBatchNumMessages;
        this.producerBatchSize = $.producerBatchSize;
        this.producerCompressionCodec = $.producerCompressionCodec;
        this.producerCompressionLevel = $.producerCompressionLevel;
        this.producerLingerMs = $.producerLingerMs;
        this.producerQueueBufferingMaxKbytes = $.producerQueueBufferingMaxKbytes;
        this.producerQueueBufferingMaxMessages = $.producerQueueBufferingMaxMessages;
        this.producerRequestRequiredAcks = $.producerRequestRequiredAcks;
        this.skipBrokenMessages = $.skipBrokenMessages;
        this.threadPerConsumer = $.threadPerConsumer;
        this.topics = $.topics;
    }

    public static Builder builder() {
        return new Builder();
    }
    public static Builder builder(ServiceIntegrationClickhouseKafkaUserConfigTableArgs defaults) {
        return new Builder(defaults);
    }

    public static final class Builder {
        private ServiceIntegrationClickhouseKafkaUserConfigTableArgs $;

        public Builder() {
            $ = new ServiceIntegrationClickhouseKafkaUserConfigTableArgs();
        }

        public Builder(ServiceIntegrationClickhouseKafkaUserConfigTableArgs defaults) {
            $ = new ServiceIntegrationClickhouseKafkaUserConfigTableArgs(Objects.requireNonNull(defaults));
        }

        /**
         * @param autoOffsetReset Enum: `beginning`, `earliest`, `end`, `largest`, `latest`, `smallest`. Determines where to start reading from Kafka when no offset is stored or the stored offset is out of range. `earliest` starts from the beginning, `latest` starts from the end. Default: `earliest`.
         * 
         * @return builder
         * 
         */
        public Builder autoOffsetReset(@Nullable Output<String> autoOffsetReset) {
            $.autoOffsetReset = autoOffsetReset;
            return this;
        }

        /**
         * @param autoOffsetReset Enum: `beginning`, `earliest`, `end`, `largest`, `latest`, `smallest`. Determines where to start reading from Kafka when no offset is stored or the stored offset is out of range. `earliest` starts from the beginning, `latest` starts from the end. Default: `earliest`.
         * 
         * @return builder
         * 
         */
        public Builder autoOffsetReset(String autoOffsetReset) {
            return autoOffsetReset(Output.of(autoOffsetReset));
        }

        /**
         * @param columns Array of column definitions that specify the structure of the ClickHouse table. Each column maps to a field in the Kafka messages
         * 
         * @return builder
         * 
         */
        public Builder columns(Output<List<ServiceIntegrationClickhouseKafkaUserConfigTableColumnArgs>> columns) {
            $.columns = columns;
            return this;
        }

        /**
         * @param columns Array of column definitions that specify the structure of the ClickHouse table. Each column maps to a field in the Kafka messages
         * 
         * @return builder
         * 
         */
        public Builder columns(List<ServiceIntegrationClickhouseKafkaUserConfigTableColumnArgs> columns) {
            return columns(Output.of(columns));
        }

        /**
         * @param columns Array of column definitions that specify the structure of the ClickHouse table. Each column maps to a field in the Kafka messages
         * 
         * @return builder
         * 
         */
        public Builder columns(ServiceIntegrationClickhouseKafkaUserConfigTableColumnArgs... columns) {
            return columns(List.of(columns));
        }

        /**
         * @param dataFormat Enum: `Avro`, `AvroConfluent`, `CSV`, `JSONAsString`, `JSONCompactEachRow`, `JSONCompactStringsEachRow`, `JSONEachRow`, `JSONStringsEachRow`, `MsgPack`, `Parquet`, `RawBLOB`, `TSKV`, `TSV`, `TabSeparated`. The format of the messages in the Kafka topics. Determines how ClickHouse parses and serializes the data (e.g., JSON, CSV, Avro). Default: `JSONEachRow`.
         * 
         * @return builder
         * 
         */
        public Builder dataFormat(Output<String> dataFormat) {
            $.dataFormat = dataFormat;
            return this;
        }

        /**
         * @param dataFormat Enum: `Avro`, `AvroConfluent`, `CSV`, `JSONAsString`, `JSONCompactEachRow`, `JSONCompactStringsEachRow`, `JSONEachRow`, `JSONStringsEachRow`, `MsgPack`, `Parquet`, `RawBLOB`, `TSKV`, `TSV`, `TabSeparated`. The format of the messages in the Kafka topics. Determines how ClickHouse parses and serializes the data (e.g., JSON, CSV, Avro). Default: `JSONEachRow`.
         * 
         * @return builder
         * 
         */
        public Builder dataFormat(String dataFormat) {
            return dataFormat(Output.of(dataFormat));
        }

        /**
         * @param dateTimeInputFormat Enum: `basic`, `bestEffort`, `bestEffortUs`. Specifies how ClickHouse should parse DateTime values from text-based input formats. `basic` uses simple parsing, `bestEffort` attempts more flexible parsing. Default: `basic`.
         * 
         * @return builder
         * 
         */
        public Builder dateTimeInputFormat(@Nullable Output<String> dateTimeInputFormat) {
            $.dateTimeInputFormat = dateTimeInputFormat;
            return this;
        }

        /**
         * @param dateTimeInputFormat Enum: `basic`, `bestEffort`, `bestEffortUs`. Specifies how ClickHouse should parse DateTime values from text-based input formats. `basic` uses simple parsing, `bestEffort` attempts more flexible parsing. Default: `basic`.
         * 
         * @return builder
         * 
         */
        public Builder dateTimeInputFormat(String dateTimeInputFormat) {
            return dateTimeInputFormat(Output.of(dateTimeInputFormat));
        }

        /**
         * @param groupName The Kafka consumer group name. Multiple consumers with the same group name will share the workload and maintain offset positions. Default: `clickhouse`.
         * 
         * @return builder
         * 
         */
        public Builder groupName(Output<String> groupName) {
            $.groupName = groupName;
            return this;
        }

        /**
         * @param groupName The Kafka consumer group name. Multiple consumers with the same group name will share the workload and maintain offset positions. Default: `clickhouse`.
         * 
         * @return builder
         * 
         */
        public Builder groupName(String groupName) {
            return groupName(Output.of(groupName));
        }

        /**
         * @param handleErrorMode Enum: `default`, `stream`. Defines how ClickHouse should handle errors when processing Kafka messages. `default` stops on errors, `stream` continues processing and logs errors. Default: `default`.
         * 
         * @return builder
         * 
         */
        public Builder handleErrorMode(@Nullable Output<String> handleErrorMode) {
            $.handleErrorMode = handleErrorMode;
            return this;
        }

        /**
         * @param handleErrorMode Enum: `default`, `stream`. Defines how ClickHouse should handle errors when processing Kafka messages. `default` stops on errors, `stream` continues processing and logs errors. Default: `default`.
         * 
         * @return builder
         * 
         */
        public Builder handleErrorMode(String handleErrorMode) {
            return handleErrorMode(Output.of(handleErrorMode));
        }

        /**
         * @param maxBlockSize Maximum number of rows to collect before flushing data between Kafka and ClickHouse. Default: `0`.
         * 
         * @return builder
         * 
         */
        public Builder maxBlockSize(@Nullable Output<Integer> maxBlockSize) {
            $.maxBlockSize = maxBlockSize;
            return this;
        }

        /**
         * @param maxBlockSize Maximum number of rows to collect before flushing data between Kafka and ClickHouse. Default: `0`.
         * 
         * @return builder
         * 
         */
        public Builder maxBlockSize(Integer maxBlockSize) {
            return maxBlockSize(Output.of(maxBlockSize));
        }

        /**
         * @param maxRowsPerMessage Maximum number of rows that can be processed from a single Kafka message for row-based formats. Useful for controlling memory usage. Default: `1`.
         * 
         * @return builder
         * 
         */
        public Builder maxRowsPerMessage(@Nullable Output<Integer> maxRowsPerMessage) {
            $.maxRowsPerMessage = maxRowsPerMessage;
            return this;
        }

        /**
         * @param maxRowsPerMessage Maximum number of rows that can be processed from a single Kafka message for row-based formats. Useful for controlling memory usage. Default: `1`.
         * 
         * @return builder
         * 
         */
        public Builder maxRowsPerMessage(Integer maxRowsPerMessage) {
            return maxRowsPerMessage(Output.of(maxRowsPerMessage));
        }

        /**
         * @param name The name of the ClickHouse table to be created. This table can consume data from and write data to the specified Kafka topics. Example: `events`.
         * 
         * @return builder
         * 
         */
        public Builder name(Output<String> name) {
            $.name = name;
            return this;
        }

        /**
         * @param name The name of the ClickHouse table to be created. This table can consume data from and write data to the specified Kafka topics. Example: `events`.
         * 
         * @return builder
         * 
         */
        public Builder name(String name) {
            return name(Output.of(name));
        }

        /**
         * @param numConsumers Number of Kafka consumers to run per table per replica. Increasing this can improve throughput but may increase resource usage. Default: `1`.
         * 
         * @return builder
         * 
         */
        public Builder numConsumers(@Nullable Output<Integer> numConsumers) {
            $.numConsumers = numConsumers;
            return this;
        }

        /**
         * @param numConsumers Number of Kafka consumers to run per table per replica. Increasing this can improve throughput but may increase resource usage. Default: `1`.
         * 
         * @return builder
         * 
         */
        public Builder numConsumers(Integer numConsumers) {
            return numConsumers(Output.of(numConsumers));
        }

        /**
         * @param pollMaxBatchSize Maximum number of messages to fetch in a single Kafka poll operation for reading. Default: `0`.
         * 
         * @return builder
         * 
         */
        public Builder pollMaxBatchSize(@Nullable Output<Integer> pollMaxBatchSize) {
            $.pollMaxBatchSize = pollMaxBatchSize;
            return this;
        }

        /**
         * @param pollMaxBatchSize Maximum number of messages to fetch in a single Kafka poll operation for reading. Default: `0`.
         * 
         * @return builder
         * 
         */
        public Builder pollMaxBatchSize(Integer pollMaxBatchSize) {
            return pollMaxBatchSize(Output.of(pollMaxBatchSize));
        }

        /**
         * @param pollMaxTimeoutMs Timeout in milliseconds for a single poll from Kafka. Takes the value of the stream*flush*interval_ms server setting by default (500ms). Default: `0`.
         * 
         * @return builder
         * 
         */
        public Builder pollMaxTimeoutMs(@Nullable Output<Integer> pollMaxTimeoutMs) {
            $.pollMaxTimeoutMs = pollMaxTimeoutMs;
            return this;
        }

        /**
         * @param pollMaxTimeoutMs Timeout in milliseconds for a single poll from Kafka. Takes the value of the stream*flush*interval_ms server setting by default (500ms). Default: `0`.
         * 
         * @return builder
         * 
         */
        public Builder pollMaxTimeoutMs(Integer pollMaxTimeoutMs) {
            return pollMaxTimeoutMs(Output.of(pollMaxTimeoutMs));
        }

        /**
         * @param producerBatchNumMessages The maximum number of messages in a batch sent to Kafka. If the number of messages exceeds this value, the batch is sent. Default: `10000`.
         * 
         * @return builder
         * 
         */
        public Builder producerBatchNumMessages(@Nullable Output<Integer> producerBatchNumMessages) {
            $.producerBatchNumMessages = producerBatchNumMessages;
            return this;
        }

        /**
         * @param producerBatchNumMessages The maximum number of messages in a batch sent to Kafka. If the number of messages exceeds this value, the batch is sent. Default: `10000`.
         * 
         * @return builder
         * 
         */
        public Builder producerBatchNumMessages(Integer producerBatchNumMessages) {
            return producerBatchNumMessages(Output.of(producerBatchNumMessages));
        }

        /**
         * @param producerBatchSize The maximum size in bytes of a batch of messages sent to Kafka. If the batch size is exceeded, the batch is sent.
         * 
         * @return builder
         * 
         */
        public Builder producerBatchSize(@Nullable Output<Integer> producerBatchSize) {
            $.producerBatchSize = producerBatchSize;
            return this;
        }

        /**
         * @param producerBatchSize The maximum size in bytes of a batch of messages sent to Kafka. If the batch size is exceeded, the batch is sent.
         * 
         * @return builder
         * 
         */
        public Builder producerBatchSize(Integer producerBatchSize) {
            return producerBatchSize(Output.of(producerBatchSize));
        }

        /**
         * @param producerCompressionCodec Enum: `gzip`, `lz4`, `none`, `snappy`, `zstd`. The compression codec to use when sending a batch of messages to Kafka. Default: `none`.
         * 
         * @return builder
         * 
         */
        public Builder producerCompressionCodec(@Nullable Output<String> producerCompressionCodec) {
            $.producerCompressionCodec = producerCompressionCodec;
            return this;
        }

        /**
         * @param producerCompressionCodec Enum: `gzip`, `lz4`, `none`, `snappy`, `zstd`. The compression codec to use when sending a batch of messages to Kafka. Default: `none`.
         * 
         * @return builder
         * 
         */
        public Builder producerCompressionCodec(String producerCompressionCodec) {
            return producerCompressionCodec(Output.of(producerCompressionCodec));
        }

        /**
         * @param producerCompressionLevel The compression level to use when sending a batch of messages to Kafka. Usable range is algorithm-dependent: [0-9] for gzip; [0-12] for lz4; only 0 for snappy; -1 = codec-dependent default compression level. Default: `-1`.
         * 
         * @return builder
         * 
         */
        public Builder producerCompressionLevel(@Nullable Output<Integer> producerCompressionLevel) {
            $.producerCompressionLevel = producerCompressionLevel;
            return this;
        }

        /**
         * @param producerCompressionLevel The compression level to use when sending a batch of messages to Kafka. Usable range is algorithm-dependent: [0-9] for gzip; [0-12] for lz4; only 0 for snappy; -1 = codec-dependent default compression level. Default: `-1`.
         * 
         * @return builder
         * 
         */
        public Builder producerCompressionLevel(Integer producerCompressionLevel) {
            return producerCompressionLevel(Output.of(producerCompressionLevel));
        }

        /**
         * @param producerLingerMs The time in milliseconds to wait for additional messages before sending a batch. If the time is exceeded, the batch is sent. Default: `5`.
         * 
         * @return builder
         * 
         */
        public Builder producerLingerMs(@Nullable Output<Integer> producerLingerMs) {
            $.producerLingerMs = producerLingerMs;
            return this;
        }

        /**
         * @param producerLingerMs The time in milliseconds to wait for additional messages before sending a batch. If the time is exceeded, the batch is sent. Default: `5`.
         * 
         * @return builder
         * 
         */
        public Builder producerLingerMs(Integer producerLingerMs) {
            return producerLingerMs(Output.of(producerLingerMs));
        }

        /**
         * @param producerQueueBufferingMaxKbytes The maximum size of the buffer in kilobytes before sending.
         * 
         * @return builder
         * 
         */
        public Builder producerQueueBufferingMaxKbytes(@Nullable Output<Integer> producerQueueBufferingMaxKbytes) {
            $.producerQueueBufferingMaxKbytes = producerQueueBufferingMaxKbytes;
            return this;
        }

        /**
         * @param producerQueueBufferingMaxKbytes The maximum size of the buffer in kilobytes before sending.
         * 
         * @return builder
         * 
         */
        public Builder producerQueueBufferingMaxKbytes(Integer producerQueueBufferingMaxKbytes) {
            return producerQueueBufferingMaxKbytes(Output.of(producerQueueBufferingMaxKbytes));
        }

        /**
         * @param producerQueueBufferingMaxMessages The maximum number of messages to buffer before sending. Default: `100000`.
         * 
         * @return builder
         * 
         */
        public Builder producerQueueBufferingMaxMessages(@Nullable Output<Integer> producerQueueBufferingMaxMessages) {
            $.producerQueueBufferingMaxMessages = producerQueueBufferingMaxMessages;
            return this;
        }

        /**
         * @param producerQueueBufferingMaxMessages The maximum number of messages to buffer before sending. Default: `100000`.
         * 
         * @return builder
         * 
         */
        public Builder producerQueueBufferingMaxMessages(Integer producerQueueBufferingMaxMessages) {
            return producerQueueBufferingMaxMessages(Output.of(producerQueueBufferingMaxMessages));
        }

        /**
         * @param producerRequestRequiredAcks The number of acknowledgements the leader broker must receive from ISR brokers before responding to the request: 0=Broker does not send any response/ack to client, -1 will block until message is committed by all in sync replicas (ISRs). Default: `-1`.
         * 
         * @return builder
         * 
         */
        public Builder producerRequestRequiredAcks(@Nullable Output<Integer> producerRequestRequiredAcks) {
            $.producerRequestRequiredAcks = producerRequestRequiredAcks;
            return this;
        }

        /**
         * @param producerRequestRequiredAcks The number of acknowledgements the leader broker must receive from ISR brokers before responding to the request: 0=Broker does not send any response/ack to client, -1 will block until message is committed by all in sync replicas (ISRs). Default: `-1`.
         * 
         * @return builder
         * 
         */
        public Builder producerRequestRequiredAcks(Integer producerRequestRequiredAcks) {
            return producerRequestRequiredAcks(Output.of(producerRequestRequiredAcks));
        }

        /**
         * @param skipBrokenMessages Number of broken messages to skip before stopping processing when reading from Kafka. Useful for handling corrupted data without failing the entire integration. Default: `0`.
         * 
         * @return builder
         * 
         */
        public Builder skipBrokenMessages(@Nullable Output<Integer> skipBrokenMessages) {
            $.skipBrokenMessages = skipBrokenMessages;
            return this;
        }

        /**
         * @param skipBrokenMessages Number of broken messages to skip before stopping processing when reading from Kafka. Useful for handling corrupted data without failing the entire integration. Default: `0`.
         * 
         * @return builder
         * 
         */
        public Builder skipBrokenMessages(Integer skipBrokenMessages) {
            return skipBrokenMessages(Output.of(skipBrokenMessages));
        }

        /**
         * @param threadPerConsumer When enabled, each consumer runs in its own thread, providing better isolation and potentially better performance for high-throughput scenarios. Default: `false`.
         * 
         * @return builder
         * 
         */
        public Builder threadPerConsumer(@Nullable Output<Boolean> threadPerConsumer) {
            $.threadPerConsumer = threadPerConsumer;
            return this;
        }

        /**
         * @param threadPerConsumer When enabled, each consumer runs in its own thread, providing better isolation and potentially better performance for high-throughput scenarios. Default: `false`.
         * 
         * @return builder
         * 
         */
        public Builder threadPerConsumer(Boolean threadPerConsumer) {
            return threadPerConsumer(Output.of(threadPerConsumer));
        }

        /**
         * @param topics Array of Kafka topics that this table will read data from or write data to. Messages from all specified topics will be inserted into this table, and data inserted into this table will be published to the topics
         * 
         * @return builder
         * 
         */
        public Builder topics(Output<List<ServiceIntegrationClickhouseKafkaUserConfigTableTopicArgs>> topics) {
            $.topics = topics;
            return this;
        }

        /**
         * @param topics Array of Kafka topics that this table will read data from or write data to. Messages from all specified topics will be inserted into this table, and data inserted into this table will be published to the topics
         * 
         * @return builder
         * 
         */
        public Builder topics(List<ServiceIntegrationClickhouseKafkaUserConfigTableTopicArgs> topics) {
            return topics(Output.of(topics));
        }

        /**
         * @param topics Array of Kafka topics that this table will read data from or write data to. Messages from all specified topics will be inserted into this table, and data inserted into this table will be published to the topics
         * 
         * @return builder
         * 
         */
        public Builder topics(ServiceIntegrationClickhouseKafkaUserConfigTableTopicArgs... topics) {
            return topics(List.of(topics));
        }

        public ServiceIntegrationClickhouseKafkaUserConfigTableArgs build() {
            if ($.columns == null) {
                throw new MissingRequiredPropertyException("ServiceIntegrationClickhouseKafkaUserConfigTableArgs", "columns");
            }
            if ($.dataFormat == null) {
                throw new MissingRequiredPropertyException("ServiceIntegrationClickhouseKafkaUserConfigTableArgs", "dataFormat");
            }
            if ($.groupName == null) {
                throw new MissingRequiredPropertyException("ServiceIntegrationClickhouseKafkaUserConfigTableArgs", "groupName");
            }
            if ($.name == null) {
                throw new MissingRequiredPropertyException("ServiceIntegrationClickhouseKafkaUserConfigTableArgs", "name");
            }
            if ($.topics == null) {
                throw new MissingRequiredPropertyException("ServiceIntegrationClickhouseKafkaUserConfigTableArgs", "topics");
            }
            return $;
        }
    }

}
