// *** WARNING: this file was generated by pulumi-java-gen. ***
// *** Do not edit by hand unless you're certain you know what you are doing! ***

package com.pulumi.aiven.inputs;

import com.pulumi.aiven.inputs.ServiceIntegrationClickhouseKafkaUserConfigTableColumnArgs;
import com.pulumi.aiven.inputs.ServiceIntegrationClickhouseKafkaUserConfigTableTopicArgs;
import com.pulumi.core.Output;
import com.pulumi.core.annotations.Import;
import com.pulumi.exceptions.MissingRequiredPropertyException;
import java.lang.Boolean;
import java.lang.Integer;
import java.lang.String;
import java.util.List;
import java.util.Objects;
import java.util.Optional;
import javax.annotation.Nullable;


public final class ServiceIntegrationClickhouseKafkaUserConfigTableArgs extends com.pulumi.resources.ResourceArgs {

    public static final ServiceIntegrationClickhouseKafkaUserConfigTableArgs Empty = new ServiceIntegrationClickhouseKafkaUserConfigTableArgs();

    /**
     * Enum: `beginning`, `earliest`, `end`, `largest`, `latest`, `smallest`. Action to take when there is no initial offset in offset store or the desired offset is out of range. Default: `earliest`.
     * 
     */
    @Import(name="autoOffsetReset")
    private @Nullable Output<String> autoOffsetReset;

    /**
     * @return Enum: `beginning`, `earliest`, `end`, `largest`, `latest`, `smallest`. Action to take when there is no initial offset in offset store or the desired offset is out of range. Default: `earliest`.
     * 
     */
    public Optional<Output<String>> autoOffsetReset() {
        return Optional.ofNullable(this.autoOffsetReset);
    }

    /**
     * Table columns
     * 
     */
    @Import(name="columns", required=true)
    private Output<List<ServiceIntegrationClickhouseKafkaUserConfigTableColumnArgs>> columns;

    /**
     * @return Table columns
     * 
     */
    public Output<List<ServiceIntegrationClickhouseKafkaUserConfigTableColumnArgs>> columns() {
        return this.columns;
    }

    /**
     * Enum: `Avro`, `AvroConfluent`, `CSV`, `JSONAsString`, `JSONCompactEachRow`, `JSONCompactStringsEachRow`, `JSONEachRow`, `JSONStringsEachRow`, `MsgPack`, `Parquet`, `RawBLOB`, `TSKV`, `TSV`, `TabSeparated`. Message data format. Default: `JSONEachRow`.
     * 
     */
    @Import(name="dataFormat", required=true)
    private Output<String> dataFormat;

    /**
     * @return Enum: `Avro`, `AvroConfluent`, `CSV`, `JSONAsString`, `JSONCompactEachRow`, `JSONCompactStringsEachRow`, `JSONEachRow`, `JSONStringsEachRow`, `MsgPack`, `Parquet`, `RawBLOB`, `TSKV`, `TSV`, `TabSeparated`. Message data format. Default: `JSONEachRow`.
     * 
     */
    public Output<String> dataFormat() {
        return this.dataFormat;
    }

    /**
     * Enum: `basic`, `best_effort`, `best_effort_us`. Method to read DateTime from text input formats. Default: `basic`.
     * 
     */
    @Import(name="dateTimeInputFormat")
    private @Nullable Output<String> dateTimeInputFormat;

    /**
     * @return Enum: `basic`, `best_effort`, `best_effort_us`. Method to read DateTime from text input formats. Default: `basic`.
     * 
     */
    public Optional<Output<String>> dateTimeInputFormat() {
        return Optional.ofNullable(this.dateTimeInputFormat);
    }

    /**
     * Kafka consumers group. Default: `clickhouse`.
     * 
     */
    @Import(name="groupName", required=true)
    private Output<String> groupName;

    /**
     * @return Kafka consumers group. Default: `clickhouse`.
     * 
     */
    public Output<String> groupName() {
        return this.groupName;
    }

    /**
     * Enum: `default`, `stream`. How to handle errors for Kafka engine. Default: `default`.
     * 
     */
    @Import(name="handleErrorMode")
    private @Nullable Output<String> handleErrorMode;

    /**
     * @return Enum: `default`, `stream`. How to handle errors for Kafka engine. Default: `default`.
     * 
     */
    public Optional<Output<String>> handleErrorMode() {
        return Optional.ofNullable(this.handleErrorMode);
    }

    /**
     * Number of row collected by poll(s) for flushing data from Kafka. Default: `0`.
     * 
     */
    @Import(name="maxBlockSize")
    private @Nullable Output<Integer> maxBlockSize;

    /**
     * @return Number of row collected by poll(s) for flushing data from Kafka. Default: `0`.
     * 
     */
    public Optional<Output<Integer>> maxBlockSize() {
        return Optional.ofNullable(this.maxBlockSize);
    }

    /**
     * The maximum number of rows produced in one kafka message for row-based formats. Default: `1`.
     * 
     */
    @Import(name="maxRowsPerMessage")
    private @Nullable Output<Integer> maxRowsPerMessage;

    /**
     * @return The maximum number of rows produced in one kafka message for row-based formats. Default: `1`.
     * 
     */
    public Optional<Output<Integer>> maxRowsPerMessage() {
        return Optional.ofNullable(this.maxRowsPerMessage);
    }

    /**
     * Name of the table. Example: `events`.
     * 
     */
    @Import(name="name", required=true)
    private Output<String> name;

    /**
     * @return Name of the table. Example: `events`.
     * 
     */
    public Output<String> name() {
        return this.name;
    }

    /**
     * The number of consumers per table per replica. Default: `1`.
     * 
     */
    @Import(name="numConsumers")
    private @Nullable Output<Integer> numConsumers;

    /**
     * @return The number of consumers per table per replica. Default: `1`.
     * 
     */
    public Optional<Output<Integer>> numConsumers() {
        return Optional.ofNullable(this.numConsumers);
    }

    /**
     * Maximum amount of messages to be polled in a single Kafka poll. Default: `0`.
     * 
     */
    @Import(name="pollMaxBatchSize")
    private @Nullable Output<Integer> pollMaxBatchSize;

    /**
     * @return Maximum amount of messages to be polled in a single Kafka poll. Default: `0`.
     * 
     */
    public Optional<Output<Integer>> pollMaxBatchSize() {
        return Optional.ofNullable(this.pollMaxBatchSize);
    }

    /**
     * Timeout in milliseconds for a single poll from Kafka. Takes the value of the stream*flush*interval_ms server setting by default (500ms). Default: `0`.
     * 
     */
    @Import(name="pollMaxTimeoutMs")
    private @Nullable Output<Integer> pollMaxTimeoutMs;

    /**
     * @return Timeout in milliseconds for a single poll from Kafka. Takes the value of the stream*flush*interval_ms server setting by default (500ms). Default: `0`.
     * 
     */
    public Optional<Output<Integer>> pollMaxTimeoutMs() {
        return Optional.ofNullable(this.pollMaxTimeoutMs);
    }

    /**
     * Skip at least this number of broken messages from Kafka topic per block. Default: `0`.
     * 
     */
    @Import(name="skipBrokenMessages")
    private @Nullable Output<Integer> skipBrokenMessages;

    /**
     * @return Skip at least this number of broken messages from Kafka topic per block. Default: `0`.
     * 
     */
    public Optional<Output<Integer>> skipBrokenMessages() {
        return Optional.ofNullable(this.skipBrokenMessages);
    }

    /**
     * Provide an independent thread for each consumer. All consumers run in the same thread by default. Default: `false`.
     * 
     */
    @Import(name="threadPerConsumer")
    private @Nullable Output<Boolean> threadPerConsumer;

    /**
     * @return Provide an independent thread for each consumer. All consumers run in the same thread by default. Default: `false`.
     * 
     */
    public Optional<Output<Boolean>> threadPerConsumer() {
        return Optional.ofNullable(this.threadPerConsumer);
    }

    /**
     * Kafka topics
     * 
     */
    @Import(name="topics", required=true)
    private Output<List<ServiceIntegrationClickhouseKafkaUserConfigTableTopicArgs>> topics;

    /**
     * @return Kafka topics
     * 
     */
    public Output<List<ServiceIntegrationClickhouseKafkaUserConfigTableTopicArgs>> topics() {
        return this.topics;
    }

    private ServiceIntegrationClickhouseKafkaUserConfigTableArgs() {}

    private ServiceIntegrationClickhouseKafkaUserConfigTableArgs(ServiceIntegrationClickhouseKafkaUserConfigTableArgs $) {
        this.autoOffsetReset = $.autoOffsetReset;
        this.columns = $.columns;
        this.dataFormat = $.dataFormat;
        this.dateTimeInputFormat = $.dateTimeInputFormat;
        this.groupName = $.groupName;
        this.handleErrorMode = $.handleErrorMode;
        this.maxBlockSize = $.maxBlockSize;
        this.maxRowsPerMessage = $.maxRowsPerMessage;
        this.name = $.name;
        this.numConsumers = $.numConsumers;
        this.pollMaxBatchSize = $.pollMaxBatchSize;
        this.pollMaxTimeoutMs = $.pollMaxTimeoutMs;
        this.skipBrokenMessages = $.skipBrokenMessages;
        this.threadPerConsumer = $.threadPerConsumer;
        this.topics = $.topics;
    }

    public static Builder builder() {
        return new Builder();
    }
    public static Builder builder(ServiceIntegrationClickhouseKafkaUserConfigTableArgs defaults) {
        return new Builder(defaults);
    }

    public static final class Builder {
        private ServiceIntegrationClickhouseKafkaUserConfigTableArgs $;

        public Builder() {
            $ = new ServiceIntegrationClickhouseKafkaUserConfigTableArgs();
        }

        public Builder(ServiceIntegrationClickhouseKafkaUserConfigTableArgs defaults) {
            $ = new ServiceIntegrationClickhouseKafkaUserConfigTableArgs(Objects.requireNonNull(defaults));
        }

        /**
         * @param autoOffsetReset Enum: `beginning`, `earliest`, `end`, `largest`, `latest`, `smallest`. Action to take when there is no initial offset in offset store or the desired offset is out of range. Default: `earliest`.
         * 
         * @return builder
         * 
         */
        public Builder autoOffsetReset(@Nullable Output<String> autoOffsetReset) {
            $.autoOffsetReset = autoOffsetReset;
            return this;
        }

        /**
         * @param autoOffsetReset Enum: `beginning`, `earliest`, `end`, `largest`, `latest`, `smallest`. Action to take when there is no initial offset in offset store or the desired offset is out of range. Default: `earliest`.
         * 
         * @return builder
         * 
         */
        public Builder autoOffsetReset(String autoOffsetReset) {
            return autoOffsetReset(Output.of(autoOffsetReset));
        }

        /**
         * @param columns Table columns
         * 
         * @return builder
         * 
         */
        public Builder columns(Output<List<ServiceIntegrationClickhouseKafkaUserConfigTableColumnArgs>> columns) {
            $.columns = columns;
            return this;
        }

        /**
         * @param columns Table columns
         * 
         * @return builder
         * 
         */
        public Builder columns(List<ServiceIntegrationClickhouseKafkaUserConfigTableColumnArgs> columns) {
            return columns(Output.of(columns));
        }

        /**
         * @param columns Table columns
         * 
         * @return builder
         * 
         */
        public Builder columns(ServiceIntegrationClickhouseKafkaUserConfigTableColumnArgs... columns) {
            return columns(List.of(columns));
        }

        /**
         * @param dataFormat Enum: `Avro`, `AvroConfluent`, `CSV`, `JSONAsString`, `JSONCompactEachRow`, `JSONCompactStringsEachRow`, `JSONEachRow`, `JSONStringsEachRow`, `MsgPack`, `Parquet`, `RawBLOB`, `TSKV`, `TSV`, `TabSeparated`. Message data format. Default: `JSONEachRow`.
         * 
         * @return builder
         * 
         */
        public Builder dataFormat(Output<String> dataFormat) {
            $.dataFormat = dataFormat;
            return this;
        }

        /**
         * @param dataFormat Enum: `Avro`, `AvroConfluent`, `CSV`, `JSONAsString`, `JSONCompactEachRow`, `JSONCompactStringsEachRow`, `JSONEachRow`, `JSONStringsEachRow`, `MsgPack`, `Parquet`, `RawBLOB`, `TSKV`, `TSV`, `TabSeparated`. Message data format. Default: `JSONEachRow`.
         * 
         * @return builder
         * 
         */
        public Builder dataFormat(String dataFormat) {
            return dataFormat(Output.of(dataFormat));
        }

        /**
         * @param dateTimeInputFormat Enum: `basic`, `best_effort`, `best_effort_us`. Method to read DateTime from text input formats. Default: `basic`.
         * 
         * @return builder
         * 
         */
        public Builder dateTimeInputFormat(@Nullable Output<String> dateTimeInputFormat) {
            $.dateTimeInputFormat = dateTimeInputFormat;
            return this;
        }

        /**
         * @param dateTimeInputFormat Enum: `basic`, `best_effort`, `best_effort_us`. Method to read DateTime from text input formats. Default: `basic`.
         * 
         * @return builder
         * 
         */
        public Builder dateTimeInputFormat(String dateTimeInputFormat) {
            return dateTimeInputFormat(Output.of(dateTimeInputFormat));
        }

        /**
         * @param groupName Kafka consumers group. Default: `clickhouse`.
         * 
         * @return builder
         * 
         */
        public Builder groupName(Output<String> groupName) {
            $.groupName = groupName;
            return this;
        }

        /**
         * @param groupName Kafka consumers group. Default: `clickhouse`.
         * 
         * @return builder
         * 
         */
        public Builder groupName(String groupName) {
            return groupName(Output.of(groupName));
        }

        /**
         * @param handleErrorMode Enum: `default`, `stream`. How to handle errors for Kafka engine. Default: `default`.
         * 
         * @return builder
         * 
         */
        public Builder handleErrorMode(@Nullable Output<String> handleErrorMode) {
            $.handleErrorMode = handleErrorMode;
            return this;
        }

        /**
         * @param handleErrorMode Enum: `default`, `stream`. How to handle errors for Kafka engine. Default: `default`.
         * 
         * @return builder
         * 
         */
        public Builder handleErrorMode(String handleErrorMode) {
            return handleErrorMode(Output.of(handleErrorMode));
        }

        /**
         * @param maxBlockSize Number of row collected by poll(s) for flushing data from Kafka. Default: `0`.
         * 
         * @return builder
         * 
         */
        public Builder maxBlockSize(@Nullable Output<Integer> maxBlockSize) {
            $.maxBlockSize = maxBlockSize;
            return this;
        }

        /**
         * @param maxBlockSize Number of row collected by poll(s) for flushing data from Kafka. Default: `0`.
         * 
         * @return builder
         * 
         */
        public Builder maxBlockSize(Integer maxBlockSize) {
            return maxBlockSize(Output.of(maxBlockSize));
        }

        /**
         * @param maxRowsPerMessage The maximum number of rows produced in one kafka message for row-based formats. Default: `1`.
         * 
         * @return builder
         * 
         */
        public Builder maxRowsPerMessage(@Nullable Output<Integer> maxRowsPerMessage) {
            $.maxRowsPerMessage = maxRowsPerMessage;
            return this;
        }

        /**
         * @param maxRowsPerMessage The maximum number of rows produced in one kafka message for row-based formats. Default: `1`.
         * 
         * @return builder
         * 
         */
        public Builder maxRowsPerMessage(Integer maxRowsPerMessage) {
            return maxRowsPerMessage(Output.of(maxRowsPerMessage));
        }

        /**
         * @param name Name of the table. Example: `events`.
         * 
         * @return builder
         * 
         */
        public Builder name(Output<String> name) {
            $.name = name;
            return this;
        }

        /**
         * @param name Name of the table. Example: `events`.
         * 
         * @return builder
         * 
         */
        public Builder name(String name) {
            return name(Output.of(name));
        }

        /**
         * @param numConsumers The number of consumers per table per replica. Default: `1`.
         * 
         * @return builder
         * 
         */
        public Builder numConsumers(@Nullable Output<Integer> numConsumers) {
            $.numConsumers = numConsumers;
            return this;
        }

        /**
         * @param numConsumers The number of consumers per table per replica. Default: `1`.
         * 
         * @return builder
         * 
         */
        public Builder numConsumers(Integer numConsumers) {
            return numConsumers(Output.of(numConsumers));
        }

        /**
         * @param pollMaxBatchSize Maximum amount of messages to be polled in a single Kafka poll. Default: `0`.
         * 
         * @return builder
         * 
         */
        public Builder pollMaxBatchSize(@Nullable Output<Integer> pollMaxBatchSize) {
            $.pollMaxBatchSize = pollMaxBatchSize;
            return this;
        }

        /**
         * @param pollMaxBatchSize Maximum amount of messages to be polled in a single Kafka poll. Default: `0`.
         * 
         * @return builder
         * 
         */
        public Builder pollMaxBatchSize(Integer pollMaxBatchSize) {
            return pollMaxBatchSize(Output.of(pollMaxBatchSize));
        }

        /**
         * @param pollMaxTimeoutMs Timeout in milliseconds for a single poll from Kafka. Takes the value of the stream*flush*interval_ms server setting by default (500ms). Default: `0`.
         * 
         * @return builder
         * 
         */
        public Builder pollMaxTimeoutMs(@Nullable Output<Integer> pollMaxTimeoutMs) {
            $.pollMaxTimeoutMs = pollMaxTimeoutMs;
            return this;
        }

        /**
         * @param pollMaxTimeoutMs Timeout in milliseconds for a single poll from Kafka. Takes the value of the stream*flush*interval_ms server setting by default (500ms). Default: `0`.
         * 
         * @return builder
         * 
         */
        public Builder pollMaxTimeoutMs(Integer pollMaxTimeoutMs) {
            return pollMaxTimeoutMs(Output.of(pollMaxTimeoutMs));
        }

        /**
         * @param skipBrokenMessages Skip at least this number of broken messages from Kafka topic per block. Default: `0`.
         * 
         * @return builder
         * 
         */
        public Builder skipBrokenMessages(@Nullable Output<Integer> skipBrokenMessages) {
            $.skipBrokenMessages = skipBrokenMessages;
            return this;
        }

        /**
         * @param skipBrokenMessages Skip at least this number of broken messages from Kafka topic per block. Default: `0`.
         * 
         * @return builder
         * 
         */
        public Builder skipBrokenMessages(Integer skipBrokenMessages) {
            return skipBrokenMessages(Output.of(skipBrokenMessages));
        }

        /**
         * @param threadPerConsumer Provide an independent thread for each consumer. All consumers run in the same thread by default. Default: `false`.
         * 
         * @return builder
         * 
         */
        public Builder threadPerConsumer(@Nullable Output<Boolean> threadPerConsumer) {
            $.threadPerConsumer = threadPerConsumer;
            return this;
        }

        /**
         * @param threadPerConsumer Provide an independent thread for each consumer. All consumers run in the same thread by default. Default: `false`.
         * 
         * @return builder
         * 
         */
        public Builder threadPerConsumer(Boolean threadPerConsumer) {
            return threadPerConsumer(Output.of(threadPerConsumer));
        }

        /**
         * @param topics Kafka topics
         * 
         * @return builder
         * 
         */
        public Builder topics(Output<List<ServiceIntegrationClickhouseKafkaUserConfigTableTopicArgs>> topics) {
            $.topics = topics;
            return this;
        }

        /**
         * @param topics Kafka topics
         * 
         * @return builder
         * 
         */
        public Builder topics(List<ServiceIntegrationClickhouseKafkaUserConfigTableTopicArgs> topics) {
            return topics(Output.of(topics));
        }

        /**
         * @param topics Kafka topics
         * 
         * @return builder
         * 
         */
        public Builder topics(ServiceIntegrationClickhouseKafkaUserConfigTableTopicArgs... topics) {
            return topics(List.of(topics));
        }

        public ServiceIntegrationClickhouseKafkaUserConfigTableArgs build() {
            if ($.columns == null) {
                throw new MissingRequiredPropertyException("ServiceIntegrationClickhouseKafkaUserConfigTableArgs", "columns");
            }
            if ($.dataFormat == null) {
                throw new MissingRequiredPropertyException("ServiceIntegrationClickhouseKafkaUserConfigTableArgs", "dataFormat");
            }
            if ($.groupName == null) {
                throw new MissingRequiredPropertyException("ServiceIntegrationClickhouseKafkaUserConfigTableArgs", "groupName");
            }
            if ($.name == null) {
                throw new MissingRequiredPropertyException("ServiceIntegrationClickhouseKafkaUserConfigTableArgs", "name");
            }
            if ($.topics == null) {
                throw new MissingRequiredPropertyException("ServiceIntegrationClickhouseKafkaUserConfigTableArgs", "topics");
            }
            return $;
        }
    }

}
