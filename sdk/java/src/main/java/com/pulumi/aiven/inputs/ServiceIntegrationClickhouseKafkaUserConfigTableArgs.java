// *** WARNING: this file was generated by pulumi-language-java. ***
// *** Do not edit by hand unless you're certain you know what you are doing! ***

package com.pulumi.aiven.inputs;

import com.pulumi.aiven.inputs.ServiceIntegrationClickhouseKafkaUserConfigTableColumnArgs;
import com.pulumi.aiven.inputs.ServiceIntegrationClickhouseKafkaUserConfigTableTopicArgs;
import com.pulumi.core.Output;
import com.pulumi.core.annotations.Import;
import com.pulumi.exceptions.MissingRequiredPropertyException;
import java.lang.Boolean;
import java.lang.Integer;
import java.lang.String;
import java.util.List;
import java.util.Objects;
import java.util.Optional;
import javax.annotation.Nullable;


public final class ServiceIntegrationClickhouseKafkaUserConfigTableArgs extends com.pulumi.resources.ResourceArgs {

    public static final ServiceIntegrationClickhouseKafkaUserConfigTableArgs Empty = new ServiceIntegrationClickhouseKafkaUserConfigTableArgs();

    /**
     * Enum: `beginning`, `earliest`, `end`, `largest`, `latest`, `smallest`. Action to take when there is no initial offset in offset store or the desired offset is out of range. Default: `earliest`.
     * 
     */
    @Import(name="autoOffsetReset")
    private @Nullable Output<String> autoOffsetReset;

    /**
     * @return Enum: `beginning`, `earliest`, `end`, `largest`, `latest`, `smallest`. Action to take when there is no initial offset in offset store or the desired offset is out of range. Default: `earliest`.
     * 
     */
    public Optional<Output<String>> autoOffsetReset() {
        return Optional.ofNullable(this.autoOffsetReset);
    }

    /**
     * Table columns
     * 
     */
    @Import(name="columns", required=true)
    private Output<List<ServiceIntegrationClickhouseKafkaUserConfigTableColumnArgs>> columns;

    /**
     * @return Table columns
     * 
     */
    public Output<List<ServiceIntegrationClickhouseKafkaUserConfigTableColumnArgs>> columns() {
        return this.columns;
    }

    /**
     * Enum: `Avro`, `AvroConfluent`, `CSV`, `JSONAsString`, `JSONCompactEachRow`, `JSONCompactStringsEachRow`, `JSONEachRow`, `JSONStringsEachRow`, `MsgPack`, `Parquet`, `RawBLOB`, `TSKV`, `TSV`, `TabSeparated`. Message data format. Default: `JSONEachRow`.
     * 
     */
    @Import(name="dataFormat", required=true)
    private Output<String> dataFormat;

    /**
     * @return Enum: `Avro`, `AvroConfluent`, `CSV`, `JSONAsString`, `JSONCompactEachRow`, `JSONCompactStringsEachRow`, `JSONEachRow`, `JSONStringsEachRow`, `MsgPack`, `Parquet`, `RawBLOB`, `TSKV`, `TSV`, `TabSeparated`. Message data format. Default: `JSONEachRow`.
     * 
     */
    public Output<String> dataFormat() {
        return this.dataFormat;
    }

    /**
     * Enum: `basic`, `best_effort`, `best_effort_us`. Method to read DateTime from text input formats. Default: `basic`.
     * 
     */
    @Import(name="dateTimeInputFormat")
    private @Nullable Output<String> dateTimeInputFormat;

    /**
     * @return Enum: `basic`, `best_effort`, `best_effort_us`. Method to read DateTime from text input formats. Default: `basic`.
     * 
     */
    public Optional<Output<String>> dateTimeInputFormat() {
        return Optional.ofNullable(this.dateTimeInputFormat);
    }

    /**
     * Kafka consumers group. Default: `clickhouse`.
     * 
     */
    @Import(name="groupName", required=true)
    private Output<String> groupName;

    /**
     * @return Kafka consumers group. Default: `clickhouse`.
     * 
     */
    public Output<String> groupName() {
        return this.groupName;
    }

    /**
     * Enum: `default`, `stream`. How to handle errors for Kafka engine. Default: `default`.
     * 
     */
    @Import(name="handleErrorMode")
    private @Nullable Output<String> handleErrorMode;

    /**
     * @return Enum: `default`, `stream`. How to handle errors for Kafka engine. Default: `default`.
     * 
     */
    public Optional<Output<String>> handleErrorMode() {
        return Optional.ofNullable(this.handleErrorMode);
    }

    /**
     * Number of row collected by poll(s) for flushing data from Kafka. Default: `0`.
     * 
     */
    @Import(name="maxBlockSize")
    private @Nullable Output<Integer> maxBlockSize;

    /**
     * @return Number of row collected by poll(s) for flushing data from Kafka. Default: `0`.
     * 
     */
    public Optional<Output<Integer>> maxBlockSize() {
        return Optional.ofNullable(this.maxBlockSize);
    }

    /**
     * The maximum number of rows produced in one kafka message for row-based formats. Default: `1`.
     * 
     */
    @Import(name="maxRowsPerMessage")
    private @Nullable Output<Integer> maxRowsPerMessage;

    /**
     * @return The maximum number of rows produced in one kafka message for row-based formats. Default: `1`.
     * 
     */
    public Optional<Output<Integer>> maxRowsPerMessage() {
        return Optional.ofNullable(this.maxRowsPerMessage);
    }

    /**
     * Name of the table. Example: `events`.
     * 
     */
    @Import(name="name", required=true)
    private Output<String> name;

    /**
     * @return Name of the table. Example: `events`.
     * 
     */
    public Output<String> name() {
        return this.name;
    }

    /**
     * The number of consumers per table per replica. Default: `1`.
     * 
     */
    @Import(name="numConsumers")
    private @Nullable Output<Integer> numConsumers;

    /**
     * @return The number of consumers per table per replica. Default: `1`.
     * 
     */
    public Optional<Output<Integer>> numConsumers() {
        return Optional.ofNullable(this.numConsumers);
    }

    /**
     * Maximum amount of messages to be polled in a single Kafka poll. Default: `0`.
     * 
     */
    @Import(name="pollMaxBatchSize")
    private @Nullable Output<Integer> pollMaxBatchSize;

    /**
     * @return Maximum amount of messages to be polled in a single Kafka poll. Default: `0`.
     * 
     */
    public Optional<Output<Integer>> pollMaxBatchSize() {
        return Optional.ofNullable(this.pollMaxBatchSize);
    }

    /**
     * Timeout in milliseconds for a single poll from Kafka. Takes the value of the stream*flush*interval_ms server setting by default (500ms). Default: `0`.
     * 
     */
    @Import(name="pollMaxTimeoutMs")
    private @Nullable Output<Integer> pollMaxTimeoutMs;

    /**
     * @return Timeout in milliseconds for a single poll from Kafka. Takes the value of the stream*flush*interval_ms server setting by default (500ms). Default: `0`.
     * 
     */
    public Optional<Output<Integer>> pollMaxTimeoutMs() {
        return Optional.ofNullable(this.pollMaxTimeoutMs);
    }

    /**
     * The maximum number of messages in a batch sent to Kafka. If the number of messages exceeds this value, the batch is sent. Default: `10000`.
     * 
     */
    @Import(name="producerBatchNumMessages")
    private @Nullable Output<Integer> producerBatchNumMessages;

    /**
     * @return The maximum number of messages in a batch sent to Kafka. If the number of messages exceeds this value, the batch is sent. Default: `10000`.
     * 
     */
    public Optional<Output<Integer>> producerBatchNumMessages() {
        return Optional.ofNullable(this.producerBatchNumMessages);
    }

    /**
     * The maximum size in bytes of a batch of messages sent to Kafka. If the batch size is exceeded, the batch is sent. Default: `1000000`.
     * 
     */
    @Import(name="producerBatchSize")
    private @Nullable Output<Integer> producerBatchSize;

    /**
     * @return The maximum size in bytes of a batch of messages sent to Kafka. If the batch size is exceeded, the batch is sent. Default: `1000000`.
     * 
     */
    public Optional<Output<Integer>> producerBatchSize() {
        return Optional.ofNullable(this.producerBatchSize);
    }

    /**
     * Enum: `gzip`, `lz4`, `none`, `snappy`, `zstd`. The compression codec to use when sending a batch of messages to Kafka. Default: `none`.
     * 
     */
    @Import(name="producerCompressionCodec")
    private @Nullable Output<String> producerCompressionCodec;

    /**
     * @return Enum: `gzip`, `lz4`, `none`, `snappy`, `zstd`. The compression codec to use when sending a batch of messages to Kafka. Default: `none`.
     * 
     */
    public Optional<Output<String>> producerCompressionCodec() {
        return Optional.ofNullable(this.producerCompressionCodec);
    }

    /**
     * The compression level to use when sending a batch of messages to Kafka. Usable range is algorithm-dependent: [0-9] for gzip; [0-12] for lz4; only 0 for snappy; -1 = codec-dependent default compression level. Default: `-1`.
     * 
     */
    @Import(name="producerCompressionLevel")
    private @Nullable Output<Integer> producerCompressionLevel;

    /**
     * @return The compression level to use when sending a batch of messages to Kafka. Usable range is algorithm-dependent: [0-9] for gzip; [0-12] for lz4; only 0 for snappy; -1 = codec-dependent default compression level. Default: `-1`.
     * 
     */
    public Optional<Output<Integer>> producerCompressionLevel() {
        return Optional.ofNullable(this.producerCompressionLevel);
    }

    /**
     * The time in milliseconds to wait for additional messages before sending a batch. If the time is exceeded, the batch is sent. Default: `5`.
     * 
     */
    @Import(name="producerLingerMs")
    private @Nullable Output<Integer> producerLingerMs;

    /**
     * @return The time in milliseconds to wait for additional messages before sending a batch. If the time is exceeded, the batch is sent. Default: `5`.
     * 
     */
    public Optional<Output<Integer>> producerLingerMs() {
        return Optional.ofNullable(this.producerLingerMs);
    }

    /**
     * The maximum size of the buffer in kilobytes before sending. Default: `1048576`.
     * 
     */
    @Import(name="producerQueueBufferingMaxKbytes")
    private @Nullable Output<Integer> producerQueueBufferingMaxKbytes;

    /**
     * @return The maximum size of the buffer in kilobytes before sending. Default: `1048576`.
     * 
     */
    public Optional<Output<Integer>> producerQueueBufferingMaxKbytes() {
        return Optional.ofNullable(this.producerQueueBufferingMaxKbytes);
    }

    /**
     * The maximum number of messages to buffer before sending. Default: `100000`.
     * 
     */
    @Import(name="producerQueueBufferingMaxMessages")
    private @Nullable Output<Integer> producerQueueBufferingMaxMessages;

    /**
     * @return The maximum number of messages to buffer before sending. Default: `100000`.
     * 
     */
    public Optional<Output<Integer>> producerQueueBufferingMaxMessages() {
        return Optional.ofNullable(this.producerQueueBufferingMaxMessages);
    }

    /**
     * The number of acknowledgements the leader broker must receive from ISR brokers before responding to the request: 0=Broker does not send any response/ack to client, -1 will block until message is committed by all in sync replicas (ISRs). Default: `-1`.
     * 
     */
    @Import(name="producerRequestRequiredAcks")
    private @Nullable Output<Integer> producerRequestRequiredAcks;

    /**
     * @return The number of acknowledgements the leader broker must receive from ISR brokers before responding to the request: 0=Broker does not send any response/ack to client, -1 will block until message is committed by all in sync replicas (ISRs). Default: `-1`.
     * 
     */
    public Optional<Output<Integer>> producerRequestRequiredAcks() {
        return Optional.ofNullable(this.producerRequestRequiredAcks);
    }

    /**
     * Skip at least this number of broken messages from Kafka topic per block. Default: `0`.
     * 
     */
    @Import(name="skipBrokenMessages")
    private @Nullable Output<Integer> skipBrokenMessages;

    /**
     * @return Skip at least this number of broken messages from Kafka topic per block. Default: `0`.
     * 
     */
    public Optional<Output<Integer>> skipBrokenMessages() {
        return Optional.ofNullable(this.skipBrokenMessages);
    }

    /**
     * Provide an independent thread for each consumer. All consumers run in the same thread by default. Default: `false`.
     * 
     */
    @Import(name="threadPerConsumer")
    private @Nullable Output<Boolean> threadPerConsumer;

    /**
     * @return Provide an independent thread for each consumer. All consumers run in the same thread by default. Default: `false`.
     * 
     */
    public Optional<Output<Boolean>> threadPerConsumer() {
        return Optional.ofNullable(this.threadPerConsumer);
    }

    /**
     * Kafka topics
     * 
     */
    @Import(name="topics", required=true)
    private Output<List<ServiceIntegrationClickhouseKafkaUserConfigTableTopicArgs>> topics;

    /**
     * @return Kafka topics
     * 
     */
    public Output<List<ServiceIntegrationClickhouseKafkaUserConfigTableTopicArgs>> topics() {
        return this.topics;
    }

    private ServiceIntegrationClickhouseKafkaUserConfigTableArgs() {}

    private ServiceIntegrationClickhouseKafkaUserConfigTableArgs(ServiceIntegrationClickhouseKafkaUserConfigTableArgs $) {
        this.autoOffsetReset = $.autoOffsetReset;
        this.columns = $.columns;
        this.dataFormat = $.dataFormat;
        this.dateTimeInputFormat = $.dateTimeInputFormat;
        this.groupName = $.groupName;
        this.handleErrorMode = $.handleErrorMode;
        this.maxBlockSize = $.maxBlockSize;
        this.maxRowsPerMessage = $.maxRowsPerMessage;
        this.name = $.name;
        this.numConsumers = $.numConsumers;
        this.pollMaxBatchSize = $.pollMaxBatchSize;
        this.pollMaxTimeoutMs = $.pollMaxTimeoutMs;
        this.producerBatchNumMessages = $.producerBatchNumMessages;
        this.producerBatchSize = $.producerBatchSize;
        this.producerCompressionCodec = $.producerCompressionCodec;
        this.producerCompressionLevel = $.producerCompressionLevel;
        this.producerLingerMs = $.producerLingerMs;
        this.producerQueueBufferingMaxKbytes = $.producerQueueBufferingMaxKbytes;
        this.producerQueueBufferingMaxMessages = $.producerQueueBufferingMaxMessages;
        this.producerRequestRequiredAcks = $.producerRequestRequiredAcks;
        this.skipBrokenMessages = $.skipBrokenMessages;
        this.threadPerConsumer = $.threadPerConsumer;
        this.topics = $.topics;
    }

    public static Builder builder() {
        return new Builder();
    }
    public static Builder builder(ServiceIntegrationClickhouseKafkaUserConfigTableArgs defaults) {
        return new Builder(defaults);
    }

    public static final class Builder {
        private ServiceIntegrationClickhouseKafkaUserConfigTableArgs $;

        public Builder() {
            $ = new ServiceIntegrationClickhouseKafkaUserConfigTableArgs();
        }

        public Builder(ServiceIntegrationClickhouseKafkaUserConfigTableArgs defaults) {
            $ = new ServiceIntegrationClickhouseKafkaUserConfigTableArgs(Objects.requireNonNull(defaults));
        }

        /**
         * @param autoOffsetReset Enum: `beginning`, `earliest`, `end`, `largest`, `latest`, `smallest`. Action to take when there is no initial offset in offset store or the desired offset is out of range. Default: `earliest`.
         * 
         * @return builder
         * 
         */
        public Builder autoOffsetReset(@Nullable Output<String> autoOffsetReset) {
            $.autoOffsetReset = autoOffsetReset;
            return this;
        }

        /**
         * @param autoOffsetReset Enum: `beginning`, `earliest`, `end`, `largest`, `latest`, `smallest`. Action to take when there is no initial offset in offset store or the desired offset is out of range. Default: `earliest`.
         * 
         * @return builder
         * 
         */
        public Builder autoOffsetReset(String autoOffsetReset) {
            return autoOffsetReset(Output.of(autoOffsetReset));
        }

        /**
         * @param columns Table columns
         * 
         * @return builder
         * 
         */
        public Builder columns(Output<List<ServiceIntegrationClickhouseKafkaUserConfigTableColumnArgs>> columns) {
            $.columns = columns;
            return this;
        }

        /**
         * @param columns Table columns
         * 
         * @return builder
         * 
         */
        public Builder columns(List<ServiceIntegrationClickhouseKafkaUserConfigTableColumnArgs> columns) {
            return columns(Output.of(columns));
        }

        /**
         * @param columns Table columns
         * 
         * @return builder
         * 
         */
        public Builder columns(ServiceIntegrationClickhouseKafkaUserConfigTableColumnArgs... columns) {
            return columns(List.of(columns));
        }

        /**
         * @param dataFormat Enum: `Avro`, `AvroConfluent`, `CSV`, `JSONAsString`, `JSONCompactEachRow`, `JSONCompactStringsEachRow`, `JSONEachRow`, `JSONStringsEachRow`, `MsgPack`, `Parquet`, `RawBLOB`, `TSKV`, `TSV`, `TabSeparated`. Message data format. Default: `JSONEachRow`.
         * 
         * @return builder
         * 
         */
        public Builder dataFormat(Output<String> dataFormat) {
            $.dataFormat = dataFormat;
            return this;
        }

        /**
         * @param dataFormat Enum: `Avro`, `AvroConfluent`, `CSV`, `JSONAsString`, `JSONCompactEachRow`, `JSONCompactStringsEachRow`, `JSONEachRow`, `JSONStringsEachRow`, `MsgPack`, `Parquet`, `RawBLOB`, `TSKV`, `TSV`, `TabSeparated`. Message data format. Default: `JSONEachRow`.
         * 
         * @return builder
         * 
         */
        public Builder dataFormat(String dataFormat) {
            return dataFormat(Output.of(dataFormat));
        }

        /**
         * @param dateTimeInputFormat Enum: `basic`, `best_effort`, `best_effort_us`. Method to read DateTime from text input formats. Default: `basic`.
         * 
         * @return builder
         * 
         */
        public Builder dateTimeInputFormat(@Nullable Output<String> dateTimeInputFormat) {
            $.dateTimeInputFormat = dateTimeInputFormat;
            return this;
        }

        /**
         * @param dateTimeInputFormat Enum: `basic`, `best_effort`, `best_effort_us`. Method to read DateTime from text input formats. Default: `basic`.
         * 
         * @return builder
         * 
         */
        public Builder dateTimeInputFormat(String dateTimeInputFormat) {
            return dateTimeInputFormat(Output.of(dateTimeInputFormat));
        }

        /**
         * @param groupName Kafka consumers group. Default: `clickhouse`.
         * 
         * @return builder
         * 
         */
        public Builder groupName(Output<String> groupName) {
            $.groupName = groupName;
            return this;
        }

        /**
         * @param groupName Kafka consumers group. Default: `clickhouse`.
         * 
         * @return builder
         * 
         */
        public Builder groupName(String groupName) {
            return groupName(Output.of(groupName));
        }

        /**
         * @param handleErrorMode Enum: `default`, `stream`. How to handle errors for Kafka engine. Default: `default`.
         * 
         * @return builder
         * 
         */
        public Builder handleErrorMode(@Nullable Output<String> handleErrorMode) {
            $.handleErrorMode = handleErrorMode;
            return this;
        }

        /**
         * @param handleErrorMode Enum: `default`, `stream`. How to handle errors for Kafka engine. Default: `default`.
         * 
         * @return builder
         * 
         */
        public Builder handleErrorMode(String handleErrorMode) {
            return handleErrorMode(Output.of(handleErrorMode));
        }

        /**
         * @param maxBlockSize Number of row collected by poll(s) for flushing data from Kafka. Default: `0`.
         * 
         * @return builder
         * 
         */
        public Builder maxBlockSize(@Nullable Output<Integer> maxBlockSize) {
            $.maxBlockSize = maxBlockSize;
            return this;
        }

        /**
         * @param maxBlockSize Number of row collected by poll(s) for flushing data from Kafka. Default: `0`.
         * 
         * @return builder
         * 
         */
        public Builder maxBlockSize(Integer maxBlockSize) {
            return maxBlockSize(Output.of(maxBlockSize));
        }

        /**
         * @param maxRowsPerMessage The maximum number of rows produced in one kafka message for row-based formats. Default: `1`.
         * 
         * @return builder
         * 
         */
        public Builder maxRowsPerMessage(@Nullable Output<Integer> maxRowsPerMessage) {
            $.maxRowsPerMessage = maxRowsPerMessage;
            return this;
        }

        /**
         * @param maxRowsPerMessage The maximum number of rows produced in one kafka message for row-based formats. Default: `1`.
         * 
         * @return builder
         * 
         */
        public Builder maxRowsPerMessage(Integer maxRowsPerMessage) {
            return maxRowsPerMessage(Output.of(maxRowsPerMessage));
        }

        /**
         * @param name Name of the table. Example: `events`.
         * 
         * @return builder
         * 
         */
        public Builder name(Output<String> name) {
            $.name = name;
            return this;
        }

        /**
         * @param name Name of the table. Example: `events`.
         * 
         * @return builder
         * 
         */
        public Builder name(String name) {
            return name(Output.of(name));
        }

        /**
         * @param numConsumers The number of consumers per table per replica. Default: `1`.
         * 
         * @return builder
         * 
         */
        public Builder numConsumers(@Nullable Output<Integer> numConsumers) {
            $.numConsumers = numConsumers;
            return this;
        }

        /**
         * @param numConsumers The number of consumers per table per replica. Default: `1`.
         * 
         * @return builder
         * 
         */
        public Builder numConsumers(Integer numConsumers) {
            return numConsumers(Output.of(numConsumers));
        }

        /**
         * @param pollMaxBatchSize Maximum amount of messages to be polled in a single Kafka poll. Default: `0`.
         * 
         * @return builder
         * 
         */
        public Builder pollMaxBatchSize(@Nullable Output<Integer> pollMaxBatchSize) {
            $.pollMaxBatchSize = pollMaxBatchSize;
            return this;
        }

        /**
         * @param pollMaxBatchSize Maximum amount of messages to be polled in a single Kafka poll. Default: `0`.
         * 
         * @return builder
         * 
         */
        public Builder pollMaxBatchSize(Integer pollMaxBatchSize) {
            return pollMaxBatchSize(Output.of(pollMaxBatchSize));
        }

        /**
         * @param pollMaxTimeoutMs Timeout in milliseconds for a single poll from Kafka. Takes the value of the stream*flush*interval_ms server setting by default (500ms). Default: `0`.
         * 
         * @return builder
         * 
         */
        public Builder pollMaxTimeoutMs(@Nullable Output<Integer> pollMaxTimeoutMs) {
            $.pollMaxTimeoutMs = pollMaxTimeoutMs;
            return this;
        }

        /**
         * @param pollMaxTimeoutMs Timeout in milliseconds for a single poll from Kafka. Takes the value of the stream*flush*interval_ms server setting by default (500ms). Default: `0`.
         * 
         * @return builder
         * 
         */
        public Builder pollMaxTimeoutMs(Integer pollMaxTimeoutMs) {
            return pollMaxTimeoutMs(Output.of(pollMaxTimeoutMs));
        }

        /**
         * @param producerBatchNumMessages The maximum number of messages in a batch sent to Kafka. If the number of messages exceeds this value, the batch is sent. Default: `10000`.
         * 
         * @return builder
         * 
         */
        public Builder producerBatchNumMessages(@Nullable Output<Integer> producerBatchNumMessages) {
            $.producerBatchNumMessages = producerBatchNumMessages;
            return this;
        }

        /**
         * @param producerBatchNumMessages The maximum number of messages in a batch sent to Kafka. If the number of messages exceeds this value, the batch is sent. Default: `10000`.
         * 
         * @return builder
         * 
         */
        public Builder producerBatchNumMessages(Integer producerBatchNumMessages) {
            return producerBatchNumMessages(Output.of(producerBatchNumMessages));
        }

        /**
         * @param producerBatchSize The maximum size in bytes of a batch of messages sent to Kafka. If the batch size is exceeded, the batch is sent. Default: `1000000`.
         * 
         * @return builder
         * 
         */
        public Builder producerBatchSize(@Nullable Output<Integer> producerBatchSize) {
            $.producerBatchSize = producerBatchSize;
            return this;
        }

        /**
         * @param producerBatchSize The maximum size in bytes of a batch of messages sent to Kafka. If the batch size is exceeded, the batch is sent. Default: `1000000`.
         * 
         * @return builder
         * 
         */
        public Builder producerBatchSize(Integer producerBatchSize) {
            return producerBatchSize(Output.of(producerBatchSize));
        }

        /**
         * @param producerCompressionCodec Enum: `gzip`, `lz4`, `none`, `snappy`, `zstd`. The compression codec to use when sending a batch of messages to Kafka. Default: `none`.
         * 
         * @return builder
         * 
         */
        public Builder producerCompressionCodec(@Nullable Output<String> producerCompressionCodec) {
            $.producerCompressionCodec = producerCompressionCodec;
            return this;
        }

        /**
         * @param producerCompressionCodec Enum: `gzip`, `lz4`, `none`, `snappy`, `zstd`. The compression codec to use when sending a batch of messages to Kafka. Default: `none`.
         * 
         * @return builder
         * 
         */
        public Builder producerCompressionCodec(String producerCompressionCodec) {
            return producerCompressionCodec(Output.of(producerCompressionCodec));
        }

        /**
         * @param producerCompressionLevel The compression level to use when sending a batch of messages to Kafka. Usable range is algorithm-dependent: [0-9] for gzip; [0-12] for lz4; only 0 for snappy; -1 = codec-dependent default compression level. Default: `-1`.
         * 
         * @return builder
         * 
         */
        public Builder producerCompressionLevel(@Nullable Output<Integer> producerCompressionLevel) {
            $.producerCompressionLevel = producerCompressionLevel;
            return this;
        }

        /**
         * @param producerCompressionLevel The compression level to use when sending a batch of messages to Kafka. Usable range is algorithm-dependent: [0-9] for gzip; [0-12] for lz4; only 0 for snappy; -1 = codec-dependent default compression level. Default: `-1`.
         * 
         * @return builder
         * 
         */
        public Builder producerCompressionLevel(Integer producerCompressionLevel) {
            return producerCompressionLevel(Output.of(producerCompressionLevel));
        }

        /**
         * @param producerLingerMs The time in milliseconds to wait for additional messages before sending a batch. If the time is exceeded, the batch is sent. Default: `5`.
         * 
         * @return builder
         * 
         */
        public Builder producerLingerMs(@Nullable Output<Integer> producerLingerMs) {
            $.producerLingerMs = producerLingerMs;
            return this;
        }

        /**
         * @param producerLingerMs The time in milliseconds to wait for additional messages before sending a batch. If the time is exceeded, the batch is sent. Default: `5`.
         * 
         * @return builder
         * 
         */
        public Builder producerLingerMs(Integer producerLingerMs) {
            return producerLingerMs(Output.of(producerLingerMs));
        }

        /**
         * @param producerQueueBufferingMaxKbytes The maximum size of the buffer in kilobytes before sending. Default: `1048576`.
         * 
         * @return builder
         * 
         */
        public Builder producerQueueBufferingMaxKbytes(@Nullable Output<Integer> producerQueueBufferingMaxKbytes) {
            $.producerQueueBufferingMaxKbytes = producerQueueBufferingMaxKbytes;
            return this;
        }

        /**
         * @param producerQueueBufferingMaxKbytes The maximum size of the buffer in kilobytes before sending. Default: `1048576`.
         * 
         * @return builder
         * 
         */
        public Builder producerQueueBufferingMaxKbytes(Integer producerQueueBufferingMaxKbytes) {
            return producerQueueBufferingMaxKbytes(Output.of(producerQueueBufferingMaxKbytes));
        }

        /**
         * @param producerQueueBufferingMaxMessages The maximum number of messages to buffer before sending. Default: `100000`.
         * 
         * @return builder
         * 
         */
        public Builder producerQueueBufferingMaxMessages(@Nullable Output<Integer> producerQueueBufferingMaxMessages) {
            $.producerQueueBufferingMaxMessages = producerQueueBufferingMaxMessages;
            return this;
        }

        /**
         * @param producerQueueBufferingMaxMessages The maximum number of messages to buffer before sending. Default: `100000`.
         * 
         * @return builder
         * 
         */
        public Builder producerQueueBufferingMaxMessages(Integer producerQueueBufferingMaxMessages) {
            return producerQueueBufferingMaxMessages(Output.of(producerQueueBufferingMaxMessages));
        }

        /**
         * @param producerRequestRequiredAcks The number of acknowledgements the leader broker must receive from ISR brokers before responding to the request: 0=Broker does not send any response/ack to client, -1 will block until message is committed by all in sync replicas (ISRs). Default: `-1`.
         * 
         * @return builder
         * 
         */
        public Builder producerRequestRequiredAcks(@Nullable Output<Integer> producerRequestRequiredAcks) {
            $.producerRequestRequiredAcks = producerRequestRequiredAcks;
            return this;
        }

        /**
         * @param producerRequestRequiredAcks The number of acknowledgements the leader broker must receive from ISR brokers before responding to the request: 0=Broker does not send any response/ack to client, -1 will block until message is committed by all in sync replicas (ISRs). Default: `-1`.
         * 
         * @return builder
         * 
         */
        public Builder producerRequestRequiredAcks(Integer producerRequestRequiredAcks) {
            return producerRequestRequiredAcks(Output.of(producerRequestRequiredAcks));
        }

        /**
         * @param skipBrokenMessages Skip at least this number of broken messages from Kafka topic per block. Default: `0`.
         * 
         * @return builder
         * 
         */
        public Builder skipBrokenMessages(@Nullable Output<Integer> skipBrokenMessages) {
            $.skipBrokenMessages = skipBrokenMessages;
            return this;
        }

        /**
         * @param skipBrokenMessages Skip at least this number of broken messages from Kafka topic per block. Default: `0`.
         * 
         * @return builder
         * 
         */
        public Builder skipBrokenMessages(Integer skipBrokenMessages) {
            return skipBrokenMessages(Output.of(skipBrokenMessages));
        }

        /**
         * @param threadPerConsumer Provide an independent thread for each consumer. All consumers run in the same thread by default. Default: `false`.
         * 
         * @return builder
         * 
         */
        public Builder threadPerConsumer(@Nullable Output<Boolean> threadPerConsumer) {
            $.threadPerConsumer = threadPerConsumer;
            return this;
        }

        /**
         * @param threadPerConsumer Provide an independent thread for each consumer. All consumers run in the same thread by default. Default: `false`.
         * 
         * @return builder
         * 
         */
        public Builder threadPerConsumer(Boolean threadPerConsumer) {
            return threadPerConsumer(Output.of(threadPerConsumer));
        }

        /**
         * @param topics Kafka topics
         * 
         * @return builder
         * 
         */
        public Builder topics(Output<List<ServiceIntegrationClickhouseKafkaUserConfigTableTopicArgs>> topics) {
            $.topics = topics;
            return this;
        }

        /**
         * @param topics Kafka topics
         * 
         * @return builder
         * 
         */
        public Builder topics(List<ServiceIntegrationClickhouseKafkaUserConfigTableTopicArgs> topics) {
            return topics(Output.of(topics));
        }

        /**
         * @param topics Kafka topics
         * 
         * @return builder
         * 
         */
        public Builder topics(ServiceIntegrationClickhouseKafkaUserConfigTableTopicArgs... topics) {
            return topics(List.of(topics));
        }

        public ServiceIntegrationClickhouseKafkaUserConfigTableArgs build() {
            if ($.columns == null) {
                throw new MissingRequiredPropertyException("ServiceIntegrationClickhouseKafkaUserConfigTableArgs", "columns");
            }
            if ($.dataFormat == null) {
                throw new MissingRequiredPropertyException("ServiceIntegrationClickhouseKafkaUserConfigTableArgs", "dataFormat");
            }
            if ($.groupName == null) {
                throw new MissingRequiredPropertyException("ServiceIntegrationClickhouseKafkaUserConfigTableArgs", "groupName");
            }
            if ($.name == null) {
                throw new MissingRequiredPropertyException("ServiceIntegrationClickhouseKafkaUserConfigTableArgs", "name");
            }
            if ($.topics == null) {
                throw new MissingRequiredPropertyException("ServiceIntegrationClickhouseKafkaUserConfigTableArgs", "topics");
            }
            return $;
        }
    }

}
