// *** WARNING: this file was generated by pulumi-java-gen. ***
// *** Do not edit by hand unless you're certain you know what you are doing! ***

package com.pulumi.aiven.outputs;

import com.pulumi.core.annotations.CustomType;
import java.lang.String;
import java.util.List;
import java.util.Objects;
import java.util.Optional;
import javax.annotation.Nullable;

@CustomType
public final class FlinkJobTableUpsertKafka {
    /**
     * @return Defines the columns from the SQL schema of the data table that are considered keys in the Kafka messages. This property cannot be changed, doing so forces recreation of the resource.
     * 
     */
    private @Nullable List<String> keyFields;
    /**
     * @return Sets the format that is used to convert the key part of Kafka messages. The possible values are `avro`, `avro-confluent`, `debezium-avro-confluent`, `debezium-json` and `json`. This property cannot be changed, doing so forces recreation of the resource.
     * 
     */
    private @Nullable String keyFormat;
    /**
     * @return Controls the startup method for the Kafka consumer that Aiven for Apache Flink is using. The possible values are `earliest-offset`, `latest-offset`, `group-offsets` and `timestamp`. This property cannot be changed, doing so forces recreation of the resource.
     * 
     */
    private @Nullable String scanStartupMode;
    /**
     * @return Topic name This property cannot be changed, doing so forces recreation of the resource.
     * 
     */
    private @Nullable String topic;
    /**
     * @return Controls how key columns are handled in the message value. Select ALL to include the physical columns of the table schema in the message value. Select EXCEPT_KEY to exclude the physical columns of the table schema from the message value. This is the default for upsert Kafka connectors. The possible values are `[ALL EXCEPT_KEY]`. This property cannot be changed, doing so forces recreation of the resource.
     * 
     */
    private @Nullable String valueFieldsInclude;
    /**
     * @return Sets the format that is used to convert the value part of Kafka messages. The possible values are `avro`, `avro-confluent`, `debezium-avro-confluent`, `debezium-json` and `json`. This property cannot be changed, doing so forces recreation of the resource.
     * 
     */
    private @Nullable String valueFormat;

    private FlinkJobTableUpsertKafka() {}
    /**
     * @return Defines the columns from the SQL schema of the data table that are considered keys in the Kafka messages. This property cannot be changed, doing so forces recreation of the resource.
     * 
     */
    public List<String> keyFields() {
        return this.keyFields == null ? List.of() : this.keyFields;
    }
    /**
     * @return Sets the format that is used to convert the key part of Kafka messages. The possible values are `avro`, `avro-confluent`, `debezium-avro-confluent`, `debezium-json` and `json`. This property cannot be changed, doing so forces recreation of the resource.
     * 
     */
    public Optional<String> keyFormat() {
        return Optional.ofNullable(this.keyFormat);
    }
    /**
     * @return Controls the startup method for the Kafka consumer that Aiven for Apache Flink is using. The possible values are `earliest-offset`, `latest-offset`, `group-offsets` and `timestamp`. This property cannot be changed, doing so forces recreation of the resource.
     * 
     */
    public Optional<String> scanStartupMode() {
        return Optional.ofNullable(this.scanStartupMode);
    }
    /**
     * @return Topic name This property cannot be changed, doing so forces recreation of the resource.
     * 
     */
    public Optional<String> topic() {
        return Optional.ofNullable(this.topic);
    }
    /**
     * @return Controls how key columns are handled in the message value. Select ALL to include the physical columns of the table schema in the message value. Select EXCEPT_KEY to exclude the physical columns of the table schema from the message value. This is the default for upsert Kafka connectors. The possible values are `[ALL EXCEPT_KEY]`. This property cannot be changed, doing so forces recreation of the resource.
     * 
     */
    public Optional<String> valueFieldsInclude() {
        return Optional.ofNullable(this.valueFieldsInclude);
    }
    /**
     * @return Sets the format that is used to convert the value part of Kafka messages. The possible values are `avro`, `avro-confluent`, `debezium-avro-confluent`, `debezium-json` and `json`. This property cannot be changed, doing so forces recreation of the resource.
     * 
     */
    public Optional<String> valueFormat() {
        return Optional.ofNullable(this.valueFormat);
    }

    public static Builder builder() {
        return new Builder();
    }

    public static Builder builder(FlinkJobTableUpsertKafka defaults) {
        return new Builder(defaults);
    }
    @CustomType.Builder
    public static final class Builder {
        private @Nullable List<String> keyFields;
        private @Nullable String keyFormat;
        private @Nullable String scanStartupMode;
        private @Nullable String topic;
        private @Nullable String valueFieldsInclude;
        private @Nullable String valueFormat;
        public Builder() {}
        public Builder(FlinkJobTableUpsertKafka defaults) {
    	      Objects.requireNonNull(defaults);
    	      this.keyFields = defaults.keyFields;
    	      this.keyFormat = defaults.keyFormat;
    	      this.scanStartupMode = defaults.scanStartupMode;
    	      this.topic = defaults.topic;
    	      this.valueFieldsInclude = defaults.valueFieldsInclude;
    	      this.valueFormat = defaults.valueFormat;
        }

        @CustomType.Setter
        public Builder keyFields(@Nullable List<String> keyFields) {
            this.keyFields = keyFields;
            return this;
        }
        public Builder keyFields(String... keyFields) {
            return keyFields(List.of(keyFields));
        }
        @CustomType.Setter
        public Builder keyFormat(@Nullable String keyFormat) {
            this.keyFormat = keyFormat;
            return this;
        }
        @CustomType.Setter
        public Builder scanStartupMode(@Nullable String scanStartupMode) {
            this.scanStartupMode = scanStartupMode;
            return this;
        }
        @CustomType.Setter
        public Builder topic(@Nullable String topic) {
            this.topic = topic;
            return this;
        }
        @CustomType.Setter
        public Builder valueFieldsInclude(@Nullable String valueFieldsInclude) {
            this.valueFieldsInclude = valueFieldsInclude;
            return this;
        }
        @CustomType.Setter
        public Builder valueFormat(@Nullable String valueFormat) {
            this.valueFormat = valueFormat;
            return this;
        }
        public FlinkJobTableUpsertKafka build() {
            final var o = new FlinkJobTableUpsertKafka();
            o.keyFields = keyFields;
            o.keyFormat = keyFormat;
            o.scanStartupMode = scanStartupMode;
            o.topic = topic;
            o.valueFieldsInclude = valueFieldsInclude;
            o.valueFormat = valueFormat;
            return o;
        }
    }
}
