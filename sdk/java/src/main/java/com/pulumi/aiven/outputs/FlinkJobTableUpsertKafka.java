// *** WARNING: this file was generated by pulumi-java-gen. ***
// *** Do not edit by hand unless you're certain you know what you are doing! ***

package com.pulumi.aiven.outputs;

import com.pulumi.core.annotations.CustomType;
import java.lang.String;
import java.util.List;
import java.util.Objects;
import java.util.Optional;
import javax.annotation.Nullable;

@CustomType
public final class FlinkJobTableUpsertKafka {
    /**
     * @return Defines the columns from the SQL schema of the data table that are considered keys in the Kafka messages. This property cannot be changed, doing so forces recreation of the resource.
     * 
     */
    private final @Nullable List<String> keyFields;
    /**
     * @return Sets the format that is used to convert the key part of Kafka messages. The possible values are `avro`, `avro-confluent`, `debezium-avro-confluent`, `debezium-json` and `json`. This property cannot be changed, doing so forces recreation of the resource.
     * 
     */
    private final @Nullable String keyFormat;
    /**
     * @return Controls the startup method for the Kafka consumer that Aiven for Apache Flink is using. The possible values are `earliest-offset`, `latest-offset`, `group-offsets` and `timestamp`. This property cannot be changed, doing so forces recreation of the resource.
     * 
     */
    private final @Nullable String scanStartupMode;
    /**
     * @return Topic name This property cannot be changed, doing so forces recreation of the resource.
     * 
     */
    private final @Nullable String topic;
    /**
     * @return Controls how key columns are handled in the message value. Select ALL to include the physical columns of the table schema in the message value. Select EXCEPT_KEY to exclude the physical columns of the table schema from the message value. This is the default for upsert Kafka connectors. The possible values are `[ALL EXCEPT_KEY]`. This property cannot be changed, doing so forces recreation of the resource.
     * 
     */
    private final @Nullable String valueFieldsInclude;
    /**
     * @return Sets the format that is used to convert the value part of Kafka messages. The possible values are `avro`, `avro-confluent`, `debezium-avro-confluent`, `debezium-json` and `json`. This property cannot be changed, doing so forces recreation of the resource.
     * 
     */
    private final @Nullable String valueFormat;

    @CustomType.Constructor
    private FlinkJobTableUpsertKafka(
        @CustomType.Parameter("keyFields") @Nullable List<String> keyFields,
        @CustomType.Parameter("keyFormat") @Nullable String keyFormat,
        @CustomType.Parameter("scanStartupMode") @Nullable String scanStartupMode,
        @CustomType.Parameter("topic") @Nullable String topic,
        @CustomType.Parameter("valueFieldsInclude") @Nullable String valueFieldsInclude,
        @CustomType.Parameter("valueFormat") @Nullable String valueFormat) {
        this.keyFields = keyFields;
        this.keyFormat = keyFormat;
        this.scanStartupMode = scanStartupMode;
        this.topic = topic;
        this.valueFieldsInclude = valueFieldsInclude;
        this.valueFormat = valueFormat;
    }

    /**
     * @return Defines the columns from the SQL schema of the data table that are considered keys in the Kafka messages. This property cannot be changed, doing so forces recreation of the resource.
     * 
     */
    public List<String> keyFields() {
        return this.keyFields == null ? List.of() : this.keyFields;
    }
    /**
     * @return Sets the format that is used to convert the key part of Kafka messages. The possible values are `avro`, `avro-confluent`, `debezium-avro-confluent`, `debezium-json` and `json`. This property cannot be changed, doing so forces recreation of the resource.
     * 
     */
    public Optional<String> keyFormat() {
        return Optional.ofNullable(this.keyFormat);
    }
    /**
     * @return Controls the startup method for the Kafka consumer that Aiven for Apache Flink is using. The possible values are `earliest-offset`, `latest-offset`, `group-offsets` and `timestamp`. This property cannot be changed, doing so forces recreation of the resource.
     * 
     */
    public Optional<String> scanStartupMode() {
        return Optional.ofNullable(this.scanStartupMode);
    }
    /**
     * @return Topic name This property cannot be changed, doing so forces recreation of the resource.
     * 
     */
    public Optional<String> topic() {
        return Optional.ofNullable(this.topic);
    }
    /**
     * @return Controls how key columns are handled in the message value. Select ALL to include the physical columns of the table schema in the message value. Select EXCEPT_KEY to exclude the physical columns of the table schema from the message value. This is the default for upsert Kafka connectors. The possible values are `[ALL EXCEPT_KEY]`. This property cannot be changed, doing so forces recreation of the resource.
     * 
     */
    public Optional<String> valueFieldsInclude() {
        return Optional.ofNullable(this.valueFieldsInclude);
    }
    /**
     * @return Sets the format that is used to convert the value part of Kafka messages. The possible values are `avro`, `avro-confluent`, `debezium-avro-confluent`, `debezium-json` and `json`. This property cannot be changed, doing so forces recreation of the resource.
     * 
     */
    public Optional<String> valueFormat() {
        return Optional.ofNullable(this.valueFormat);
    }

    public static Builder builder() {
        return new Builder();
    }

    public static Builder builder(FlinkJobTableUpsertKafka defaults) {
        return new Builder(defaults);
    }

    public static final class Builder {
        private @Nullable List<String> keyFields;
        private @Nullable String keyFormat;
        private @Nullable String scanStartupMode;
        private @Nullable String topic;
        private @Nullable String valueFieldsInclude;
        private @Nullable String valueFormat;

        public Builder() {
    	      // Empty
        }

        public Builder(FlinkJobTableUpsertKafka defaults) {
    	      Objects.requireNonNull(defaults);
    	      this.keyFields = defaults.keyFields;
    	      this.keyFormat = defaults.keyFormat;
    	      this.scanStartupMode = defaults.scanStartupMode;
    	      this.topic = defaults.topic;
    	      this.valueFieldsInclude = defaults.valueFieldsInclude;
    	      this.valueFormat = defaults.valueFormat;
        }

        public Builder keyFields(@Nullable List<String> keyFields) {
            this.keyFields = keyFields;
            return this;
        }
        public Builder keyFields(String... keyFields) {
            return keyFields(List.of(keyFields));
        }
        public Builder keyFormat(@Nullable String keyFormat) {
            this.keyFormat = keyFormat;
            return this;
        }
        public Builder scanStartupMode(@Nullable String scanStartupMode) {
            this.scanStartupMode = scanStartupMode;
            return this;
        }
        public Builder topic(@Nullable String topic) {
            this.topic = topic;
            return this;
        }
        public Builder valueFieldsInclude(@Nullable String valueFieldsInclude) {
            this.valueFieldsInclude = valueFieldsInclude;
            return this;
        }
        public Builder valueFormat(@Nullable String valueFormat) {
            this.valueFormat = valueFormat;
            return this;
        }        public FlinkJobTableUpsertKafka build() {
            return new FlinkJobTableUpsertKafka(keyFields, keyFormat, scanStartupMode, topic, valueFieldsInclude, valueFormat);
        }
    }
}
