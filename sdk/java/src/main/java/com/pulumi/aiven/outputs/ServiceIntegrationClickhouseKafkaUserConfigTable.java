// *** WARNING: this file was generated by pulumi-language-java. ***
// *** Do not edit by hand unless you're certain you know what you are doing! ***

package com.pulumi.aiven.outputs;

import com.pulumi.aiven.outputs.ServiceIntegrationClickhouseKafkaUserConfigTableColumn;
import com.pulumi.aiven.outputs.ServiceIntegrationClickhouseKafkaUserConfigTableTopic;
import com.pulumi.core.annotations.CustomType;
import com.pulumi.exceptions.MissingRequiredPropertyException;
import java.lang.Boolean;
import java.lang.Integer;
import java.lang.String;
import java.util.List;
import java.util.Objects;
import java.util.Optional;
import javax.annotation.Nullable;

@CustomType
public final class ServiceIntegrationClickhouseKafkaUserConfigTable {
    /**
     * @return Enum: `beginning`, `earliest`, `end`, `largest`, `latest`, `smallest`. Determines where to start reading from Kafka when no offset is stored or the stored offset is out of range. `earliest` starts from the beginning, `latest` starts from the end. Default: `earliest`.
     * 
     */
    private @Nullable String autoOffsetReset;
    /**
     * @return Array of column definitions that specify the structure of the ClickHouse table. Each column maps to a field in the Kafka messages
     * 
     */
    private List<ServiceIntegrationClickhouseKafkaUserConfigTableColumn> columns;
    /**
     * @return Enum: `Avro`, `AvroConfluent`, `CSV`, `JSONAsString`, `JSONCompactEachRow`, `JSONCompactStringsEachRow`, `JSONEachRow`, `JSONStringsEachRow`, `MsgPack`, `Parquet`, `RawBLOB`, `TSKV`, `TSV`, `TabSeparated`. The format of the messages in the Kafka topics. Determines how ClickHouse parses and serializes the data (e.g., JSON, CSV, Avro). Default: `JSONEachRow`.
     * 
     */
    private String dataFormat;
    /**
     * @return Enum: `basic`, `best_effort`, `best_effort_us`. Specifies how ClickHouse should parse DateTime values from text-based input formats. `basic` uses simple parsing, `best_effort` attempts more flexible parsing. Default: `basic`.
     * 
     */
    private @Nullable String dateTimeInputFormat;
    /**
     * @return The Kafka consumer group name. Multiple consumers with the same group name will share the workload and maintain offset positions. Default: `clickhouse`.
     * 
     */
    private String groupName;
    /**
     * @return Enum: `default`, `stream`. Defines how ClickHouse should handle errors when processing Kafka messages. `default` stops on errors, `stream` continues processing and logs errors. Default: `default`.
     * 
     */
    private @Nullable String handleErrorMode;
    /**
     * @return Maximum number of rows to collect before flushing data between Kafka and ClickHouse. Default: `0`.
     * 
     */
    private @Nullable Integer maxBlockSize;
    /**
     * @return Maximum number of rows that can be processed from a single Kafka message for row-based formats. Useful for controlling memory usage. Default: `1`.
     * 
     */
    private @Nullable Integer maxRowsPerMessage;
    /**
     * @return The name of the ClickHouse table to be created. This table can consume data from and write data to the specified Kafka topics. Example: `events`.
     * 
     */
    private String name;
    /**
     * @return Number of Kafka consumers to run per table per replica. Increasing this can improve throughput but may increase resource usage. Default: `1`.
     * 
     */
    private @Nullable Integer numConsumers;
    /**
     * @return Maximum number of messages to fetch in a single Kafka poll operation for reading. Default: `0`.
     * 
     */
    private @Nullable Integer pollMaxBatchSize;
    /**
     * @return Timeout in milliseconds for a single poll from Kafka. Takes the value of the stream*flush*interval_ms server setting by default (500ms). Default: `0`.
     * 
     */
    private @Nullable Integer pollMaxTimeoutMs;
    /**
     * @return The maximum number of messages in a batch sent to Kafka. If the number of messages exceeds this value, the batch is sent. Default: `10000`.
     * 
     */
    private @Nullable Integer producerBatchNumMessages;
    /**
     * @return The maximum size in bytes of a batch of messages sent to Kafka. If the batch size is exceeded, the batch is sent.
     * 
     */
    private @Nullable Integer producerBatchSize;
    /**
     * @return Enum: `gzip`, `lz4`, `none`, `snappy`, `zstd`. The compression codec to use when sending a batch of messages to Kafka. Default: `none`.
     * 
     */
    private @Nullable String producerCompressionCodec;
    /**
     * @return The compression level to use when sending a batch of messages to Kafka. Usable range is algorithm-dependent: [0-9] for gzip; [0-12] for lz4; only 0 for snappy; -1 = codec-dependent default compression level. Default: `-1`.
     * 
     */
    private @Nullable Integer producerCompressionLevel;
    /**
     * @return The time in milliseconds to wait for additional messages before sending a batch. If the time is exceeded, the batch is sent. Default: `5`.
     * 
     */
    private @Nullable Integer producerLingerMs;
    /**
     * @return The maximum size of the buffer in kilobytes before sending.
     * 
     */
    private @Nullable Integer producerQueueBufferingMaxKbytes;
    /**
     * @return The maximum number of messages to buffer before sending. Default: `100000`.
     * 
     */
    private @Nullable Integer producerQueueBufferingMaxMessages;
    /**
     * @return The number of acknowledgements the leader broker must receive from ISR brokers before responding to the request: 0=Broker does not send any response/ack to client, -1 will block until message is committed by all in sync replicas (ISRs). Default: `-1`.
     * 
     */
    private @Nullable Integer producerRequestRequiredAcks;
    /**
     * @return Number of broken messages to skip before stopping processing when reading from Kafka. Useful for handling corrupted data without failing the entire integration. Default: `0`.
     * 
     */
    private @Nullable Integer skipBrokenMessages;
    /**
     * @return When enabled, each consumer runs in its own thread, providing better isolation and potentially better performance for high-throughput scenarios. Default: `false`.
     * 
     */
    private @Nullable Boolean threadPerConsumer;
    /**
     * @return Array of Kafka topics that this table will read data from or write data to. Messages from all specified topics will be inserted into this table, and data inserted into this table will be published to the topics
     * 
     */
    private List<ServiceIntegrationClickhouseKafkaUserConfigTableTopic> topics;

    private ServiceIntegrationClickhouseKafkaUserConfigTable() {}
    /**
     * @return Enum: `beginning`, `earliest`, `end`, `largest`, `latest`, `smallest`. Determines where to start reading from Kafka when no offset is stored or the stored offset is out of range. `earliest` starts from the beginning, `latest` starts from the end. Default: `earliest`.
     * 
     */
    public Optional<String> autoOffsetReset() {
        return Optional.ofNullable(this.autoOffsetReset);
    }
    /**
     * @return Array of column definitions that specify the structure of the ClickHouse table. Each column maps to a field in the Kafka messages
     * 
     */
    public List<ServiceIntegrationClickhouseKafkaUserConfigTableColumn> columns() {
        return this.columns;
    }
    /**
     * @return Enum: `Avro`, `AvroConfluent`, `CSV`, `JSONAsString`, `JSONCompactEachRow`, `JSONCompactStringsEachRow`, `JSONEachRow`, `JSONStringsEachRow`, `MsgPack`, `Parquet`, `RawBLOB`, `TSKV`, `TSV`, `TabSeparated`. The format of the messages in the Kafka topics. Determines how ClickHouse parses and serializes the data (e.g., JSON, CSV, Avro). Default: `JSONEachRow`.
     * 
     */
    public String dataFormat() {
        return this.dataFormat;
    }
    /**
     * @return Enum: `basic`, `best_effort`, `best_effort_us`. Specifies how ClickHouse should parse DateTime values from text-based input formats. `basic` uses simple parsing, `best_effort` attempts more flexible parsing. Default: `basic`.
     * 
     */
    public Optional<String> dateTimeInputFormat() {
        return Optional.ofNullable(this.dateTimeInputFormat);
    }
    /**
     * @return The Kafka consumer group name. Multiple consumers with the same group name will share the workload and maintain offset positions. Default: `clickhouse`.
     * 
     */
    public String groupName() {
        return this.groupName;
    }
    /**
     * @return Enum: `default`, `stream`. Defines how ClickHouse should handle errors when processing Kafka messages. `default` stops on errors, `stream` continues processing and logs errors. Default: `default`.
     * 
     */
    public Optional<String> handleErrorMode() {
        return Optional.ofNullable(this.handleErrorMode);
    }
    /**
     * @return Maximum number of rows to collect before flushing data between Kafka and ClickHouse. Default: `0`.
     * 
     */
    public Optional<Integer> maxBlockSize() {
        return Optional.ofNullable(this.maxBlockSize);
    }
    /**
     * @return Maximum number of rows that can be processed from a single Kafka message for row-based formats. Useful for controlling memory usage. Default: `1`.
     * 
     */
    public Optional<Integer> maxRowsPerMessage() {
        return Optional.ofNullable(this.maxRowsPerMessage);
    }
    /**
     * @return The name of the ClickHouse table to be created. This table can consume data from and write data to the specified Kafka topics. Example: `events`.
     * 
     */
    public String name() {
        return this.name;
    }
    /**
     * @return Number of Kafka consumers to run per table per replica. Increasing this can improve throughput but may increase resource usage. Default: `1`.
     * 
     */
    public Optional<Integer> numConsumers() {
        return Optional.ofNullable(this.numConsumers);
    }
    /**
     * @return Maximum number of messages to fetch in a single Kafka poll operation for reading. Default: `0`.
     * 
     */
    public Optional<Integer> pollMaxBatchSize() {
        return Optional.ofNullable(this.pollMaxBatchSize);
    }
    /**
     * @return Timeout in milliseconds for a single poll from Kafka. Takes the value of the stream*flush*interval_ms server setting by default (500ms). Default: `0`.
     * 
     */
    public Optional<Integer> pollMaxTimeoutMs() {
        return Optional.ofNullable(this.pollMaxTimeoutMs);
    }
    /**
     * @return The maximum number of messages in a batch sent to Kafka. If the number of messages exceeds this value, the batch is sent. Default: `10000`.
     * 
     */
    public Optional<Integer> producerBatchNumMessages() {
        return Optional.ofNullable(this.producerBatchNumMessages);
    }
    /**
     * @return The maximum size in bytes of a batch of messages sent to Kafka. If the batch size is exceeded, the batch is sent.
     * 
     */
    public Optional<Integer> producerBatchSize() {
        return Optional.ofNullable(this.producerBatchSize);
    }
    /**
     * @return Enum: `gzip`, `lz4`, `none`, `snappy`, `zstd`. The compression codec to use when sending a batch of messages to Kafka. Default: `none`.
     * 
     */
    public Optional<String> producerCompressionCodec() {
        return Optional.ofNullable(this.producerCompressionCodec);
    }
    /**
     * @return The compression level to use when sending a batch of messages to Kafka. Usable range is algorithm-dependent: [0-9] for gzip; [0-12] for lz4; only 0 for snappy; -1 = codec-dependent default compression level. Default: `-1`.
     * 
     */
    public Optional<Integer> producerCompressionLevel() {
        return Optional.ofNullable(this.producerCompressionLevel);
    }
    /**
     * @return The time in milliseconds to wait for additional messages before sending a batch. If the time is exceeded, the batch is sent. Default: `5`.
     * 
     */
    public Optional<Integer> producerLingerMs() {
        return Optional.ofNullable(this.producerLingerMs);
    }
    /**
     * @return The maximum size of the buffer in kilobytes before sending.
     * 
     */
    public Optional<Integer> producerQueueBufferingMaxKbytes() {
        return Optional.ofNullable(this.producerQueueBufferingMaxKbytes);
    }
    /**
     * @return The maximum number of messages to buffer before sending. Default: `100000`.
     * 
     */
    public Optional<Integer> producerQueueBufferingMaxMessages() {
        return Optional.ofNullable(this.producerQueueBufferingMaxMessages);
    }
    /**
     * @return The number of acknowledgements the leader broker must receive from ISR brokers before responding to the request: 0=Broker does not send any response/ack to client, -1 will block until message is committed by all in sync replicas (ISRs). Default: `-1`.
     * 
     */
    public Optional<Integer> producerRequestRequiredAcks() {
        return Optional.ofNullable(this.producerRequestRequiredAcks);
    }
    /**
     * @return Number of broken messages to skip before stopping processing when reading from Kafka. Useful for handling corrupted data without failing the entire integration. Default: `0`.
     * 
     */
    public Optional<Integer> skipBrokenMessages() {
        return Optional.ofNullable(this.skipBrokenMessages);
    }
    /**
     * @return When enabled, each consumer runs in its own thread, providing better isolation and potentially better performance for high-throughput scenarios. Default: `false`.
     * 
     */
    public Optional<Boolean> threadPerConsumer() {
        return Optional.ofNullable(this.threadPerConsumer);
    }
    /**
     * @return Array of Kafka topics that this table will read data from or write data to. Messages from all specified topics will be inserted into this table, and data inserted into this table will be published to the topics
     * 
     */
    public List<ServiceIntegrationClickhouseKafkaUserConfigTableTopic> topics() {
        return this.topics;
    }

    public static Builder builder() {
        return new Builder();
    }

    public static Builder builder(ServiceIntegrationClickhouseKafkaUserConfigTable defaults) {
        return new Builder(defaults);
    }
    @CustomType.Builder
    public static final class Builder {
        private @Nullable String autoOffsetReset;
        private List<ServiceIntegrationClickhouseKafkaUserConfigTableColumn> columns;
        private String dataFormat;
        private @Nullable String dateTimeInputFormat;
        private String groupName;
        private @Nullable String handleErrorMode;
        private @Nullable Integer maxBlockSize;
        private @Nullable Integer maxRowsPerMessage;
        private String name;
        private @Nullable Integer numConsumers;
        private @Nullable Integer pollMaxBatchSize;
        private @Nullable Integer pollMaxTimeoutMs;
        private @Nullable Integer producerBatchNumMessages;
        private @Nullable Integer producerBatchSize;
        private @Nullable String producerCompressionCodec;
        private @Nullable Integer producerCompressionLevel;
        private @Nullable Integer producerLingerMs;
        private @Nullable Integer producerQueueBufferingMaxKbytes;
        private @Nullable Integer producerQueueBufferingMaxMessages;
        private @Nullable Integer producerRequestRequiredAcks;
        private @Nullable Integer skipBrokenMessages;
        private @Nullable Boolean threadPerConsumer;
        private List<ServiceIntegrationClickhouseKafkaUserConfigTableTopic> topics;
        public Builder() {}
        public Builder(ServiceIntegrationClickhouseKafkaUserConfigTable defaults) {
    	      Objects.requireNonNull(defaults);
    	      this.autoOffsetReset = defaults.autoOffsetReset;
    	      this.columns = defaults.columns;
    	      this.dataFormat = defaults.dataFormat;
    	      this.dateTimeInputFormat = defaults.dateTimeInputFormat;
    	      this.groupName = defaults.groupName;
    	      this.handleErrorMode = defaults.handleErrorMode;
    	      this.maxBlockSize = defaults.maxBlockSize;
    	      this.maxRowsPerMessage = defaults.maxRowsPerMessage;
    	      this.name = defaults.name;
    	      this.numConsumers = defaults.numConsumers;
    	      this.pollMaxBatchSize = defaults.pollMaxBatchSize;
    	      this.pollMaxTimeoutMs = defaults.pollMaxTimeoutMs;
    	      this.producerBatchNumMessages = defaults.producerBatchNumMessages;
    	      this.producerBatchSize = defaults.producerBatchSize;
    	      this.producerCompressionCodec = defaults.producerCompressionCodec;
    	      this.producerCompressionLevel = defaults.producerCompressionLevel;
    	      this.producerLingerMs = defaults.producerLingerMs;
    	      this.producerQueueBufferingMaxKbytes = defaults.producerQueueBufferingMaxKbytes;
    	      this.producerQueueBufferingMaxMessages = defaults.producerQueueBufferingMaxMessages;
    	      this.producerRequestRequiredAcks = defaults.producerRequestRequiredAcks;
    	      this.skipBrokenMessages = defaults.skipBrokenMessages;
    	      this.threadPerConsumer = defaults.threadPerConsumer;
    	      this.topics = defaults.topics;
        }

        @CustomType.Setter
        public Builder autoOffsetReset(@Nullable String autoOffsetReset) {

            this.autoOffsetReset = autoOffsetReset;
            return this;
        }
        @CustomType.Setter
        public Builder columns(List<ServiceIntegrationClickhouseKafkaUserConfigTableColumn> columns) {
            if (columns == null) {
              throw new MissingRequiredPropertyException("ServiceIntegrationClickhouseKafkaUserConfigTable", "columns");
            }
            this.columns = columns;
            return this;
        }
        public Builder columns(ServiceIntegrationClickhouseKafkaUserConfigTableColumn... columns) {
            return columns(List.of(columns));
        }
        @CustomType.Setter
        public Builder dataFormat(String dataFormat) {
            if (dataFormat == null) {
              throw new MissingRequiredPropertyException("ServiceIntegrationClickhouseKafkaUserConfigTable", "dataFormat");
            }
            this.dataFormat = dataFormat;
            return this;
        }
        @CustomType.Setter
        public Builder dateTimeInputFormat(@Nullable String dateTimeInputFormat) {

            this.dateTimeInputFormat = dateTimeInputFormat;
            return this;
        }
        @CustomType.Setter
        public Builder groupName(String groupName) {
            if (groupName == null) {
              throw new MissingRequiredPropertyException("ServiceIntegrationClickhouseKafkaUserConfigTable", "groupName");
            }
            this.groupName = groupName;
            return this;
        }
        @CustomType.Setter
        public Builder handleErrorMode(@Nullable String handleErrorMode) {

            this.handleErrorMode = handleErrorMode;
            return this;
        }
        @CustomType.Setter
        public Builder maxBlockSize(@Nullable Integer maxBlockSize) {

            this.maxBlockSize = maxBlockSize;
            return this;
        }
        @CustomType.Setter
        public Builder maxRowsPerMessage(@Nullable Integer maxRowsPerMessage) {

            this.maxRowsPerMessage = maxRowsPerMessage;
            return this;
        }
        @CustomType.Setter
        public Builder name(String name) {
            if (name == null) {
              throw new MissingRequiredPropertyException("ServiceIntegrationClickhouseKafkaUserConfigTable", "name");
            }
            this.name = name;
            return this;
        }
        @CustomType.Setter
        public Builder numConsumers(@Nullable Integer numConsumers) {

            this.numConsumers = numConsumers;
            return this;
        }
        @CustomType.Setter
        public Builder pollMaxBatchSize(@Nullable Integer pollMaxBatchSize) {

            this.pollMaxBatchSize = pollMaxBatchSize;
            return this;
        }
        @CustomType.Setter
        public Builder pollMaxTimeoutMs(@Nullable Integer pollMaxTimeoutMs) {

            this.pollMaxTimeoutMs = pollMaxTimeoutMs;
            return this;
        }
        @CustomType.Setter
        public Builder producerBatchNumMessages(@Nullable Integer producerBatchNumMessages) {

            this.producerBatchNumMessages = producerBatchNumMessages;
            return this;
        }
        @CustomType.Setter
        public Builder producerBatchSize(@Nullable Integer producerBatchSize) {

            this.producerBatchSize = producerBatchSize;
            return this;
        }
        @CustomType.Setter
        public Builder producerCompressionCodec(@Nullable String producerCompressionCodec) {

            this.producerCompressionCodec = producerCompressionCodec;
            return this;
        }
        @CustomType.Setter
        public Builder producerCompressionLevel(@Nullable Integer producerCompressionLevel) {

            this.producerCompressionLevel = producerCompressionLevel;
            return this;
        }
        @CustomType.Setter
        public Builder producerLingerMs(@Nullable Integer producerLingerMs) {

            this.producerLingerMs = producerLingerMs;
            return this;
        }
        @CustomType.Setter
        public Builder producerQueueBufferingMaxKbytes(@Nullable Integer producerQueueBufferingMaxKbytes) {

            this.producerQueueBufferingMaxKbytes = producerQueueBufferingMaxKbytes;
            return this;
        }
        @CustomType.Setter
        public Builder producerQueueBufferingMaxMessages(@Nullable Integer producerQueueBufferingMaxMessages) {

            this.producerQueueBufferingMaxMessages = producerQueueBufferingMaxMessages;
            return this;
        }
        @CustomType.Setter
        public Builder producerRequestRequiredAcks(@Nullable Integer producerRequestRequiredAcks) {

            this.producerRequestRequiredAcks = producerRequestRequiredAcks;
            return this;
        }
        @CustomType.Setter
        public Builder skipBrokenMessages(@Nullable Integer skipBrokenMessages) {

            this.skipBrokenMessages = skipBrokenMessages;
            return this;
        }
        @CustomType.Setter
        public Builder threadPerConsumer(@Nullable Boolean threadPerConsumer) {

            this.threadPerConsumer = threadPerConsumer;
            return this;
        }
        @CustomType.Setter
        public Builder topics(List<ServiceIntegrationClickhouseKafkaUserConfigTableTopic> topics) {
            if (topics == null) {
              throw new MissingRequiredPropertyException("ServiceIntegrationClickhouseKafkaUserConfigTable", "topics");
            }
            this.topics = topics;
            return this;
        }
        public Builder topics(ServiceIntegrationClickhouseKafkaUserConfigTableTopic... topics) {
            return topics(List.of(topics));
        }
        public ServiceIntegrationClickhouseKafkaUserConfigTable build() {
            final var _resultValue = new ServiceIntegrationClickhouseKafkaUserConfigTable();
            _resultValue.autoOffsetReset = autoOffsetReset;
            _resultValue.columns = columns;
            _resultValue.dataFormat = dataFormat;
            _resultValue.dateTimeInputFormat = dateTimeInputFormat;
            _resultValue.groupName = groupName;
            _resultValue.handleErrorMode = handleErrorMode;
            _resultValue.maxBlockSize = maxBlockSize;
            _resultValue.maxRowsPerMessage = maxRowsPerMessage;
            _resultValue.name = name;
            _resultValue.numConsumers = numConsumers;
            _resultValue.pollMaxBatchSize = pollMaxBatchSize;
            _resultValue.pollMaxTimeoutMs = pollMaxTimeoutMs;
            _resultValue.producerBatchNumMessages = producerBatchNumMessages;
            _resultValue.producerBatchSize = producerBatchSize;
            _resultValue.producerCompressionCodec = producerCompressionCodec;
            _resultValue.producerCompressionLevel = producerCompressionLevel;
            _resultValue.producerLingerMs = producerLingerMs;
            _resultValue.producerQueueBufferingMaxKbytes = producerQueueBufferingMaxKbytes;
            _resultValue.producerQueueBufferingMaxMessages = producerQueueBufferingMaxMessages;
            _resultValue.producerRequestRequiredAcks = producerRequestRequiredAcks;
            _resultValue.skipBrokenMessages = skipBrokenMessages;
            _resultValue.threadPerConsumer = threadPerConsumer;
            _resultValue.topics = topics;
            return _resultValue;
        }
    }
}
