// *** WARNING: this file was generated by pulumi-java-gen. ***
// *** Do not edit by hand unless you're certain you know what you are doing! ***

package com.pulumi.aiven.outputs;

import com.pulumi.aiven.outputs.ServiceIntegrationClickhouseKafkaUserConfigTableColumn;
import com.pulumi.aiven.outputs.ServiceIntegrationClickhouseKafkaUserConfigTableTopic;
import com.pulumi.core.annotations.CustomType;
import com.pulumi.exceptions.MissingRequiredPropertyException;
import java.lang.Boolean;
import java.lang.Integer;
import java.lang.String;
import java.util.List;
import java.util.Objects;
import java.util.Optional;
import javax.annotation.Nullable;

@CustomType
public final class ServiceIntegrationClickhouseKafkaUserConfigTable {
    /**
     * @return Enum: `beginning`, `earliest`, `end`, `largest`, `latest`, `smallest`. Action to take when there is no initial offset in offset store or the desired offset is out of range. Default: `earliest`.
     * 
     */
    private @Nullable String autoOffsetReset;
    /**
     * @return Table columns
     * 
     */
    private List<ServiceIntegrationClickhouseKafkaUserConfigTableColumn> columns;
    /**
     * @return Enum: `Avro`, `AvroConfluent`, `CSV`, `JSONAsString`, `JSONCompactEachRow`, `JSONCompactStringsEachRow`, `JSONEachRow`, `JSONStringsEachRow`, `MsgPack`, `Parquet`, `RawBLOB`, `TSKV`, `TSV`, `TabSeparated`. Message data format. Default: `JSONEachRow`.
     * 
     */
    private String dataFormat;
    /**
     * @return Enum: `basic`, `best_effort`, `best_effort_us`. Method to read DateTime from text input formats. Default: `basic`.
     * 
     */
    private @Nullable String dateTimeInputFormat;
    /**
     * @return Kafka consumers group. Default: `clickhouse`.
     * 
     */
    private String groupName;
    /**
     * @return Enum: `default`, `stream`. How to handle errors for Kafka engine. Default: `default`.
     * 
     */
    private @Nullable String handleErrorMode;
    /**
     * @return Number of row collected by poll(s) for flushing data from Kafka. Default: `0`.
     * 
     */
    private @Nullable Integer maxBlockSize;
    /**
     * @return The maximum number of rows produced in one kafka message for row-based formats. Default: `1`.
     * 
     */
    private @Nullable Integer maxRowsPerMessage;
    /**
     * @return Name of the table. Example: `events`.
     * 
     */
    private String name;
    /**
     * @return The number of consumers per table per replica. Default: `1`.
     * 
     */
    private @Nullable Integer numConsumers;
    /**
     * @return Maximum amount of messages to be polled in a single Kafka poll. Default: `0`.
     * 
     */
    private @Nullable Integer pollMaxBatchSize;
    /**
     * @return Timeout in milliseconds for a single poll from Kafka. Takes the value of the stream*flush*interval_ms server setting by default (500ms). Default: `0`.
     * 
     */
    private @Nullable Integer pollMaxTimeoutMs;
    /**
     * @return The maximum number of messages in a batch sent to Kafka. If the number of messages exceeds this value, the batch is sent. Default: `10000`.
     * 
     */
    private @Nullable Integer producerBatchNumMessages;
    /**
     * @return The maximum size in bytes of a batch of messages sent to Kafka. If the batch size is exceeded, the batch is sent. Default: `1000000`.
     * 
     */
    private @Nullable Integer producerBatchSize;
    /**
     * @return Enum: `gzip`, `lz4`, `none`, `snappy`, `zstd`. The compression codec to use when sending a batch of messages to Kafka. Default: `none`.
     * 
     */
    private @Nullable String producerCompressionCodec;
    /**
     * @return The compression level to use when sending a batch of messages to Kafka. Usable range is algorithm-dependent: [0-9] for gzip; [0-12] for lz4; only 0 for snappy; -1 = codec-dependent default compression level. Default: `-1`.
     * 
     */
    private @Nullable Integer producerCompressionLevel;
    /**
     * @return The time in milliseconds to wait for additional messages before sending a batch. If the time is exceeded, the batch is sent. Default: `5`.
     * 
     */
    private @Nullable Integer producerLingerMs;
    /**
     * @return The maximum size of the buffer in kilobytes before sending. Default: `1048576`.
     * 
     */
    private @Nullable Integer producerQueueBufferingMaxKbytes;
    /**
     * @return The maximum number of messages to buffer before sending. Default: `100000`.
     * 
     */
    private @Nullable Integer producerQueueBufferingMaxMessages;
    /**
     * @return The number of acknowledgements the leader broker must receive from ISR brokers before responding to the request: 0=Broker does not send any response/ack to client, -1 will block until message is committed by all in sync replicas (ISRs). Default: `-1`.
     * 
     */
    private @Nullable Integer producerRequestRequiredAcks;
    /**
     * @return Skip at least this number of broken messages from Kafka topic per block. Default: `0`.
     * 
     */
    private @Nullable Integer skipBrokenMessages;
    /**
     * @return Provide an independent thread for each consumer. All consumers run in the same thread by default. Default: `false`.
     * 
     */
    private @Nullable Boolean threadPerConsumer;
    /**
     * @return Kafka topics
     * 
     */
    private List<ServiceIntegrationClickhouseKafkaUserConfigTableTopic> topics;

    private ServiceIntegrationClickhouseKafkaUserConfigTable() {}
    /**
     * @return Enum: `beginning`, `earliest`, `end`, `largest`, `latest`, `smallest`. Action to take when there is no initial offset in offset store or the desired offset is out of range. Default: `earliest`.
     * 
     */
    public Optional<String> autoOffsetReset() {
        return Optional.ofNullable(this.autoOffsetReset);
    }
    /**
     * @return Table columns
     * 
     */
    public List<ServiceIntegrationClickhouseKafkaUserConfigTableColumn> columns() {
        return this.columns;
    }
    /**
     * @return Enum: `Avro`, `AvroConfluent`, `CSV`, `JSONAsString`, `JSONCompactEachRow`, `JSONCompactStringsEachRow`, `JSONEachRow`, `JSONStringsEachRow`, `MsgPack`, `Parquet`, `RawBLOB`, `TSKV`, `TSV`, `TabSeparated`. Message data format. Default: `JSONEachRow`.
     * 
     */
    public String dataFormat() {
        return this.dataFormat;
    }
    /**
     * @return Enum: `basic`, `best_effort`, `best_effort_us`. Method to read DateTime from text input formats. Default: `basic`.
     * 
     */
    public Optional<String> dateTimeInputFormat() {
        return Optional.ofNullable(this.dateTimeInputFormat);
    }
    /**
     * @return Kafka consumers group. Default: `clickhouse`.
     * 
     */
    public String groupName() {
        return this.groupName;
    }
    /**
     * @return Enum: `default`, `stream`. How to handle errors for Kafka engine. Default: `default`.
     * 
     */
    public Optional<String> handleErrorMode() {
        return Optional.ofNullable(this.handleErrorMode);
    }
    /**
     * @return Number of row collected by poll(s) for flushing data from Kafka. Default: `0`.
     * 
     */
    public Optional<Integer> maxBlockSize() {
        return Optional.ofNullable(this.maxBlockSize);
    }
    /**
     * @return The maximum number of rows produced in one kafka message for row-based formats. Default: `1`.
     * 
     */
    public Optional<Integer> maxRowsPerMessage() {
        return Optional.ofNullable(this.maxRowsPerMessage);
    }
    /**
     * @return Name of the table. Example: `events`.
     * 
     */
    public String name() {
        return this.name;
    }
    /**
     * @return The number of consumers per table per replica. Default: `1`.
     * 
     */
    public Optional<Integer> numConsumers() {
        return Optional.ofNullable(this.numConsumers);
    }
    /**
     * @return Maximum amount of messages to be polled in a single Kafka poll. Default: `0`.
     * 
     */
    public Optional<Integer> pollMaxBatchSize() {
        return Optional.ofNullable(this.pollMaxBatchSize);
    }
    /**
     * @return Timeout in milliseconds for a single poll from Kafka. Takes the value of the stream*flush*interval_ms server setting by default (500ms). Default: `0`.
     * 
     */
    public Optional<Integer> pollMaxTimeoutMs() {
        return Optional.ofNullable(this.pollMaxTimeoutMs);
    }
    /**
     * @return The maximum number of messages in a batch sent to Kafka. If the number of messages exceeds this value, the batch is sent. Default: `10000`.
     * 
     */
    public Optional<Integer> producerBatchNumMessages() {
        return Optional.ofNullable(this.producerBatchNumMessages);
    }
    /**
     * @return The maximum size in bytes of a batch of messages sent to Kafka. If the batch size is exceeded, the batch is sent. Default: `1000000`.
     * 
     */
    public Optional<Integer> producerBatchSize() {
        return Optional.ofNullable(this.producerBatchSize);
    }
    /**
     * @return Enum: `gzip`, `lz4`, `none`, `snappy`, `zstd`. The compression codec to use when sending a batch of messages to Kafka. Default: `none`.
     * 
     */
    public Optional<String> producerCompressionCodec() {
        return Optional.ofNullable(this.producerCompressionCodec);
    }
    /**
     * @return The compression level to use when sending a batch of messages to Kafka. Usable range is algorithm-dependent: [0-9] for gzip; [0-12] for lz4; only 0 for snappy; -1 = codec-dependent default compression level. Default: `-1`.
     * 
     */
    public Optional<Integer> producerCompressionLevel() {
        return Optional.ofNullable(this.producerCompressionLevel);
    }
    /**
     * @return The time in milliseconds to wait for additional messages before sending a batch. If the time is exceeded, the batch is sent. Default: `5`.
     * 
     */
    public Optional<Integer> producerLingerMs() {
        return Optional.ofNullable(this.producerLingerMs);
    }
    /**
     * @return The maximum size of the buffer in kilobytes before sending. Default: `1048576`.
     * 
     */
    public Optional<Integer> producerQueueBufferingMaxKbytes() {
        return Optional.ofNullable(this.producerQueueBufferingMaxKbytes);
    }
    /**
     * @return The maximum number of messages to buffer before sending. Default: `100000`.
     * 
     */
    public Optional<Integer> producerQueueBufferingMaxMessages() {
        return Optional.ofNullable(this.producerQueueBufferingMaxMessages);
    }
    /**
     * @return The number of acknowledgements the leader broker must receive from ISR brokers before responding to the request: 0=Broker does not send any response/ack to client, -1 will block until message is committed by all in sync replicas (ISRs). Default: `-1`.
     * 
     */
    public Optional<Integer> producerRequestRequiredAcks() {
        return Optional.ofNullable(this.producerRequestRequiredAcks);
    }
    /**
     * @return Skip at least this number of broken messages from Kafka topic per block. Default: `0`.
     * 
     */
    public Optional<Integer> skipBrokenMessages() {
        return Optional.ofNullable(this.skipBrokenMessages);
    }
    /**
     * @return Provide an independent thread for each consumer. All consumers run in the same thread by default. Default: `false`.
     * 
     */
    public Optional<Boolean> threadPerConsumer() {
        return Optional.ofNullable(this.threadPerConsumer);
    }
    /**
     * @return Kafka topics
     * 
     */
    public List<ServiceIntegrationClickhouseKafkaUserConfigTableTopic> topics() {
        return this.topics;
    }

    public static Builder builder() {
        return new Builder();
    }

    public static Builder builder(ServiceIntegrationClickhouseKafkaUserConfigTable defaults) {
        return new Builder(defaults);
    }
    @CustomType.Builder
    public static final class Builder {
        private @Nullable String autoOffsetReset;
        private List<ServiceIntegrationClickhouseKafkaUserConfigTableColumn> columns;
        private String dataFormat;
        private @Nullable String dateTimeInputFormat;
        private String groupName;
        private @Nullable String handleErrorMode;
        private @Nullable Integer maxBlockSize;
        private @Nullable Integer maxRowsPerMessage;
        private String name;
        private @Nullable Integer numConsumers;
        private @Nullable Integer pollMaxBatchSize;
        private @Nullable Integer pollMaxTimeoutMs;
        private @Nullable Integer producerBatchNumMessages;
        private @Nullable Integer producerBatchSize;
        private @Nullable String producerCompressionCodec;
        private @Nullable Integer producerCompressionLevel;
        private @Nullable Integer producerLingerMs;
        private @Nullable Integer producerQueueBufferingMaxKbytes;
        private @Nullable Integer producerQueueBufferingMaxMessages;
        private @Nullable Integer producerRequestRequiredAcks;
        private @Nullable Integer skipBrokenMessages;
        private @Nullable Boolean threadPerConsumer;
        private List<ServiceIntegrationClickhouseKafkaUserConfigTableTopic> topics;
        public Builder() {}
        public Builder(ServiceIntegrationClickhouseKafkaUserConfigTable defaults) {
    	      Objects.requireNonNull(defaults);
    	      this.autoOffsetReset = defaults.autoOffsetReset;
    	      this.columns = defaults.columns;
    	      this.dataFormat = defaults.dataFormat;
    	      this.dateTimeInputFormat = defaults.dateTimeInputFormat;
    	      this.groupName = defaults.groupName;
    	      this.handleErrorMode = defaults.handleErrorMode;
    	      this.maxBlockSize = defaults.maxBlockSize;
    	      this.maxRowsPerMessage = defaults.maxRowsPerMessage;
    	      this.name = defaults.name;
    	      this.numConsumers = defaults.numConsumers;
    	      this.pollMaxBatchSize = defaults.pollMaxBatchSize;
    	      this.pollMaxTimeoutMs = defaults.pollMaxTimeoutMs;
    	      this.producerBatchNumMessages = defaults.producerBatchNumMessages;
    	      this.producerBatchSize = defaults.producerBatchSize;
    	      this.producerCompressionCodec = defaults.producerCompressionCodec;
    	      this.producerCompressionLevel = defaults.producerCompressionLevel;
    	      this.producerLingerMs = defaults.producerLingerMs;
    	      this.producerQueueBufferingMaxKbytes = defaults.producerQueueBufferingMaxKbytes;
    	      this.producerQueueBufferingMaxMessages = defaults.producerQueueBufferingMaxMessages;
    	      this.producerRequestRequiredAcks = defaults.producerRequestRequiredAcks;
    	      this.skipBrokenMessages = defaults.skipBrokenMessages;
    	      this.threadPerConsumer = defaults.threadPerConsumer;
    	      this.topics = defaults.topics;
        }

        @CustomType.Setter
        public Builder autoOffsetReset(@Nullable String autoOffsetReset) {

            this.autoOffsetReset = autoOffsetReset;
            return this;
        }
        @CustomType.Setter
        public Builder columns(List<ServiceIntegrationClickhouseKafkaUserConfigTableColumn> columns) {
            if (columns == null) {
              throw new MissingRequiredPropertyException("ServiceIntegrationClickhouseKafkaUserConfigTable", "columns");
            }
            this.columns = columns;
            return this;
        }
        public Builder columns(ServiceIntegrationClickhouseKafkaUserConfigTableColumn... columns) {
            return columns(List.of(columns));
        }
        @CustomType.Setter
        public Builder dataFormat(String dataFormat) {
            if (dataFormat == null) {
              throw new MissingRequiredPropertyException("ServiceIntegrationClickhouseKafkaUserConfigTable", "dataFormat");
            }
            this.dataFormat = dataFormat;
            return this;
        }
        @CustomType.Setter
        public Builder dateTimeInputFormat(@Nullable String dateTimeInputFormat) {

            this.dateTimeInputFormat = dateTimeInputFormat;
            return this;
        }
        @CustomType.Setter
        public Builder groupName(String groupName) {
            if (groupName == null) {
              throw new MissingRequiredPropertyException("ServiceIntegrationClickhouseKafkaUserConfigTable", "groupName");
            }
            this.groupName = groupName;
            return this;
        }
        @CustomType.Setter
        public Builder handleErrorMode(@Nullable String handleErrorMode) {

            this.handleErrorMode = handleErrorMode;
            return this;
        }
        @CustomType.Setter
        public Builder maxBlockSize(@Nullable Integer maxBlockSize) {

            this.maxBlockSize = maxBlockSize;
            return this;
        }
        @CustomType.Setter
        public Builder maxRowsPerMessage(@Nullable Integer maxRowsPerMessage) {

            this.maxRowsPerMessage = maxRowsPerMessage;
            return this;
        }
        @CustomType.Setter
        public Builder name(String name) {
            if (name == null) {
              throw new MissingRequiredPropertyException("ServiceIntegrationClickhouseKafkaUserConfigTable", "name");
            }
            this.name = name;
            return this;
        }
        @CustomType.Setter
        public Builder numConsumers(@Nullable Integer numConsumers) {

            this.numConsumers = numConsumers;
            return this;
        }
        @CustomType.Setter
        public Builder pollMaxBatchSize(@Nullable Integer pollMaxBatchSize) {

            this.pollMaxBatchSize = pollMaxBatchSize;
            return this;
        }
        @CustomType.Setter
        public Builder pollMaxTimeoutMs(@Nullable Integer pollMaxTimeoutMs) {

            this.pollMaxTimeoutMs = pollMaxTimeoutMs;
            return this;
        }
        @CustomType.Setter
        public Builder producerBatchNumMessages(@Nullable Integer producerBatchNumMessages) {

            this.producerBatchNumMessages = producerBatchNumMessages;
            return this;
        }
        @CustomType.Setter
        public Builder producerBatchSize(@Nullable Integer producerBatchSize) {

            this.producerBatchSize = producerBatchSize;
            return this;
        }
        @CustomType.Setter
        public Builder producerCompressionCodec(@Nullable String producerCompressionCodec) {

            this.producerCompressionCodec = producerCompressionCodec;
            return this;
        }
        @CustomType.Setter
        public Builder producerCompressionLevel(@Nullable Integer producerCompressionLevel) {

            this.producerCompressionLevel = producerCompressionLevel;
            return this;
        }
        @CustomType.Setter
        public Builder producerLingerMs(@Nullable Integer producerLingerMs) {

            this.producerLingerMs = producerLingerMs;
            return this;
        }
        @CustomType.Setter
        public Builder producerQueueBufferingMaxKbytes(@Nullable Integer producerQueueBufferingMaxKbytes) {

            this.producerQueueBufferingMaxKbytes = producerQueueBufferingMaxKbytes;
            return this;
        }
        @CustomType.Setter
        public Builder producerQueueBufferingMaxMessages(@Nullable Integer producerQueueBufferingMaxMessages) {

            this.producerQueueBufferingMaxMessages = producerQueueBufferingMaxMessages;
            return this;
        }
        @CustomType.Setter
        public Builder producerRequestRequiredAcks(@Nullable Integer producerRequestRequiredAcks) {

            this.producerRequestRequiredAcks = producerRequestRequiredAcks;
            return this;
        }
        @CustomType.Setter
        public Builder skipBrokenMessages(@Nullable Integer skipBrokenMessages) {

            this.skipBrokenMessages = skipBrokenMessages;
            return this;
        }
        @CustomType.Setter
        public Builder threadPerConsumer(@Nullable Boolean threadPerConsumer) {

            this.threadPerConsumer = threadPerConsumer;
            return this;
        }
        @CustomType.Setter
        public Builder topics(List<ServiceIntegrationClickhouseKafkaUserConfigTableTopic> topics) {
            if (topics == null) {
              throw new MissingRequiredPropertyException("ServiceIntegrationClickhouseKafkaUserConfigTable", "topics");
            }
            this.topics = topics;
            return this;
        }
        public Builder topics(ServiceIntegrationClickhouseKafkaUserConfigTableTopic... topics) {
            return topics(List.of(topics));
        }
        public ServiceIntegrationClickhouseKafkaUserConfigTable build() {
            final var _resultValue = new ServiceIntegrationClickhouseKafkaUserConfigTable();
            _resultValue.autoOffsetReset = autoOffsetReset;
            _resultValue.columns = columns;
            _resultValue.dataFormat = dataFormat;
            _resultValue.dateTimeInputFormat = dateTimeInputFormat;
            _resultValue.groupName = groupName;
            _resultValue.handleErrorMode = handleErrorMode;
            _resultValue.maxBlockSize = maxBlockSize;
            _resultValue.maxRowsPerMessage = maxRowsPerMessage;
            _resultValue.name = name;
            _resultValue.numConsumers = numConsumers;
            _resultValue.pollMaxBatchSize = pollMaxBatchSize;
            _resultValue.pollMaxTimeoutMs = pollMaxTimeoutMs;
            _resultValue.producerBatchNumMessages = producerBatchNumMessages;
            _resultValue.producerBatchSize = producerBatchSize;
            _resultValue.producerCompressionCodec = producerCompressionCodec;
            _resultValue.producerCompressionLevel = producerCompressionLevel;
            _resultValue.producerLingerMs = producerLingerMs;
            _resultValue.producerQueueBufferingMaxKbytes = producerQueueBufferingMaxKbytes;
            _resultValue.producerQueueBufferingMaxMessages = producerQueueBufferingMaxMessages;
            _resultValue.producerRequestRequiredAcks = producerRequestRequiredAcks;
            _resultValue.skipBrokenMessages = skipBrokenMessages;
            _resultValue.threadPerConsumer = threadPerConsumer;
            _resultValue.topics = topics;
            return _resultValue;
        }
    }
}
