// Code generated by the Pulumi Terraform Bridge (tfgen) Tool DO NOT EDIT.
// *** WARNING: Do not edit by hand unless you're certain you know what you are doing! ***

package aiven

import (
	"context"
	"reflect"

	"github.com/pkg/errors"
	"github.com/pulumi/pulumi/sdk/v3/go/pulumi"
)

// The Flink Table resource allows the creation and management of Aiven Tables.
//
// ## Example Usage
//
// ```go
// package main
//
// import (
// 	"fmt"
//
// 	"github.com/pulumi/pulumi-aiven/sdk/v4/go/aiven"
// 	"github.com/pulumi/pulumi/sdk/v3/go/pulumi"
// )
//
// func main() {
// 	pulumi.Run(func(ctx *pulumi.Context) error {
// 		_, err := aiven.NewFlinkJobTable(ctx, "table", &aiven.FlinkJobTableArgs{
// 			Project:       pulumi.Any(data.Aiven_project.Pr1.Project),
// 			ServiceName:   pulumi.Any(aiven_flink.Flink.Service_name),
// 			TableName:     pulumi.String("<TABLE_NAME>"),
// 			IntegrationId: pulumi.Any(aiven_service_integration.Flink_kafka.Service_id),
// 			JdbcTable:     pulumi.String("<JDBC_TABLE_NAME>"),
// 			KafkaTopic:    pulumi.Any(aiven_kafka_topic.Table_topic.Topic_name),
// 			SchemaSql:     pulumi.String(fmt.Sprintf("%v%v%v%v", "      `+\"`cpu`\"+` INT,\n", "      `+\"`node`\"+` INT,\n", "      `+\"`occurred_at`\"+` TIMESTAMP(3) METADATA FROM 'timestamp',\n", "      WATERMARK FOR `+\"`occurred_at`\"+` AS `+\"`occurred_at`\"+` - INTERVAL '5' SECOND\n")),
// 		})
// 		if err != nil {
// 			return err
// 		}
// 		return nil
// 	})
// }
// ```
type FlinkJobTable struct {
	pulumi.CustomResourceState

	// The id of the service integration that is used with this table. It must have the service integration type `flink`. To set up proper dependencies please refer to this variable as a reference. This property cannot be changed, doing so forces recreation of the resource.
	IntegrationId pulumi.StringOutput `pulumi:"integrationId"`
	// Name of the jdbc table that is to be connected to this table. Valid if the service integration id refers to a mysql or postgres service. This property cannot be changed, doing so forces recreation of the resource.
	JdbcTable pulumi.StringPtrOutput `pulumi:"jdbcTable"`
	// When used as a source, upsert Kafka connectors update values that use an existing key and delete values that are null. For sinks, the connector correspondingly writes update or delete messages in a compacted topic. If no matching key is found, the values are added as new entries. For more information, see the Apache Flink documentation The possible values are `kafka` and `upsert-kafka`. This property cannot be changed, doing so forces recreation of the resource.
	KafkaConnectorType pulumi.StringPtrOutput `pulumi:"kafkaConnectorType"`
	// Defines an explicit list of physical columns from the table schema that configure the data type for the key format. This property cannot be changed, doing so forces recreation of the resource.
	KafkaKeyFields pulumi.StringArrayOutput `pulumi:"kafkaKeyFields"`
	// Kafka Key Format The possible values are `avro`, `avro-confluent`, `debezium-avro-confluent`, `debezium-json` and `json`. This property cannot be changed, doing so forces recreation of the resource.
	KafkaKeyFormat pulumi.StringPtrOutput `pulumi:"kafkaKeyFormat"`
	// Startup mode The possible values are `earliest-offset`, `latest-offset`, `group-offsets` and `timestamp`. This property cannot be changed, doing so forces recreation of the resource.
	KafkaStartupMode pulumi.StringPtrOutput `pulumi:"kafkaStartupMode"`
	// Name of the kafka topic that is to be connected to this table. Valid if the service integration id refers to a kafka service. This property cannot be changed, doing so forces recreation of the resource.
	KafkaTopic pulumi.StringPtrOutput `pulumi:"kafkaTopic"`
	// Kafka Value Format The possible values are `avro`, `avro-confluent`, `debezium-avro-confluent`, `debezium-json` and `json`. This property cannot be changed, doing so forces recreation of the resource.
	KafkaValueFormat pulumi.StringPtrOutput `pulumi:"kafkaValueFormat"`
	// [LIKE](https://nightlies.apache.org/flink/flink-docs-master/docs/dev/table/sql/create/#like) statement for table creation. This property cannot be changed, doing so forces recreation of the resource.
	LikeOptions pulumi.StringPtrOutput `pulumi:"likeOptions"`
	// Identifies the project this resource belongs to. To set up proper dependencies please refer to this variable as a reference. This property cannot be changed, doing so forces recreation of the resource.
	Project pulumi.StringOutput `pulumi:"project"`
	// The SQL statement to create the table. This property cannot be changed, doing so forces recreation of the resource.
	SchemaSql pulumi.StringOutput `pulumi:"schemaSql"`
	// Specifies the name of the service that this resource belongs to. To set up proper dependencies please refer to this variable as a reference. This property cannot be changed, doing so forces recreation of the resource.
	ServiceName pulumi.StringOutput `pulumi:"serviceName"`
	// The Table ID of the flink table in the flink service.
	TableId pulumi.StringOutput `pulumi:"tableId"`
	// Specifies the name of the table. This property cannot be changed, doing so forces recreation of the resource.
	TableName pulumi.StringOutput `pulumi:"tableName"`
}

// NewFlinkJobTable registers a new resource with the given unique name, arguments, and options.
func NewFlinkJobTable(ctx *pulumi.Context,
	name string, args *FlinkJobTableArgs, opts ...pulumi.ResourceOption) (*FlinkJobTable, error) {
	if args == nil {
		return nil, errors.New("missing one or more required arguments")
	}

	if args.IntegrationId == nil {
		return nil, errors.New("invalid value for required argument 'IntegrationId'")
	}
	if args.Project == nil {
		return nil, errors.New("invalid value for required argument 'Project'")
	}
	if args.SchemaSql == nil {
		return nil, errors.New("invalid value for required argument 'SchemaSql'")
	}
	if args.ServiceName == nil {
		return nil, errors.New("invalid value for required argument 'ServiceName'")
	}
	if args.TableName == nil {
		return nil, errors.New("invalid value for required argument 'TableName'")
	}
	var resource FlinkJobTable
	err := ctx.RegisterResource("aiven:index/flinkJobTable:FlinkJobTable", name, args, &resource, opts...)
	if err != nil {
		return nil, err
	}
	return &resource, nil
}

// GetFlinkJobTable gets an existing FlinkJobTable resource's state with the given name, ID, and optional
// state properties that are used to uniquely qualify the lookup (nil if not required).
func GetFlinkJobTable(ctx *pulumi.Context,
	name string, id pulumi.IDInput, state *FlinkJobTableState, opts ...pulumi.ResourceOption) (*FlinkJobTable, error) {
	var resource FlinkJobTable
	err := ctx.ReadResource("aiven:index/flinkJobTable:FlinkJobTable", name, id, state, &resource, opts...)
	if err != nil {
		return nil, err
	}
	return &resource, nil
}

// Input properties used for looking up and filtering FlinkJobTable resources.
type flinkJobTableState struct {
	// The id of the service integration that is used with this table. It must have the service integration type `flink`. To set up proper dependencies please refer to this variable as a reference. This property cannot be changed, doing so forces recreation of the resource.
	IntegrationId *string `pulumi:"integrationId"`
	// Name of the jdbc table that is to be connected to this table. Valid if the service integration id refers to a mysql or postgres service. This property cannot be changed, doing so forces recreation of the resource.
	JdbcTable *string `pulumi:"jdbcTable"`
	// When used as a source, upsert Kafka connectors update values that use an existing key and delete values that are null. For sinks, the connector correspondingly writes update or delete messages in a compacted topic. If no matching key is found, the values are added as new entries. For more information, see the Apache Flink documentation The possible values are `kafka` and `upsert-kafka`. This property cannot be changed, doing so forces recreation of the resource.
	KafkaConnectorType *string `pulumi:"kafkaConnectorType"`
	// Defines an explicit list of physical columns from the table schema that configure the data type for the key format. This property cannot be changed, doing so forces recreation of the resource.
	KafkaKeyFields []string `pulumi:"kafkaKeyFields"`
	// Kafka Key Format The possible values are `avro`, `avro-confluent`, `debezium-avro-confluent`, `debezium-json` and `json`. This property cannot be changed, doing so forces recreation of the resource.
	KafkaKeyFormat *string `pulumi:"kafkaKeyFormat"`
	// Startup mode The possible values are `earliest-offset`, `latest-offset`, `group-offsets` and `timestamp`. This property cannot be changed, doing so forces recreation of the resource.
	KafkaStartupMode *string `pulumi:"kafkaStartupMode"`
	// Name of the kafka topic that is to be connected to this table. Valid if the service integration id refers to a kafka service. This property cannot be changed, doing so forces recreation of the resource.
	KafkaTopic *string `pulumi:"kafkaTopic"`
	// Kafka Value Format The possible values are `avro`, `avro-confluent`, `debezium-avro-confluent`, `debezium-json` and `json`. This property cannot be changed, doing so forces recreation of the resource.
	KafkaValueFormat *string `pulumi:"kafkaValueFormat"`
	// [LIKE](https://nightlies.apache.org/flink/flink-docs-master/docs/dev/table/sql/create/#like) statement for table creation. This property cannot be changed, doing so forces recreation of the resource.
	LikeOptions *string `pulumi:"likeOptions"`
	// Identifies the project this resource belongs to. To set up proper dependencies please refer to this variable as a reference. This property cannot be changed, doing so forces recreation of the resource.
	Project *string `pulumi:"project"`
	// The SQL statement to create the table. This property cannot be changed, doing so forces recreation of the resource.
	SchemaSql *string `pulumi:"schemaSql"`
	// Specifies the name of the service that this resource belongs to. To set up proper dependencies please refer to this variable as a reference. This property cannot be changed, doing so forces recreation of the resource.
	ServiceName *string `pulumi:"serviceName"`
	// The Table ID of the flink table in the flink service.
	TableId *string `pulumi:"tableId"`
	// Specifies the name of the table. This property cannot be changed, doing so forces recreation of the resource.
	TableName *string `pulumi:"tableName"`
}

type FlinkJobTableState struct {
	// The id of the service integration that is used with this table. It must have the service integration type `flink`. To set up proper dependencies please refer to this variable as a reference. This property cannot be changed, doing so forces recreation of the resource.
	IntegrationId pulumi.StringPtrInput
	// Name of the jdbc table that is to be connected to this table. Valid if the service integration id refers to a mysql or postgres service. This property cannot be changed, doing so forces recreation of the resource.
	JdbcTable pulumi.StringPtrInput
	// When used as a source, upsert Kafka connectors update values that use an existing key and delete values that are null. For sinks, the connector correspondingly writes update or delete messages in a compacted topic. If no matching key is found, the values are added as new entries. For more information, see the Apache Flink documentation The possible values are `kafka` and `upsert-kafka`. This property cannot be changed, doing so forces recreation of the resource.
	KafkaConnectorType pulumi.StringPtrInput
	// Defines an explicit list of physical columns from the table schema that configure the data type for the key format. This property cannot be changed, doing so forces recreation of the resource.
	KafkaKeyFields pulumi.StringArrayInput
	// Kafka Key Format The possible values are `avro`, `avro-confluent`, `debezium-avro-confluent`, `debezium-json` and `json`. This property cannot be changed, doing so forces recreation of the resource.
	KafkaKeyFormat pulumi.StringPtrInput
	// Startup mode The possible values are `earliest-offset`, `latest-offset`, `group-offsets` and `timestamp`. This property cannot be changed, doing so forces recreation of the resource.
	KafkaStartupMode pulumi.StringPtrInput
	// Name of the kafka topic that is to be connected to this table. Valid if the service integration id refers to a kafka service. This property cannot be changed, doing so forces recreation of the resource.
	KafkaTopic pulumi.StringPtrInput
	// Kafka Value Format The possible values are `avro`, `avro-confluent`, `debezium-avro-confluent`, `debezium-json` and `json`. This property cannot be changed, doing so forces recreation of the resource.
	KafkaValueFormat pulumi.StringPtrInput
	// [LIKE](https://nightlies.apache.org/flink/flink-docs-master/docs/dev/table/sql/create/#like) statement for table creation. This property cannot be changed, doing so forces recreation of the resource.
	LikeOptions pulumi.StringPtrInput
	// Identifies the project this resource belongs to. To set up proper dependencies please refer to this variable as a reference. This property cannot be changed, doing so forces recreation of the resource.
	Project pulumi.StringPtrInput
	// The SQL statement to create the table. This property cannot be changed, doing so forces recreation of the resource.
	SchemaSql pulumi.StringPtrInput
	// Specifies the name of the service that this resource belongs to. To set up proper dependencies please refer to this variable as a reference. This property cannot be changed, doing so forces recreation of the resource.
	ServiceName pulumi.StringPtrInput
	// The Table ID of the flink table in the flink service.
	TableId pulumi.StringPtrInput
	// Specifies the name of the table. This property cannot be changed, doing so forces recreation of the resource.
	TableName pulumi.StringPtrInput
}

func (FlinkJobTableState) ElementType() reflect.Type {
	return reflect.TypeOf((*flinkJobTableState)(nil)).Elem()
}

type flinkJobTableArgs struct {
	// The id of the service integration that is used with this table. It must have the service integration type `flink`. To set up proper dependencies please refer to this variable as a reference. This property cannot be changed, doing so forces recreation of the resource.
	IntegrationId string `pulumi:"integrationId"`
	// Name of the jdbc table that is to be connected to this table. Valid if the service integration id refers to a mysql or postgres service. This property cannot be changed, doing so forces recreation of the resource.
	JdbcTable *string `pulumi:"jdbcTable"`
	// When used as a source, upsert Kafka connectors update values that use an existing key and delete values that are null. For sinks, the connector correspondingly writes update or delete messages in a compacted topic. If no matching key is found, the values are added as new entries. For more information, see the Apache Flink documentation The possible values are `kafka` and `upsert-kafka`. This property cannot be changed, doing so forces recreation of the resource.
	KafkaConnectorType *string `pulumi:"kafkaConnectorType"`
	// Defines an explicit list of physical columns from the table schema that configure the data type for the key format. This property cannot be changed, doing so forces recreation of the resource.
	KafkaKeyFields []string `pulumi:"kafkaKeyFields"`
	// Kafka Key Format The possible values are `avro`, `avro-confluent`, `debezium-avro-confluent`, `debezium-json` and `json`. This property cannot be changed, doing so forces recreation of the resource.
	KafkaKeyFormat *string `pulumi:"kafkaKeyFormat"`
	// Startup mode The possible values are `earliest-offset`, `latest-offset`, `group-offsets` and `timestamp`. This property cannot be changed, doing so forces recreation of the resource.
	KafkaStartupMode *string `pulumi:"kafkaStartupMode"`
	// Name of the kafka topic that is to be connected to this table. Valid if the service integration id refers to a kafka service. This property cannot be changed, doing so forces recreation of the resource.
	KafkaTopic *string `pulumi:"kafkaTopic"`
	// Kafka Value Format The possible values are `avro`, `avro-confluent`, `debezium-avro-confluent`, `debezium-json` and `json`. This property cannot be changed, doing so forces recreation of the resource.
	KafkaValueFormat *string `pulumi:"kafkaValueFormat"`
	// [LIKE](https://nightlies.apache.org/flink/flink-docs-master/docs/dev/table/sql/create/#like) statement for table creation. This property cannot be changed, doing so forces recreation of the resource.
	LikeOptions *string `pulumi:"likeOptions"`
	// Identifies the project this resource belongs to. To set up proper dependencies please refer to this variable as a reference. This property cannot be changed, doing so forces recreation of the resource.
	Project string `pulumi:"project"`
	// The SQL statement to create the table. This property cannot be changed, doing so forces recreation of the resource.
	SchemaSql string `pulumi:"schemaSql"`
	// Specifies the name of the service that this resource belongs to. To set up proper dependencies please refer to this variable as a reference. This property cannot be changed, doing so forces recreation of the resource.
	ServiceName string `pulumi:"serviceName"`
	// Specifies the name of the table. This property cannot be changed, doing so forces recreation of the resource.
	TableName string `pulumi:"tableName"`
}

// The set of arguments for constructing a FlinkJobTable resource.
type FlinkJobTableArgs struct {
	// The id of the service integration that is used with this table. It must have the service integration type `flink`. To set up proper dependencies please refer to this variable as a reference. This property cannot be changed, doing so forces recreation of the resource.
	IntegrationId pulumi.StringInput
	// Name of the jdbc table that is to be connected to this table. Valid if the service integration id refers to a mysql or postgres service. This property cannot be changed, doing so forces recreation of the resource.
	JdbcTable pulumi.StringPtrInput
	// When used as a source, upsert Kafka connectors update values that use an existing key and delete values that are null. For sinks, the connector correspondingly writes update or delete messages in a compacted topic. If no matching key is found, the values are added as new entries. For more information, see the Apache Flink documentation The possible values are `kafka` and `upsert-kafka`. This property cannot be changed, doing so forces recreation of the resource.
	KafkaConnectorType pulumi.StringPtrInput
	// Defines an explicit list of physical columns from the table schema that configure the data type for the key format. This property cannot be changed, doing so forces recreation of the resource.
	KafkaKeyFields pulumi.StringArrayInput
	// Kafka Key Format The possible values are `avro`, `avro-confluent`, `debezium-avro-confluent`, `debezium-json` and `json`. This property cannot be changed, doing so forces recreation of the resource.
	KafkaKeyFormat pulumi.StringPtrInput
	// Startup mode The possible values are `earliest-offset`, `latest-offset`, `group-offsets` and `timestamp`. This property cannot be changed, doing so forces recreation of the resource.
	KafkaStartupMode pulumi.StringPtrInput
	// Name of the kafka topic that is to be connected to this table. Valid if the service integration id refers to a kafka service. This property cannot be changed, doing so forces recreation of the resource.
	KafkaTopic pulumi.StringPtrInput
	// Kafka Value Format The possible values are `avro`, `avro-confluent`, `debezium-avro-confluent`, `debezium-json` and `json`. This property cannot be changed, doing so forces recreation of the resource.
	KafkaValueFormat pulumi.StringPtrInput
	// [LIKE](https://nightlies.apache.org/flink/flink-docs-master/docs/dev/table/sql/create/#like) statement for table creation. This property cannot be changed, doing so forces recreation of the resource.
	LikeOptions pulumi.StringPtrInput
	// Identifies the project this resource belongs to. To set up proper dependencies please refer to this variable as a reference. This property cannot be changed, doing so forces recreation of the resource.
	Project pulumi.StringInput
	// The SQL statement to create the table. This property cannot be changed, doing so forces recreation of the resource.
	SchemaSql pulumi.StringInput
	// Specifies the name of the service that this resource belongs to. To set up proper dependencies please refer to this variable as a reference. This property cannot be changed, doing so forces recreation of the resource.
	ServiceName pulumi.StringInput
	// Specifies the name of the table. This property cannot be changed, doing so forces recreation of the resource.
	TableName pulumi.StringInput
}

func (FlinkJobTableArgs) ElementType() reflect.Type {
	return reflect.TypeOf((*flinkJobTableArgs)(nil)).Elem()
}

type FlinkJobTableInput interface {
	pulumi.Input

	ToFlinkJobTableOutput() FlinkJobTableOutput
	ToFlinkJobTableOutputWithContext(ctx context.Context) FlinkJobTableOutput
}

func (*FlinkJobTable) ElementType() reflect.Type {
	return reflect.TypeOf((**FlinkJobTable)(nil)).Elem()
}

func (i *FlinkJobTable) ToFlinkJobTableOutput() FlinkJobTableOutput {
	return i.ToFlinkJobTableOutputWithContext(context.Background())
}

func (i *FlinkJobTable) ToFlinkJobTableOutputWithContext(ctx context.Context) FlinkJobTableOutput {
	return pulumi.ToOutputWithContext(ctx, i).(FlinkJobTableOutput)
}

// FlinkJobTableArrayInput is an input type that accepts FlinkJobTableArray and FlinkJobTableArrayOutput values.
// You can construct a concrete instance of `FlinkJobTableArrayInput` via:
//
//          FlinkJobTableArray{ FlinkJobTableArgs{...} }
type FlinkJobTableArrayInput interface {
	pulumi.Input

	ToFlinkJobTableArrayOutput() FlinkJobTableArrayOutput
	ToFlinkJobTableArrayOutputWithContext(context.Context) FlinkJobTableArrayOutput
}

type FlinkJobTableArray []FlinkJobTableInput

func (FlinkJobTableArray) ElementType() reflect.Type {
	return reflect.TypeOf((*[]*FlinkJobTable)(nil)).Elem()
}

func (i FlinkJobTableArray) ToFlinkJobTableArrayOutput() FlinkJobTableArrayOutput {
	return i.ToFlinkJobTableArrayOutputWithContext(context.Background())
}

func (i FlinkJobTableArray) ToFlinkJobTableArrayOutputWithContext(ctx context.Context) FlinkJobTableArrayOutput {
	return pulumi.ToOutputWithContext(ctx, i).(FlinkJobTableArrayOutput)
}

// FlinkJobTableMapInput is an input type that accepts FlinkJobTableMap and FlinkJobTableMapOutput values.
// You can construct a concrete instance of `FlinkJobTableMapInput` via:
//
//          FlinkJobTableMap{ "key": FlinkJobTableArgs{...} }
type FlinkJobTableMapInput interface {
	pulumi.Input

	ToFlinkJobTableMapOutput() FlinkJobTableMapOutput
	ToFlinkJobTableMapOutputWithContext(context.Context) FlinkJobTableMapOutput
}

type FlinkJobTableMap map[string]FlinkJobTableInput

func (FlinkJobTableMap) ElementType() reflect.Type {
	return reflect.TypeOf((*map[string]*FlinkJobTable)(nil)).Elem()
}

func (i FlinkJobTableMap) ToFlinkJobTableMapOutput() FlinkJobTableMapOutput {
	return i.ToFlinkJobTableMapOutputWithContext(context.Background())
}

func (i FlinkJobTableMap) ToFlinkJobTableMapOutputWithContext(ctx context.Context) FlinkJobTableMapOutput {
	return pulumi.ToOutputWithContext(ctx, i).(FlinkJobTableMapOutput)
}

type FlinkJobTableOutput struct{ *pulumi.OutputState }

func (FlinkJobTableOutput) ElementType() reflect.Type {
	return reflect.TypeOf((**FlinkJobTable)(nil)).Elem()
}

func (o FlinkJobTableOutput) ToFlinkJobTableOutput() FlinkJobTableOutput {
	return o
}

func (o FlinkJobTableOutput) ToFlinkJobTableOutputWithContext(ctx context.Context) FlinkJobTableOutput {
	return o
}

type FlinkJobTableArrayOutput struct{ *pulumi.OutputState }

func (FlinkJobTableArrayOutput) ElementType() reflect.Type {
	return reflect.TypeOf((*[]*FlinkJobTable)(nil)).Elem()
}

func (o FlinkJobTableArrayOutput) ToFlinkJobTableArrayOutput() FlinkJobTableArrayOutput {
	return o
}

func (o FlinkJobTableArrayOutput) ToFlinkJobTableArrayOutputWithContext(ctx context.Context) FlinkJobTableArrayOutput {
	return o
}

func (o FlinkJobTableArrayOutput) Index(i pulumi.IntInput) FlinkJobTableOutput {
	return pulumi.All(o, i).ApplyT(func(vs []interface{}) *FlinkJobTable {
		return vs[0].([]*FlinkJobTable)[vs[1].(int)]
	}).(FlinkJobTableOutput)
}

type FlinkJobTableMapOutput struct{ *pulumi.OutputState }

func (FlinkJobTableMapOutput) ElementType() reflect.Type {
	return reflect.TypeOf((*map[string]*FlinkJobTable)(nil)).Elem()
}

func (o FlinkJobTableMapOutput) ToFlinkJobTableMapOutput() FlinkJobTableMapOutput {
	return o
}

func (o FlinkJobTableMapOutput) ToFlinkJobTableMapOutputWithContext(ctx context.Context) FlinkJobTableMapOutput {
	return o
}

func (o FlinkJobTableMapOutput) MapIndex(k pulumi.StringInput) FlinkJobTableOutput {
	return pulumi.All(o, k).ApplyT(func(vs []interface{}) *FlinkJobTable {
		return vs[0].(map[string]*FlinkJobTable)[vs[1].(string)]
	}).(FlinkJobTableOutput)
}

func init() {
	pulumi.RegisterInputType(reflect.TypeOf((*FlinkJobTableInput)(nil)).Elem(), &FlinkJobTable{})
	pulumi.RegisterInputType(reflect.TypeOf((*FlinkJobTableArrayInput)(nil)).Elem(), FlinkJobTableArray{})
	pulumi.RegisterInputType(reflect.TypeOf((*FlinkJobTableMapInput)(nil)).Elem(), FlinkJobTableMap{})
	pulumi.RegisterOutputType(FlinkJobTableOutput{})
	pulumi.RegisterOutputType(FlinkJobTableArrayOutput{})
	pulumi.RegisterOutputType(FlinkJobTableMapOutput{})
}
